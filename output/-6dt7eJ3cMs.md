# Summary

This detailed transcription of a technical talk by the Databricks co-founder and CEO, Arligotzi, highlights several key points related to Databricks, their platform, and the state of the data and AI industry. Here are the essential takeaways:

1. **Event Scale and Participation:**
   - Largest data and AI gathering worldwide.
   - Over 60,000 global attendees online, 16,000 in person.
   - Representatives from 140 countries, 600 training sessions, and 143 exhibitors.

2. **Contributions to Open Source:**
   - Significant contributions to open-source projects like Apache Spark, Delta Lake, and MLflow, with millions of downloads annually.
   - Databricks employees contributed 12 million lines of open-source code.

3. **Key Problems in Data and AI:**
   - **Generative AI (Gen AI):** High demand for adoption; struggles with production implementation and ensuring models are use-case specific, cost-effective, and secure.
   - **Security and Privacy:** Critical concerns about data governance and privacy across fragmented data estates.
   - **Data Fragmentation:** Overcomplexity and high costs due to various software and data siloed systems.

4. **Databricks' Vision and Strategy:**
   - **Lakehouse Concept:** Centralizing data storage in standardized, open formats stored in Data Lakes like S3, ADLS, or GCS, without vendor lock-in.
   - **Unified Data and Security Layers:** Delta Lake for data storage, Unity Catalog for governance, now being open-sourced.

5. **Recent Developments:**
   - **Project Uniform:** Ensures compatibility with both Delta Lake and Apache Iceberg formats, mitigating data format fragmentation.
   - **Acquisition of Tabular:** Aimed to address data format standardization.

6. **Databricks Platform Capabilities:**
   - **Serverless Infrastructure:** Comprehensive serverless offerings for Databricks components, eliminating the need for cluster management, optimizing data layouts, and simplifying version control.
   - **Unity Catalog:** Governance across various data types, from raw files to AI models, now open-sourced.
   - **Mosaic AI:** Platform for building custom generative AI models on user data using serverless GPUs.

7. **Data Intelligence Platform:**
   - **Democratizing Data:** Enabling anyone within an organization to query data using natural language.
   - **Democratizing AI:** Simplifying the creation and deployment of AI models on custom data for organizations.

8. **Future Directions:**
   - Emphasis on Serverless computing, with future products likely available only in Serverless.

9. **Customer Adoption:** Over 12,000 customers are leveraging the Databricks Data Intelligence platform.

The talk emphasizes Databricks' commitment to open-source principles, addressing critical industry challenges, and innovating to make data and AI more accessible and manageable.

# Transcription

 Welcome to the stage, Databricks co-founder and CEO, Arligotzi. Hi, hi. Everybody's super excited to be here. This is my favorite week every year. Okay, 52 weeks. This is the favorite one. Oh wow, lots of people still coming in. Alright, so we are super excited to welcome here, everyone. This is a global event. In fact, I think this is the largest data and AI gathering in the world right now. And we're super excited to have you here. Thank you for coming. Okay, so let's look at some stats. These stats are awesome. So we have over 60,000 people worldwide. Actually, watching. We have 16,000 people here. In fact, we've taken over all of Moscone, you know, North, Southwest, all of it. We have 140 countries represented. That's more countries that I will ever visit, probably. And I don't know, maybe some of you have visited that many, but. We have 600 training sessions. So lots and lots of people are getting training in data and AI yesterday, today. And that's super, super awesome. Over 200 teams are flying in. They're nervous. They're going to present what they've been working on all year. You know, all eyes are on data and AI, right? So everybody wants to know what are the use cases? Is this stuff working? Is it in production? So it's going to be super awesome. You're going to listen to these talks. There's 143 exhibitors. Please go to the Expo hall and check out what they're doing. They have a lot of stuff, a lot of products. So I'm going to go check it out as well. Super excited about that. Also, this community has contributed to a lot of open source projects and to open source. So this year, we're 11 years into the Spark project, and we have over a billion downloads now a year. And same thing with the Delta Lake project, over a billion downloads a year. And MLflow, which is a machine learning, gen AI ops platform, has now over 200 million downloads. And then we wanted to run a fun stats. We wanted to check how many lines of code has Databricks employees contributed to open source. And you can see here it's 12 million, actually. Actually, this one was hard to compute. It took us a long time. Initially, we thought we contributed 100 million lines, but then we ran the stats carefully. And it turns out a lot of it is just generated code. Okay? So we have to figure that out. So just put it in perspective. I think the whole Android project is about 3 million lines of code. Okay, so this is obviously we're all in on open. And if you're all in on open source and open, partners are super important. The partner ecosystem around those open source projects, the companies around it are super important. So I want to thank all of our partners and all of our sponsors. Please check them out in the Expo hall. We have, you can see here, we have all the hyperscalers. We have all the global size partnering here. But also lots of ISVs that are present. So check them out. Okay? So we have awesome lineup of speakers. So we're going to see talks from DuckDB creator. We're going to hear from Professor Fei-Fei Li. We're going to hear from the one and only, Jen Sanhwang. Professor Jayjin will be talking about small language models. We're going to hear from Texas Rangers. We're going to hear from General Motors, Block. We're going to hear from Ryan Blue, the original creator of Apache Iceberg. We're going to hear from Terry from Posit or company formerly known as RStudio. And then we have lots of, lots of announcements from Databricks speakers. Okay. So in my keynote here, I just want to give you guys the vision of what we think we can do with the Databricks platform. I'm not going to have that many announcements. And then afterwards, we're going to hear talks from different bricksters. We'll get on stage here. And most of the announcements will be there. Okay? So we've said this for a long time. Every company now on the planet wants to be a data and AI company. Okay? So in the last 18 months, every CIO, even every CEO I meet of a Fortune 500 company or a small company, think that data and AI is going to be super strategic for them over the next five years. They think that that's how they're going to win. Okay? That's going to be the main differentiating factor, is how they leverage data and AI, whether they're in financial sector or they're in retail or they're in media or they're in healthcare or in the public sector. Doesn't matter. All of it, it's going to be data and AI. Okay? And, you know, at Databricks, this is why our mission since day one has been to democratize data and AI. Even when we were researchers back at UC Berkeley, we wanted to take this kind of technology and help everyone in the world to use it. Back then, 10, 15 years ago, if you were an organization, you would have to hire 10,000 engineers and build a full data and AI stack in-house. That's what Uber was doing. That's what Twitter was doing. That's what Airbnb was doing. But these days, you can actually just leverage these platforms and, you know, drive business value for your organization. But in the last 18 months, there's been a lot of pressure. Can you deliver actual use cases? Can you actually get into production? Can we really make sure that data and AI has value? And I've been talking to lots of leaders, lots of practitioners, and I keep hearing the same thing again and again and again. Okay? There's three problems that come up when I talk to folks. Okay? So what are those three problems? Everybody wants Gen AI. Okay? It's like top of mind for everyone. It's coming from the top. It's coming from the board. There's a food fight inside the organizations who owns Gen AI. So that's number one. Number two, everybody's worried about security and privacy of their data and of their Gen AI. Okay? And their whole data estate, actually. They are worried about security and privacy for the whole data state. And that data state today is super fragmented. Okay? So, you know, it has a lot of issues. So let's double click on each of these three and go through what these two problems are. And then I'm going to tell you about the Databricks platform and what we think is the path forward for solving these three. Okay? So let's start with the first one. Everybody wants Gen AI. Everybody wants AI in the organization. They won it yesterday. And it's been an amazing year. Actually, you can see here, this is a benchmark. This benchmark is probably the most popular benchmark. It's called MMLU, Massive Multilanguage Understanding. It's got like 72 categories of different things testing you on biology, history, you name it. And you can see that the AI models, the large language models are getting better and better. See, it's almost saturating up close to 100 score. And you can't go higher than that. But it's also been an awesome year for open source. So we're seeing that the open source models are catching up. You can see DBRX there by Databricks, which was the best model in the world for two whole weeks. And... Thank you, Mark Zuckerberg, for releasing that model two weeks later. And then Lama 3. By the way, Lama 3, we haven't even seen the largest model yet being released. So it's safe to say, open source catching up very, very fast. Okay, so this is awesome. These benchmarks are awesome. The results are breathtaking. But actually, the problem is when I talk to organizations, this is an actual quote, I don't care about standard benchmarks. I want the model to do well on my data and my use case at my organization. I don't care if it's doing well on MMLU or not. That doesn't help my company. It doesn't help me succeed with my use case. So this is the number one thing. People don't know how these models are doing for their use cases, and that's all they care about. The other thing is we did a survey of our customers, and 85% of the use cases have not yet made it into production, the Gen AI use cases. Okay, so they're still sort of trying them out. They're trying to make sure that they're ready to be shipped, but they're not quite in production. To summarize, the AI problem is how do you do Gen AI on your data in your organization and how do you get it into production? And how do you do that while making sure that it has really high quality, so it's really doing the tasks that you have really well? Two, how do you make sure that it's doing that at a good cost? We don't want the cost to be prohibitive. And three, how do you make sure that we can ensure privacy on that AI? So this slide summarizes what we're seeing in the Gen AI space, the problems that people are struggling with. And when it comes to privacy, that's a whole other concern. Okay, so people are super worried about security and privacy of their AI. We're seeing intense pressure. There's talks of AI regulation. There's talks of even, you know, maybe they're going to ban open source models. People are worried about data privacy. But also, not just AI, the data is under attack. There's cyber attacks coming in into data platforms. You know, people are trying to break into their companies, so people really want to make sure that they are secure. Okay, and it's not just security of AI, it's all of the data state. It's all of the raw data. It's all the structured data, unstructured data, the AI models, notebooks, dashboards, anything that's in your organization. This is kind of slowing things down, and people are super cautious and super nervous. Okay, they want to make sure that their data state is secure. Okay, and then third, the data state is super fragmented. This is the number one thing I hear. Like every call I get on, people talk about the fragmentation of the data state. They basically say, typically if I talk to CIO, almost every call I have sounds like this. We have so many different pieces of software. I don't even know what they do. We have one of each. You know, it's, you know, I don't even know, but we have to cut it down. I'm under budget pressure. In fact, I don't just, it doesn't even look like this. I have many of each. Okay, I have many data warehouses. I have, you know, many data science platforms. I have data siloed everywhere. And consequence of this is lots of complexity, huge costs, and then lock into these proprietary different systems. Each of those systems is a little silo that you lock yourself into. Okay, so these are the three problems that we really, as a company, are focused on trying to address. At least we're trying to somehow help move things forward when it comes to these three. So let me walk you through how we do that. So Databix, we call this the Data Intelligence Platform. And our vision starts with what we call the Lakehouse. So let me walk you through that. This idea we had this about five years ago, which is, and we actually announced it at this conference, which is stop giving your data to vendors. Okay, they'll just lock you in. Stop giving your data to any vendors. Okay, it doesn't matter if it's a proprietary data warehouse in the cloud or if it's, you know, Snowflake, or if it's even Databricks, don't give it to us either. Don't give your data to us. Don't give, don't trust vendors. Don't give your data. They'll lock it in. They'll raise the price after a few years. And you have another silo on that picture now that you have to deal with. And you just keep adding these silos over the years. Okay, so what should you do instead? You should instead own your own data. You should have your own data. You should store it in cheap hard drives in the cloud called Data Lakes and just store them there, pay for it independently, make sure it has separated computing storage completely from the compute. So it's just a basic Data Lake, right? Like S3, ADLS, you know, GCS. But we need to store it in a format that's standard. So think of it as USB. So that's why we announced Open Source Delta Lake project here, I think, a bunch of years ago, maybe half a decade ago. And the idea is, okay, so then we have this USB format. And once we have this USB format, anyone can just plug in their data platform. Any of those vendors that I said don't give your data to, they should just plug in their USB stick into that data that you have in the cloud and then let the best engine win. Let's see who's best, right? Maybe this week it's us, maybe next week it's someone else. This brings disruption and removes the slot in and also reduces the cost and also lets you get many more use cases because you can use different engines for different purposes if you want. So this is our vision. Unfortunately, what happened is we almost succeeded and people are bought in, okay, everybody here in the crowd wants this. They're like, we want this, we want to own our data, we want it in a standardized format, but unfortunately there's this, like, fragmentation. So there's now two camps, okay? On Databricks, we have Delta Lake. You know, we're seeing actually 92% of all of our data go to Delta. That's about four exabytes of data every day. So 4,000 petabytes every day that's processed going to Delta. But there's lots of us, other vendors that are using this Apache Iceberg format, which is another format. Okay, so last week we announced that we're acquiring Tabular, which is... Thank you. It's a week old news, you know? And we announced that we're acquiring Tabular. Tabular was founded by the original creators of the Apache Iceberg project. Okay, so Rime Blue and Dan Weeks started the Apache Iceberg project when we were working at Netflix, and we acquired this company. So I want to tell you a little bit about why did we do that, what's our thinking around that. Okay, so the reason we did this is that we want this problem to go away so that you don't have to pick which of the two silos do I have to... Which of the USB formats do I have to store this in? If I store it in this USB format, you know, these cables won't be able to plug in. If I store it in that one, the other engines can't plug in. We don't want it to be that way, okay? Whatever you store it in, all the cables should just work, okay? We just want very simple standard for everything. So our strategy is, a year ago here we announced Project Uniform. Project Uniform is part of Delta, and we're actually announcing it being GA this week here. And Uniform, it translates to both of these formats, Delta and Iceberg. It's already doing that today, and it's GA, but you know, we really understand Delta really well. We don't understand all the intricacies of the Iceberg format, but the original creators of Apache Iceberg, they do. So now at Databricks, we have the employees from both of these projects, from Delta and Iceberg. So we really, really want to double down on making sure that Uniform has full 100% compatibility and interoperability for both of those. So if you put your data in Uniform today, it should be a no-brainer, it should just work. Then in the background, what we want to do is we want to really work with these communities. The Delta Lake community and the Apache Iceberg community, these are open source communities, right, with people around, you know, all around the world. They're governed by these foundations, like the Apache Software Foundation and the Linux Foundation. We want to work with them and actually change the formats and bring them closer and closer to each other so that the differences between them do not matter. Okay, so if you store your data in Uniform right now, then over the next, you know, period of time, as the formats get closer and closer, it won't even matter which one you have. Like, the distinction will go away, and I hope that in a year or two, we won't even care here about which one. It will be like VHS and Betamax, you know, who cares. Okay, so that's our strategy with respect to the data format. So we hope that we have just the USB format, it shouldn't matter, all the formats should be supported, and it should just work. Okay, so that's problem number one. Remember the fragmentation, your data is locked in, now the data is just sitting in a lake in a USB format. Any engine should be able to access it. Okay, remember the second problem, governance, security. How do I make sure that this stuff is super secure? So when it comes to that, we announced here Unity Catalog a few years back, and Unity Catalog is probably the most important development at Databricks since we started the company. It's probably the main reason people actually come and use Databricks these days, it's because Unity Catalog lets you do governance, not just for your tables, not just for your unstructured and raw files, but for all of your data state, including machine learning models, AI models, you name it. And it's not just access control and security, it's also discovery, it's also lineage. It's also being able to do auditing and data quality monitoring, or AI model quality monitoring. Okay, so it's super, super important. So that's what that looks like. So now we have Delta, and then we have for governance, Unity Catalog, and I'm super excited to announce that we're also open sourcing Unity Catalog this week here. So please go to Matej's talk tomorrow on Unity Catalog, and he's going to cover all the interesting things that go into it, and we'll see him open source the project. So that's the vision of the Lakehouse. So basically, all the silos that you had before, they can just access one copy of the data that's in a standardized USB format under your ownership in a lake like GCS or S3 or ADLS, and it goes through one governance layer that's just standardized, that's Unity Catalog for all of your data state, and that's also open, and that Unity Catalog open sourcing, one thing that's really important about it is that it supports two APIs that already are standard for governance. Hive Metastore API and Iceberg REST Catalog API. So that's already supported in Unity Catalog. In fact, I think Tablellar and Databricks are the only ones that support that REST API Catalog. So Unity Catalog will also just be building on APIs that are already prevalent and everybody's using. So we basically standardize the data layer and the security layer, and you own your data, and everything goes through these open interfaces, and I think that's going to be awesome for the community, for everybody in here, because we're just going to have way more use cases, we're going to be able to do much more innovation, and we'll just expand this market for everybody involved. It's just going to be great. Okay, so that's the lakehouse, but I said data intelligence, so what is that? So what we're really excited about as a company moving forward is when you take this lakehouse that supports all of your data and all of your governance, and you combine it with generative AI, in particular last year on stage here, we announced the acquisition of Mosaic AI, and when you combine Mosaic AI, which was a platform to train custom AI's on your data, when you combine Mosaic AI with the lakehouse platform, you get what we call data intelligence. So what is data intelligence? Data intelligence means that our platform trains generative AI models on your data in isolation for each customer, and leverages that throughout the platform for everything it does. So what does that mean? That sounds like a mouthful. So what do we want to do? What is data intelligence? Well, data intelligence for us is really, we want to apply it to two things. One, we want to democratize data, and second, we want to democratize AI. Okay, what is democratized data? What is democratized AI? Sounds very similar. Democratizing data means that anyone in your organization should be able to access the data directly. Okay, today that's not true. Your CEO is not going to go access the data and ask questions from the data. He or she will go to the data team and ask them, hey, can you get me this report, and they'll say, by when do you need it, and then they're going to work on it, because your CEO does not speak SQL or Python, or at least doesn't know where to find the data and submit their own queries. So we're really hoping that we can democratize this, that if you speak English or any other natural language, you should just be able to ask your question from the data, and many, many more people in the organization should be able to get insights from that data. So we're very excited about that. Demarcatizing AI is different. Demarcatizing AI means practitioners like yourselves here in the room should be able to really easily create AI models that understand your data in your organization. That's what democratizing AI is, and we want to do both of these two things. Okay, so let's start with the first one. How do we actually make you talk to your data? We want you to be able to talk to their data. Okay, so this is what data intelligence is. We want you to be able to ask, how is the business doing on its FY goals? And we want the platform to be able to understand what that means. FY stands for fiscal year. In the particular company you're at, fiscal year might start 1st of February, or maybe 1st of July, it should know that. And business in your particular company means certain KPIs that are most important in your organization, the definition of those, it should understand those, and then we want it to give you back authority answers that are certified and that are correct, and they don't have any hallucinations, and we can actually verify that these are correct. That's what data intelligence is for us. Okay, so that's what we want to do. That's what the whole company is working on. Thank you. That's just a fake screenshot. It's not an actual product, okay? But we have actual real demos live, but I'm not going to take the risk anymore. Okay. Too nervous. Okay. And then, this is, you know, if you click on a random table, data set in your organization, chances are you get something like this that's, you know, completely hard to understand. What does this even mean? Okay, TBHINCL underscore S underscore key. This is why your CEO is not asking questions from your data. Like, who knows what this means? We can fill out, so in Unity catalog, we already fill out in English text what all your data sets do using generative AI. Okay? So we fill it out like this. It describes exactly in English what all the data assets are doing, and when we have that, then we can actually do proper search. So when someone comes in and says, where do I find shipping information for home goods, it knows that, okay, you're looking for this particular data set over there because it has all those descriptions. Okay? So, that's what we're working on to make happen. And then, I do want to put a big plug for something that's already in Databricks today, which is the assistant that we've trained using DBRX. We fine tuned it on actual all of Databricks documentation, all the errors that we've seen since we started Databricks. And you can ask it today to do pretty advanced stuff. You can say, hey, write code for me that, you know, does the streaming thing, you know, writes you code like this. Okay? It just works. I use this every day. Okay? So this has made me much more productive. I'm getting a little bit rusty with my coding. But just work with the assistant and it gets things right. I can't even write half pseudo, you know, kind of looking right code, but, you know, no compiler would ever accept it. But the system will just fix it like this. It will just fix things for me. This is used over 100,000 of users every day use it in Databricks platform. So I do think this already is democratizing access to data and more and more people are able to talk to the data, ask questions from it. Okay. So that's what data intelligence is when it comes to talking to your data. Okay. What about democratizing AI? When it comes to democratizing AI, that's where our whole AI generative AI stack comes in. And we're going to hear in, you know, two talks, you're going to hear the details of this and all the announcements that we have around this. But I'll just give you the high level overview. In Mosaic AI, we basically have all the serverless GPUs around the world and we just enable you to very easily, seamlessly, in a UI, be able to build your own AI on your custom data and productionize it and evaluate it. Okay. So you, the first step is preparing your data so that it's AI ready. So the platform is great for that. Second step is how do you train or how do you fine tune, whether you want to use some fancy techniques like Dora or you want to continuously pre-train the whole model or, you know, you want to use a vector search or whatever you want to do, you can build your AI there. Then you can deploy it in production. Deploying it in production means you're running it on our GPUs in, you know, different countries. Just in a serverless manner, they're just ready to go. We're paying a lot for those GPUs today. My CFO reminds me every week. And that's where we can run these. You can run our vector search database there. And then we have evaluation. We're going to talk a lot about the evaluation here. How do you actually know your AI is doing well in production? And then, of course, finally, really important for most people in this room, how do you govern it? So how do we actually secure it? How do you make sure that we can track it, rate limit it, track the tokens? You know, make sure it's not doing something that we don't want it to do. So that's the Mosaic AI. Okay. So we'll hear from my co-founder Patrick Wendell who will talk about this in detail. Okay. So that's the full platform of Databricks. So that's what we get, the Data Intelligence platform. You have Unity Catalog, you have Delta. You have all these things accessing it. One thing, one last thing I want to announce here is that we are also super excited to announce that all of Databricks now is available in Serverless. Okay. So first of July, you'll be able to get everything in Databricks. So whether it's our notebooks, or whether it's our Spark clusters, or whether it's workflows, job processing, all the different aspects. Databricks so far are only a few parts of it with Serverless. Now you get all of it in a Serverless fashion. This is a project that has involved hundreds of hundreds of engineers for over two years. It's been a long project internally. Two, three years ago, my co-founders Matei and I told the company we got to build a lift and shift simple version of Serverless. And actually our engineers pushed back and said, hey, you guys are wrong. We should redesign it from scratch for the Serverless era. And we told them, nope, we decide, you know, in the company. And turn out we were wrong. The tech leads were right. And they've been working really hard for two years. So we basically redesigned many of the products, you know, the notebooks, the jobs, everything, as if we had started a new company. What would that look like in the Serverless era? Okay. So how do we make sure that it instantaneously comes up? There's no more clusters. Everything just works super, super fast. Under the hood, we make sure that we multiply the resources. In fact, today, you're paying us for idle time if you're not using Serverless. Actually, you're paying the cloud vendors a lot of money. And then you're paying us in addition to that for idle time. With Serverless, you know, you're just paying for what you're using. In fact, there is no cluster to set up for it to be idle or not be idle. Okay. So we'll take care of all that for you under the hood. And one thing that we're excited about this, since we own all the machines now, it's no longer this, you know, joint responsibility over machines that are running in your account and in our account. We're able to really redesign so that this year we'll be rolling a disaster recovery in a different way that's really custom for Serverless. Cost control so you can really do the tracking and you can do the tagging and you can really use AI to predict where your costs are going on the Serverless infrastructure. And we're also able to do security a different way because, again, we own all the machines so we're able to really lock it down in a different way that's not possible when it's not Serverless. So we're very excited about this. You know, all these knobs that we had before are gone, right? Cluster tuning. You have people setting up clusters. What type of machines should they use? Spot instances. This and that. Should it auto scale? None of that is available anymore. It's just gone. There's no such page. You can't do that. The data layout. How are you going to set out exactly your data sets? How are you going to optimize your data sets? That's also gone. We're just optimizing it behind the scenes because it's Serverless. We're just running the background optimization on your data sets to make it really fast and optimal using machine learning. So that's also really awesome. Capacity planning. Usage tracking. And then my favorite thing is no more versions. Okay? So there'll be no Spark versions when you're using this Serverless edition. You can't pick a version of Spark anymore so you don't have to worry about DBR upgrades and those kind of things. So we are super excited about this. Please start using Serverless. And in the future, new products that we roll out, like next year when I'm here on stage, they'll probably only be available in Serverless. So if your organization is not on Serverless, please get on it. We're rolling out Serverless infrastructure all around the world to make sure that we have availability in any region that's near you. Whichever country you're listening in from right now or representing or you flew in from, we will hopefully have a Serverless infrastructure near you there. Alright, so that is the Data Intelligence platform. And today we have over 12,000 customers that are actually leveraging this platform.