# Summary

In the given technical talk on Databricks SQL, several key points and highlights have been shared:

1. **Introduction and Growth**:
   - Databricks SQL was announced in a prior preview about four years ago.
   - It has become the fastest-growing product in Databricks' history with over 7,000 customers, including Shell, AT&T, and Adobe, using it for data warehousing.

2. **Lakehouse Concept**:
   - Traditional enterprise data architectures had separate data warehouses for business intelligence and data lakes for machine learning, leading to data silos and governance challenges.
   - The concept of the lake house combines the strengths of data warehouses and data lakes, using technologies like Delta Lake for storage and Unity Catalog for governance.

3. **Technological Advancements**:
   - Initially, it took about 370 seconds to acquire compute for Databricks SQL, now reduced to less than 5 secondsâ€”a 70x improvement.
   - Key areas focused on include core data warehousing capabilities, out-of-the-box performance, and ease of use.

4. **Core Data Warehousing Features**:
   - Full NCCC support, materialized views, and role-based access control.
   - Built a large data and AI partner ecosystem for tool compatibility (e.g., Tableau, Power BI, Looker).

5. **Performance and Cost Efficiency**:
   - Databricks SQL ensures linear scaling of costs compared to exponentially increasing costs for traditional warehouses.
   - Incorporation of AI significantly improved performance; benchmarks show Databricks SQL performing 60% faster out of the box compared to 2022.

6. **AI and Automation**:
   - Introduction of AI systems for predictive clustering, optimization, and predictive I/O.
   - AI systems integrated into every layer of the data warehousing engine, enhancing performance and efficiency.

7. **Ease of Use and User Experience**:
   - Revamped user experience to make Databricks SQL accessible to analysts and business users, not just data scientists and engineers.
   - Enhanced functionalities such as visual data lineage, simpler error messages, SQL UDFs, AI assistant integration, and built-in support for AI models and vector searches.

8. **Practical Applications and Demonstrations**:
   - Live demo showcased AI-powered data analysis, automated materialized views, and seamless integration with BI tools like Power BI.
   - Demonstrated practical application of AI functions to forecast revenue, analyze sentiment, and leverage vector search for non-exact matches.

9. **Summary**:
   - Over the past four years, significant enhancements have been made to Databricks SQL in terms of capabilities, performance, and user-friendliness.
   - The lake house concept positions Databricks SQL as a cost-effective and flexible alternative to traditional data warehouses.

In conclusion, the various advancements and integrations have made Databricks SQL a powerful, efficient, and user-friendly solution for data warehousing, driving adoption across a wide range of customers and use cases.

# Transcription

 Thank you, Ali. All right, morning. It's a little bit hard to top off after Jensen and Ali. So we announced Databricks SQL, the prior preview of it, about four years ago. And ever since, we've been humbled by reception. Databricks SQL become the fastest growing product in the history of Databricks. And today, over 7,000 customers worldwide, large and small, including Shell, AT&T, Adobe, are using Databricks SQL for their data warehousing workloads on Databricks. But one of the fundamental reasons why Databricks SQL is taken off so quickly really goes back to the idea of the lake house itself. Before lake houses, here's what a typical enterprise data architecture would look like. You might have one or multiple of data warehouses for your business intelligence workloads and what I call looking back in time. And you have one or maybe multiple of your data lakes used by your data scientists, data engineers, and AI engineers for building machine learning models looking to the future. And the two disparate stacks were really incompatible. They have different governance models, have different storage formats, proprietary versus open, and that led to a lot of data duplication, data silos, and honestly, a governance nightmare. So when we looked at this problem a little bit over four years ago, we came up with this cute little term called the lake house. And the idea of the lake house is fairly simple at a high level. Let's marry the best of both worlds from data warehouses and data lakes and combine it into a same package. But the technologies weren't completely in place back then. We had to build a lot of stuff. We have to create Delta Lake for the foundational storage layer. We have to create Unity catalog for the governance layer. But over time, the writing become on the wall that lake houses will be the future. And even proprietary warehouses started talking about lake houses. Forrester, a leading analyst firm, even come up with a whole new Forrester Wave called the lake house. And we're very proud. Databricks, by far, the leader of the Forrester Wave. Now, going back to data warehousing, one of the most important workloads on the lake house is the above you support data warehousing workloads. Databricks SQL is our product to support that. Some of you, many of you are among the 7,000 customers using Databricks SQL every day. Some of you have never tried it. Some of you might have tried it 3 or 4 years ago and informed your impression since. You might still have this impression that Databricks is great for data engineers, for data scientists, for the sure part technical people with PhDs in computer science, but not necessarily for all your analysts or your business users. But what we've really done is we've changed so much of the platform from every single layer that now the platform looks completely different. And to give you one example would be when we first released Databricks SQL, it would take about 370 seconds to acquire a warehouse for compute. But today, the number is down to less than 5. So that's a more than 70 times improvement just in 3 years. And that's just one example. We looked at what are the most important fundamental areas, and we picked 3, and we really worked on every single intro bit. And this includes core data warehousing capabilities, out-of-the-box performance, and ease of use. Let's get started with core data warehousing functionality. Of course, to support data warehousing workflow, we needed a lot of features and functionalities that were not available in the Lakehouse. We needed full NCCC support, we needed materialized views, we needed role-based access control. This were not features that were available out of the box back then. So we built all of them. And building on top of those, we now have built a very large data and AI partner ecosystem. So all your favorite tools were out-of-the-box for data warehousing, and especially in the area of business intelligence. Tableau, Power BI, ThoughtSpot, Looker, Sigma, click, all of those just worked out-of-the-box on the Databricks SQL. And this really substantially lowers the migration cost from traditional data warehouses over to Databricks SQL. The second area that we spent a lot of time on is price performance, out-of-the-box price performance. And price performance is one of the most important, typically one of the most important evaluation criteria in data warehousing POCs. And for two reasons. One is data warehouses tend to be one of the most expensive, if not the most expensive, business systems out there. You spend a lot of money on it. To be able to save on it is essential to your operational efficiency. And the second is you want to guarantee all of your analysts and business users have the best class and experience. When Monday at 9 o'clock, all of them come to work, start opening their dashboards, thousands of queries, hitting the data warehouse. You want to guarantee a particularly low latency. So one thing I hope was clear from past data and AI summits is Databricks has always been really, really good at ETL performance. Last year, this is not a new chart. Last year we published a study that would compare Databricks SQL versus a leading cloud data warehouse on how they would perform as we vary and grow data volume, data in size. So we started with 100 gigabytes, we grew all the way to 30 terabytes. We're just benchmark how well ETL performance work against these two different platforms. Initially, the two compare fairly similarly when the data set were small. But as you scale the data set, the economics of Databricks SQL really start to show because Databricks SQL scale roughly linearly in terms of cost, whereas the data warehouse scale exponentially. And it might sound surprising, but in practice it's actually not that surprising to think about it. The whole foundation of data warehouses were built to handle initially transactional business data out of OLTP databases, which tend to be relatively small. So the whole system designed from ground up to be really optimized for smaller amount of data. And when you do POCs, you typically don't load enormous amount of data into the warehouse. You grew your data over time. So they really tried to optimize for the earlier stage. But in Databricks SQL, because we come from a more of a data lake heritage, we really make sure the whole system would scale. Far beyond 30 terabyte, even to petabytes range. Now, at last year at this conference, I gave a very different style of keynote. I gave a whole technical talk about how we're using AI systems to improve data warehousing engine performance. At the time, we knew AI would be something, but we didn't have the idea, any idea how big and how important it would be. In the last 12 months, we've revamped almost every single layer of our engine to incorporate AI systems in them. And this goes all the way from a bottom physical data layout, the middle layer query engine, and the top workload management scheduling. And then we're really seeing with our own eyes how much the AI systems can improve. And honestly, I think most of us, myself included, underestimate the impact AI system would have. Now, at this point, you care, say, so tell me more, show me the actual numbers. But before I do that, let me just show you a couple examples of things we've done to give you a little bit more concrete idea. And some of these examples might even sound abstract and difficult to understand, but that's okay. The whole point is you don't have to understand all of it, but you can just see the benefit with your eyes. The first example is Michael would tell you more about in the Delta Lake talk tomorrow, about liquid clustering, which is a new fundamental breakthrough in data clustering. But on data bricks, we have built AI systems to learn based on your workloads and automatically actually suggest and cluster data for you. No need to suggest the type of columns you can cluster by, but it also actually decides should you run all of the operations online as part of your ETL workloads so you get the fastest time to clustering, or does it run it offline with a minimum impact on your ETL workloads? The same AI system is also applied to statistics, which is one of the most fundamental ingredients in career optimization for warehousing. The AI system would actually decide what type of statistics do you need on different types of columns, how do you actually minimize the amount of overhead, do we apply higher fidelity statistics, or do we apply actually more sketches depending on the workload you need? One concrete example I gave you last year, I saw you there remember, is this thing called predictive IO. At the time I joked about the term indexes, which is very difficult to pronounce. The whole idea is predictive IO applies a machine learning model to predict where the data would be and give you the benefit of indexing without the overhead, the read-write amplification of indexing. Thanks to the Mosaic AI stacks improvement over the last 12 months, we now can employ a model as an order of magnitude larger in terms of parameters and feature vectors in predictive IOs. So we just rolled out predictive IO 2.0. This is not something you have to opt in, it just works automatically under the hood, but you can accelerate way more workloads beyond the simple selections I've shown you last year. So, okay, numbers. There will be a lot of numbers in the rest of the talk. You've all seen charts like this, a vendor showing some benchmarks and beating a competitor by a wide margin. Every single vendor talk you've been to show that. I'll show you some charts like that too, but this will be the first time I bet you've seen a vendor's own chart in which the vendor didn't perform favorably. So, I think it's sort of conventional wisdom. If you have a PhD in data breaks optimization, you could get the best price performance out of data breaks one way or another. But what I really wanted to make sure is if you don't have time or you don't have that expertise to optimize your system, can you get the best performance out of a box without doing anything? So we created a synthetic benchmark exactly for that scenario, which is continuously ingest a lot of data. And then we run the TPC queries against them. And when we first did that benchmark in 2022, about two years ago, the data warehouse we benchmarked earlier was actually doing better than Databricks SQL. Last year, when we incorporated more of the AI capabilities, the two systems got to roughly on par. But something fairly magical happened last year, in the course of the last 12 months. As the AI systems evolved and become larger and larger and more sophisticated, and as you learn more and more workloads, it actually got 60% faster out of the box compared to 2022. And it now has industry leading performance. And this is completely out of the box, no tuning required. Now, some of you would ask, hey, you started the talk by talking about performance and thousands of analysts hitting the system, but that benchmark seemed to be about running individual queries. So we also created a scenario, a benchmark for the scenario really matter, which is 9am Monday, everybody opens their dashboard. In this benchmark, we created a bunch of robots that just keep hitting the warehouse using a variety of BI queries. Robots don't take coffee breaks, so each robot is probably like 10 times the normal human being. And we again just compare a leading cloud data warehouse versus data break SQL. When there's lower degree of concurrency, data break SQL and the leading warehouse would have roughly the similar low latency. But as we continue to scale and adding to 512 users, the difference really started to show. The warehouse had trouble scaling to the large number of users, and 512 robots probably similarly tens of thousands of users at this point. Whereas data break SQL continued to deliver the same low latency. Now, some of you actually, all of you probably also think, hey, I don't trust benchmarks to show, even though you say you simulate real world scenario, but I really only care about my own workloads, my own real world scenario. That's exactly what I tell our engineering team when they show me all these benchmarks. And I said, hey, I really only care about how our customers queries would actually perform. So we took a step at this. We looked at all the queries, all the BI queries are repeating on data break SQL to establish a baseline 2022 about two years ago, and we measure their performance over time, which track them for the same query, how well they're performing over time. And today, this year, in 2024, the same BI queries will run on average 33% faster. What does 33% or 73% faster? What does 73% mean? A query that used to take 10 seconds now takes 2.7 seconds. It's almost 4x faster. And the best thing is this is without you having to pay a single dime more, without you having to do anything. It's just a system getting better and better under the hood. It's because how much you're obsessed and how much we know performance matters to you. The last part is of use of use. Again, I said earlier, saw you have the impression that data break is great for data scientists, data engineers, but not necessarily great for analysts. So in the last few years, heard a lot of feedback. We revamped the user experience completely. But there's really no way for me to show you how much differently you've gotten. You really have to try it yourself. The best I can do is show you a few screenshots that demonstrated before and after. One example would be, hey, if you want to look at the data lineage or history, or how data are created, in the past, you have to run a SQL query to get a tabular result. But this year, all you need to do is the system will show you visually how the entire end-to-end lineage of all the data sets and not just for tables, also machining models. Error messages used to be daunting to the last technical user. Engineers might love it because they see stack traces, they point out exactly which line number have the issue, but this is very daunting to business users. Now we actually output a really simple error message to error codes they can Google. But even better, the AI models on the side would actually recommend an automatically fixed error for you. We also added a lot of quality of life improvements when it comes to SQL features themselves beyond what standard SQL would give you. This is like SQL UDFs, lateral column aliases, session variables, all of these are features once you start using. You ask yourself, how come not every warehouse has this feature? What have I been missing out on in the last 10 years? But of course, what we really think we can leapfrog is through the use of AI. Databricks Assistant is extremely popular, has been extended to cover all cases of SQL. We introduced AI functions in DB SQL, so all of your analysts can now get the full power of off the shelf, both open source and proprietary large language models out of a box. But even better, your AI engineers can build a new model, publish it to Unity catalog, and make it immediately available in DB SQL for all your analysts to consume. Same thing is applied to most AI stacks vector search. DB SQL can now actually create the vector index automatically. Now, rather than me showing more about this, I would like to invite Pearl onto stage to show you a live demo of what all this looks like. Pearl. Thank you, Reneald. So as Reneald shared, we're using AI to better your Databricks SQL experience by making the lake house much simpler and more powerful. So right now, I'm actually in the SQL editor, and I have some revenue data for a few shops plotted as a time series graph. And this is great and all, but I would love to see what revenue could look like several months from now. Better yet, it'd be great if I could use AI to predict this. So I actually can with our AI functions. And I'm going to go ahead and use the assistant here to help me do exactly that. So I'm going to add a forecast through September 2024. All right, so it looks like the assistant took my original code here, and it's unioning it with the forecast results. And it's doing that by just doing a simple function call to my AI forecast function. This looks good. So let me accept it and hit run. Just like that, I have my revenue forecasted. No Python or data scientists needed. But this revenue trend, it's kind of bad. And I definitely want to share this with the management team with their BI tool of choice, which is Power BI. So I'm going to use the assistant again here, and I'm going to create a materialized view that I will share to our published to Power BI. All right, assistant is hard at work. I'm going to accept that and hit run. So why a materialized view? Materialized views are really great because it accelerates queries, especially those that use AI for downstream users. Okay, my materialized view is done, and it's actually a governed object, and you need a catalog. And I can search for it here. All right, let's go ahead and publish this bad boy to Power BI. As you can see, we have some offerings for Tableau as well. And I'm going to leverage the Databricks SQL Compute Engine. I'm going to direct query to Power BI. And this connection is made by securely through my SSO login. I'm going to select all the columns here, build a column chart, let me resize it. And this can be shared with the management team now. So let's head back to the SQL editor and do more AI-powered analysis. So here I already have a query built out, and it's going to show me a bunch of Yelp reviews. And I'm actually going to go ahead and use AI query, which is actually an AI function here. And it's going to call a custom model that's going to build responses to those reviews. AI query is great because it also allows you to call foundational models like DBRX, but also external models as well. So here I have all my reviews along with the responses from my custom model, but I'm more focused on the reviews. I kind of want to see how people feel about our food and drink. So let me add a filter here. All right. Food, it looks like the filter gave me an exact match for the word food. That's perfect. Let's try drinks. There's no word match, but that's OK. Because thanks to our new vector search function, we can actually use, we can actually search using an embedding model for related reviews in regards to drinks. And this is awesome because it doesn't have to be an exact word match. As you can see, I have Brutaspresso, I have a rich cold brew, a Brutacapuccino. These are all drinks. So what the filter couldn't do, vector search can't. So I'm going to build a parameter here. Databricks makes it super easy to do so. And I can easily search for reviews about any item. For example, here I'm doing pastries. All right. So we have all the reviews here, but I need to do a little bit more analysis. This is kind of a lot. So I'm going to use an AI function. As you can see, we have quite a few to choose from, from fixing grammar to translating text. But I'm going to use Analyze Sentiment to perform sentiment analysis on the review column. OK. Let's run that. So this should give me another column that has sentiment, a sentiment value on it. And it looks like it does. I have my review along with my sentiment. I literally just built a mini rag and sequel. So now I'm just wanting to plot this so I can understand the distribution of sentiment. I'm going to hit Save. And it looks like pastries has a pretty good reputation. But let me try service quality. The store will need to know this to improve if needed. And this viz makes it super easy. So it looks like service could definitely use a bit of improvement. Look at all those negative reviews. So to wrap, I shared with you how Databricks SQL's data warehousing capabilities allow you to query AI functions, call models, and make your data work for you. So with that, let's get to work. Back to you, Reynolds. Thank you, April. All right. So in the last four years, we really revamped the entire experience of data warehousing. Databricks SQL today looked nothing like the Databricks SQL we first released. We made it, we implemented almost all of the core data warehousing capabilities so we can easily, lowest cost migration over to Databricks SQL. We provide best in class out of a box performance. And we also made it dramatically easier to use. Databricks is no longer just a platform for data scientists and data engineers, but also all your analysts. And all of this are really possible with built by building on top of data intelligence stack. And when you combine, that's why we say the best data warehouse is a lakehouse. Because all of us would rather stay in an open area environment of the lakehouse rather than being trapped in a data warehouse. We feel a lot more productive like that. In this case, the lakehouse is also a hell lot cheaper than the warehouses. Thank you.