# Summary

The speaker provides an insightful glimpse into the future of AI, focusing on advancements beyond mere language understanding, and highlighting the evolution from passive vision to active intelligence:

1. **Historical Context and Comparison:**
   - 540 million years ago, primitive organisms like trilobites developed the first eyes, initiating an era of sight, understanding, and action which culminated in the development of intelligence.

2. **Evolution of Computer Vision:**
   - AI's journey began with basic object labeling and image recognition. A decade ago, advancements in neural networks, GPUs, and big data (like the ImageNet project) significantly improved AI's vision capability.
   - The ImageNet challenge demonstrated continual improvement in AI algorithms' performance annually.
   - Recent AI developments have enabled image captioning and even generating images from textual descriptions, a milestone deemed impossible a few years prior.

3. **Beyond Seeing: The Concept of Spatial Intelligence:**
   - The idea that simply seeing is not enough; AI needs to engage in actions and learn from them.
   - Spatial intelligence enables AI to understand and predict 3D spatial relationships, which is crucial for acting in real-world environments.
   - Examples include Google's algorithm for converting photos into 3D shapes, and Stanford's advances in generating 3D models from single images.

4. **Future Directions and Applications:**
   - The integration of vision, spatial intelligence, and language will enable AI to perform complex tasks in 3D environments, akin to human spatial intelligence.
   - Exciting progress in robotic learning includes teaching robots dynamic behaviors using simulation environments like NVIDIA Omniverse.
   - AI's potential in healthcare, where smart sensors and AI can improve patient care and reduce medical staff burnout, and advanced robotics can assist with everyday tasks through brainwave control.

5. **Implications of AI's Progress:**
   - The digital evolution mirrors the Cambrian Explosion, but realizing AI's full potential requires teaching AI to reason and interact within 3D spaces.
   - The deployment of spatial intelligence in AI promises to enhance human productivity and well-being, provided it's done ethically with a focus on human-centric technology development.
   - The ultimate goal is to create AI that collaborates with humans in exploring new possibilities and improving the world.

The talk concludes with a vision of AI as a trusted partner, augmenting human capabilities and tackling complex challenges through enhanced perception and interaction.

# Transcription

 Thank you, Ali. Hi. Good morning, everyone. Really, really happy to be here. And I'm not going to show you products or live demos. So I'm here to share with you a glimpse of the future. A glimpse of the future that goes beyond just understanding language. And I call it from seeing to doing. So let me start by showing you something. Actually, please have my phones a little larger. In fact, I'm going to show you nothing. This is not a glitch. This is our world 540 million years ago. Pure, endless darkness. It was a dark due to the lack of light. Indeed, sunlight filtered a thousand meters below the ocean surface and light permeated from hydrothermal vents onto the sea floor. Although brimming with life, there was not a single eye to be found. Ali were in these oceans. No retinas. No corneas. No lenses. So all this light, all this life went unseen. There was a time when the very idea of seeing didn't exist yet. When it was something that has simply not been done before, until it was. For reasons we're only beginning to understand, trilobites, the first organisms that could sense light, emerged. They were the first inhabitants of the reality we now all take for granted. The first to discover a world in which something exists beyond the self. A world of many other selves. This ability to see, is thought to have helped usher in a period called Cambrian Explosion, where a huge variety of animal species entered fossil records. What began as a passive experience, the simple act of letting light in, soon become much richer and far more active. The nervous system began to evolve. Sight turned into insight. Seeing became understanding. Understanding led to actions, and all of these gave rise to intelligence. So half a billion years later, we're no longer satisfied with just nature's gift of intelligence. Humans are now on a quest to explore how to create machines that can see, just as intelligently as we can, if not better. Nine years ago, I gave a talk at TED, and I delivered what was an early progress report on computer vision. A subfield of artificial intelligence. Three powerful forces had emerged for the first time about a decade ago. A family of algorithms called the neural network. Fast, specialized hardware called graphic processing units, or GPUs, a you're going to hear from Jensen later. And big data, like the collection of 50 million photos that my lab spent years curating called ImageNet. When combined, these factors cause computers not only to see better than ever, but they also ushered in the age of modern AI. We have gone a long way since then. Back then, a decade ago, just labeling objects was a breakthrough, like the first glimpse of light for those early trilobites. But the speed and accuracy of neural network algorithm rapidly improved. Year after year, the annual ImageNet challenge led by my lab gauged the performance of these algorithms. And every year, the submissions broke records. As you can see from this plot showing the annual progress and some of the milestone models, they are really incredible. But we're not satisfied since then. We have further developed models in our lab as well as other labs that can segment objects and recognize even the dynamic relationships among them in videos as shown here. But there's more to come. I remembered when I first showed the world the first computer vision algorithm that can describe images and photos in human natural language, a way to do automatic picture captioning, caption writing. That was joint work with my brilliant former student, Andre Capati. At that time, I pushed my luck and asked Andre to reverse this, give a sentence and ask computers to generate photos. And Andre just said, haha, that's impossible. Well, as you can see from this recent tweet from him, that just a mere few years later, the impossible has become possible. This is thanks to the development of recent diffusion models used in generative AI. AI programs can now take any human input sentence and create a photograph or a video of something that's entirely new. Many of you have seen the beautiful result of Sora by OpenAI and many other companies recently. But even without the enormous number of GPUs, my students and my collaborators were able to create a generative model called WALT month before Sora was released. And here are just some of the results. Of course, you can see we have room for growth. And we do make mistakes. I mean, look at that cat's eye, right? It dips underneath the water without even getting wet. I call it a catastrophe. I hope someone is making better AI jokes for me. But if the past is a prologue, we will learn from these mistakes and create a future that we imagine. And in that future, we want to take full advantage of all that AI can do. For years, I have said that taking a picture is not the same as to see and understanding it. Now I would like to add on to that. Simply seeing is not enough. Seeing is for doing and learning. When we act upon the world in 3D space and time, we learn and we learn to see and do better. Nature has created a virtuous cycle of seeing and doing powered by spatial intelligence. To illustrate what your spatial intelligence does constantly, let's look at this picture. Raise your hand if you feel like this photo want to make you do something. Keep your hand up if this has actually happened in real life. In the last split of a second, your brain looked at the geometry of the glass, the placing 3D space, its relationship with the table, the cat, and everything around it, and you predicted what will happen next. And then you will dive towards that glass to save your carpet. This urge to act is innate for beings with spatial intelligence, which links perception with action. So to advance AI beyond what is capable of today, we need more than AI that can see or talk. We need AI that can do, just like what nature did to us. And indeed, we're making exciting progress here. Our recent milestone in spatial intelligence are catalyzing that virtuous cycle of teaching computers to see, do, learn, and then see and do better. But this is not easy. It took millions of years for animals to evolve spatial intelligence. And in contrast, it took only a couple of hundred thousand years to evolve language. And that evolution depends on the eye using light to project 2D images onto the retina and the brain translating those images into 3D. Only recently, a team of computer vision researchers at Google just did that. They created an algorithm that can take just a set of photos and turn that data into 3D shape or 3D scene. And here are more examples of that work. In the meantime, my students and colleagues are inspired by this work at Stanford and went a step further and created an algorithm that only required one image to generate 3D shape, like you see here. And here are a few more examples of that recent work. And recall we previously used text input to create videos. A group of researchers at University of Michigan figured out how to translate a line of text into 3D room layout. You're seeing an example here. In the meantime, my colleagues and their students at Stanford have developed an algorithm that can take an image and generate infinitely plausible spaces for a viewer to explore. These prototypes are the first budding signs of a future possibility, one where human race captures our entire world in digital forms and is able to model the richness and nuances of our world. What nature was able to do implicitly in our individual minds, spatial intelligence AI can now hope to do in our collective conscious. As the progress of spatial intelligence accelerates, a new era in this virtual cycle is playing out before our eyes. This back and forth is catalyzing robotic learning, a critical component to early embody intelligence system that needs to directly understand and interact with the 3D world. A decade ago, ImageNet from my lab enabled a database with millions of high-quality images to help computers to learn to see. Now we're doing that with behaviors and actions that teach computers how to act in 3D world. Instead of manually creating training examples, we're now using simulation environments like the one provided by NVIDIA Omniverse, powered by 3D spatial models that offer endless varieties and interactions. You're now seeing a small set of those examples of the infinite possibilities of training our robots in simulation environments in a project that my lab has been leading called Behavior. There's also exciting progress in robotic language intelligence, combining vision and spatial intelligence. Using large language model-based inputs, my student and collaborators are among the first team to show robotic arm performing a wide range of tasks based on verbal instructions like this one, asking the robot to open a drawer but watch out for the vase, or this one to unplug cell phone. It's kind of an unusual way to unplug, but okay. And this one to make a sandwich. And, well, typically, I would like a little more on my sandwich, but this is not a bad start. So in that primordial ocean 540 million years ago, the ability to see and perceive one's environment set off a cambrian explosion of interactions with other life forms. Today, that light is starting to reach digital minds, just as it once did to our ancestors. Spatial intelligence technologies are allowing machines to interact with one another, with humans, and with the 3D world, real or imagined. With this future taking shape, we can imagine how it will have a profound impact on so many lives. Let's just take healthcare as an example. In the past decade, my lab took some of the first steps towards applying AI technology to challenges impacting patient outcome and medical staff burnout. Together with my students and colleagues at Stanford School of Medicine and partnering hospitals, we're piloting smart sensors that can detect when a clinician enters a patient room without properly washing their hands. Keep track of instruments during surgery or alert care team when a patient is at physical risk, such as falling. We consider these technologies to be forms of ambient intelligence, and these extra pairs of eyes can make a difference. But I would love to see more interactive help for patients, clinicians, and caregivers who also desperately need an extra pair of hands. Imagine autonomous robots transporting medical supplies so that caregivers can have more quality time with our patients, or augmented reality guiding surgeons towards safer, more efficient, and less invasive operations. Imagine patients with severe paralysis controlling robots with their thoughts. That's right, with brainwaves, so that they can do everyday tasks that you and I take for granted. You're actually seeing a glimpse of that future now. In this pilot study from my lab, as you can see in the video, here a robot arm is cooking a Japanese sukiyaki meal controlled only by brain electrical signal. None-invasively collected through EEG caps, so no chips or no electrodes were inserted into the person's brain. This entire robotic action is done by remote brain control. Thank you. Half a billion years ago, the emergence of vision not only turned a world of darkness upside down, it also kicked off the most profound evolutionary process, the development of intelligence in the animal world. AI's breathtaking progress in the last decade is just as astounding. But the true digital Cambrian explosion won't realize its fullest potential until computers and robots have developed the kind of spatial intelligence that nature has endowed to all of us. It's now time to train our digital companions to learn how to reason and interact with this incredible 3D space we call home, and to create many new worlds for all of us to explore. Realizing this future won't be easy, and it will require all of us taking thoughtful steps to develop technology that can always put humans in the center. If done right, computers and robots powered by spatial intelligence will not only be useful tools, but they can also be trusted partners that can augment and enhance our productivity and humanity while respecting our individual dignity and lifting our collective prosperity. What excites me the most is a future in which, as AI grows ever more perceptive, insightful, and spatially aware, it joins us in our quest to satisfy our curiosity, to always pursue a better way so we can make a better world. Thank you. Thank you.