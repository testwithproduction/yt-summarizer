# Summary

The speaker begins by referencing the historical roots of the conference, which originally started as Spark Summit or Spark AI Summit, focusing on Apache Spark. Three years ago, attendees were polled regarding the challenges with Spark, identifying three major issues:

1. **Python Integration**: While Spark is natively written in Scala, there's a larger community of Python users finding Spark clunky and hard to work with in Python. The Apache Spark community launched the Project Zen initiative to make Python a first-class language, enhancing API, error messages, debug-ability, and performance. As of now, Python is significantly more integrated and preferred for Spark, with Spark downloads in over 200 countries and massive query usage.

2. **Dependency Management and Version Upgrades**: Users face difficulties with managing dependencies and upgrading Spark versions due to the monolithic architectural design where applications and Spark run in a single process. Changes require all components to be upgraded simultaneously.

3. **Interoperability Outside JVM**: Language framework developers struggle with Spark's tight JVM dependency, making it tough to work with Spark outside of JVM.

To address these issues:

- **Spark Connect Introduction**: Spark Connect is a novel approach, facilitating a language-agnostic binding for Spark, thus decoupling applications from Sparkâ€™s core process. This ensures:
  - Easier upgrades where server-side can be upgraded independently from applications.
  - Better debugging as individual applications can be debugged separately.
  - Simplified language support, enabling the creation of plugins for new languages independently.

Finally, the speaker introduces **Tarif from POSIT** to talk about SparklyR, emphasizing POSIT's influential role in creating leading open-source projects such as Dplyr, GG plot, R studio, and collaborations on Apache Arrow that significantly impact data science.

# Transcription

 All right. Thank you, Ali, for that number three speech. Good morning again. So, as many of you know, this conference actually started previously named as the Spark Summit, or Spark NAA Summit. And this talk will be going back to the roots of the original conference, Apache Spark. So, three years ago at this conference, we pulled 100 of you and we asked, what were the biggest challenges you had with Apache Spark? All right. So, about 100 of you. And here's what the 100 of you told us. By far the number one was, hey, I have a bunch of scholar users, they're in love with Spark. It's great. But they also have a whole bunch of Python users out here. As a matter of fact, there's way more of them. And they really don't get Spark. Spark is kind of clunky, it's difficult to use in Python. It's not a great tool for them. And the number two is everybody else who said, hey, I love Spark, I've been using it. I'm using Scala also. But dependency management or my Spark application is a nightmare. And version upgrades take six months, one year, three years, you name it. And then there's a consensus among the language framework developers out there, not a huge population, but a very important component of the Apache Spark community would tell us, hey, because of that tight JVM language sort of nature of Spark, it's very, very difficult to interact with Spark outside of JVM as a framework developer, not just as an end user. So, we got to work. And let's talk about the first one. Spark is Scala, but my user is only part of Python. If you've been to this conference in the past, you know this is not the first time we're talking about Python. But I found this video from about three years ago, just the other day, as I was preparing the talk. And it's from Zach Wilson, who used to be a data engineer at Airbnb. And here's what Zach has to say. Another one is Spark is actually native in Scala. So writing Spark jobs in Scala is the native way of writing it. So that's the way that Spark is the most likely to understand your job, and it's not going to be as buggy. So, Zach, Scala, I believe Zach is actually somewhere sitting here, but Scala is the native way of writing Spark, and writing is not as buggy. So it's not just the people at this conference saying that. We got to work three years ago at this conference. I think it might have still been named Spark and AI something back then, and the theme of all the slides were white background instead of dark background. We talked about the Project Zen initiative by the Apache Spark community, and it really focused on the holistic approach to make Python the first class citizen. And this includes API changes, including better error messages, debugability, performance improvements, you name it. It comes with almost every single aspect of the development experience. 2022, two years earlier, we gave a progress report and talked about all the different improvements that were done in those two Spark releases. And last year, we showed a concrete example of how much auto-complete have changed, just out of the box, from Spark 2 all the way to Spark 3.4. So, in this slide, summarizes a lot of the key important features for PySpark in Spark 3 and Spark 4. And if you look at them, it really tells you that Python is no longer just a botan on the Spark, but rather a first-class language. And there's actually many Python features that are not even available. They're Python-made, Python idiomatic, they're not available in Scala. For example, you could define Python, you should define table functions these days and use that to connect to arbitrary data sources. This is actually a much harder thing to do in Scala. At this conference alone this year, we have more than eight talks talking about various features of just PySpark itself. So, a lot of work have gone into it, but how much benefit are the users seeing? Again, this is one of those moments that I can tell you nonstop about it, but it's the best, you try it out yourself. It's actually a completely different language. When you look at the last 12 months alone, PySpark has been downloaded by over 200 countries and regions in the world, just according to PyPy stats. I was doing some number analysis the other day, I was really surprised to find this number. Just on Databricks alone for Spark versions 3.3 and above, so it does not include any of the earlier Spark versions, which is a lot of them out there, but just for Spark 3.3 versions above on Databricks, our customers have run more than five billion queries every day. To give you a sense of that scale, I think the leading cloud data warehouse runs about five billion queries a day on SQL. This is actually matching that number. It's only a portion, a small portion of the overall PySpark workloads. But the coolest thing was, as I found the earlier video from Zach, in which he said, hey, Scala is the native way of doing it, I found another video he published just about three months ago. By the way, I've never met Zach until like last week when I reached out to him, would you be okay for me to show you the video? But let me play you this video from this year by Zach. But things have changed in the data engineering space. The Spark community has gotten a lot better at about supporting Python. So if you are using Spark 3, the differences between PySpark and ScalaSpark and Spark 3 are there really isn't very much difference at all. So thank you for the endorsement from Zach. So if your impression of Spark was, hey, Spark is written natively in Scala, that's still true. We love Scala. But if your impression is, hey, if I'm really using Python, I would get super crazy JVM stack traces, I would get terrible error messages, the APIs are not idiomatic, try that again. It looks completely different from three years ago. And of course, the job is never done. We will continue working on improving Python for Spark. But I think it's fairly reasonable to declare, hey, Python is the first class language of Spark. So I will talk about the other two prompts, version upgrades, dependency management, and JVM language. Now let me dive into a little bit more about why this problem exists. So the way Spark is designed is that all of the Spark application you're right, your ETL pipelines, your data science analysis tools, your notebook logic that's running, I'm running a single monolithic process called a driver that includes all the core server-sites of Spark as well. So all of the applications actually don't run on whatever clients or servers they independently run on, right, running the same monolithic server cluster. And this is really sort of the essence of the problem because, one, all of this because they all run in the same process, the applications have to share the same dependency. And not only do they share the same dependencies to each other, they share the same dependency as Spark itself. Debugging is difficult because in order to attach a debugger, you have to attach the very process that runs all of these things. And now, last but not least, if you want to upgrade Spark, you have to upgrade the server and you have to upgrade every single application running on the server in one shot. It's all nothing. And this is a very difficult thing to do when they're all tightly coupled. So two years ago at this very conference, Martin and I introduced to you Spark Connect. The idea of Spark Connect is, again, very, very simple at a high level, we want to take the data frame and SQL API of Spark that's either Python or Scala centric and create a language-agnostic binding for it based on GRPC and Apache Arrow. And this sounds like a very small change because it's just introducing a new language binding and a new API, an electric-agnostic, but really it's the largest architectural change to Spark since the introduction of data frames APIs themselves. And with this language-agnostic API, now everything else runs as clients connecting to the language-agnostic API. So we're breaking down that monolith into, you can think of it as microservices running everywhere. And how does that impact end-to-end applications? Well, different applications now will actually run as clients connecting to the server, but they are really clients that are running in their own isolated environment. And this makes upgrades super easy because the language binding is designed to be language-agnostic and for and backward compatible from API perspective. So you could actually upgrade the Spark server side, say from Spark 3.5 to Spark 4.0 without upgrading any of the individual applications themselves. And then you can upgrade applications one by one as you like at your own pace. Same thing with debugability, now you can attach the debugger to that individual application that's running in a separate process anywhere you want without impacting the server, without impacting the rest of the applications. Now, for all of the language developers out there, this language-agnostic API makes it substantially easier to be building new languages. Just in the last few months alone, we've seen community projects that build goal bindings, Rust bindings, Shishop bindings, all of this. And it can be built entirely outside the project with their own release cadence. So one of the most popular programming languages, probably the top two programming language for data science are R and Python. Spark has built-in Python support, there's also built-in R support called SparkR. But the most popular R programming library for Spark is not the built-in SparkR. It's a separate project called SparklyR. SparklyR is made by this company called POSIT, which is actually I was talking to the POSIT folks behind the stage and I told them, hey, I think POSIT is the coolest open source company audience I've never heard of. And the reason you have not heard of it is they renamed themselves fairly recently to POSIT. But the people at POSIT created the most foundational open source projects. For example, Dplyr, the very project that defined the grandma for data frames that we're all enjoying today. GG plot, the grandma visualization, R studio, the most popular R IDE. Wes McKinney who created Panda works at POSIT. And also Apache Arrow. So I would actually like to welcome Tarif, president of POSIT on to stage to talk to you more about SparklyR.