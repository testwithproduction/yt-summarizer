# Summary

Summary of the Technical Talk:

The technical talk, spearheaded by Databricks co-founder and CEO Ali Ghodsi, centered around the significance of data and AI, providing updates and exciting announcements about Databricks.

**Key Points:**

1. **Event Scope and Participation:**
   - The event is a large global gathering with 60,000 virtual attendees and 16,000 in-person attendees from 140 countries.
   - Participants had access to 600 training sessions and there were presentations from over 200 teams.
   - There are 143 exhibitors showcasing products in the Expo hall.

2. **Open Source Contributions by Databricks:**
   - Databricks has heavily contributed to open source projects: Spark (over 1 billion downloads), Delta Lake (over 1 billion downloads), and MLflow (over 200,000 million downloads).
   - Employee contributions amount to 12 million lines of open source code.

3. **Key Areas of Focus for Databricks:**
   - Emphasis on democratizing data and AI, making it accessible to every company.
   - Addressing three major challenges faced by organizations: AI implementation, security and privacy, and data fragmentation.

4. **Challenges in AI Implementation:**
   - Companies face difficulties in leveraging generative AI (Gen AI) due to fragmented data estates and AI security concerns.
   - Only 15% of Gen AI use cases have made it into production.

5. **Technological Solutions and Strategies:**
   - Introduction of the Lakehouse architecture:
     - Delta Lake and Apache Iceberg for data storage with high compatibility and no vendor lock-in.
     - Unity Catalog for governance, now being open-sourced to the community.
   - Mosaic AI Platform:
     - Integrating AI and data intelligence for training customized AI models on enterprise data.

6. **Product Launches and Announcements:**
   - Databricks announced the availability of Databricks in a serverless fashion for all infrastructures starting July 1.
   - The rollout of AI-enhanced features like Databricks Assist, AI functions, and vector search in Databricks SQL.
   - Introduction of AIBI (AI Business Intelligence) and Genie as an AI-centric BI tool enabling conversational queries and continuous learning from enterprise data.

7. **Collaboration with NVIDIA:**
   - A partnership to integrate AI and accelerated computing.
   - Announcement of optimizing photon query engine using NVIDIA GPUs for faster and more efficient data processing.

8. **Customer Success Stories:**
   - Examples included General Motors leveraging Databricks to build better cars and Block using Databricks to integrate AI across decentralized business units.

9. **Future Vision and Research Directions:**
   - Prof. Fei-Fei Li highlighted research on spatial intelligence and its applications in robotics and healthcare.
   - Emphasis on a world where AI can see, understand, and act upon the 3D world.

10. **Culmination and Motivation:**
   - The talk focused on the potential of data intelligence platform in unlocking new opportunities and advancing innovation.
   - Jensen Huang from NVIDIA underscored the importance of integrating AI in enterprise strategies for data processing.

Overall, the talk highlighted Databricks' commitment to enhancing data and AI capabilities through innovative product offerings and strategic collaborations while emphasizing a vision of widespread AI integration and data democratization.

# Transcription

 Welcome to the stage Databricks co-founder and CEO Arligotzi. Hi, everybody. Super excited to be here. This is my favorite week every year. 52 weeks, this is the favorite one. Oh, wow. Lots of people still coming in. All right. So we are super excited to welcome here, everyone. This is a global event. In fact, I think this is the largest data and AI gathering in the world right now. And we are super excited to have you here. Thank you for coming. Okay, so let's look at some stats. These stats are awesome. So we have over 60,000 people worldwide. Actually, watching, we have 16,000 people here. In fact, we have taken over all of Moscone, you know, North, Southwest, all of it. We have 140 countries represented. That's more countries that I will ever visit, probably. And I don't know. Maybe some of you have visited that many, but we have 600 training sessions. So lots of people are getting training in data and AI yesterday, today. And that's super, super awesome. Over 200 teams are flying in. They're nervous. They're going to present what they've been working on all year. You know, all eyes are on data and AI, right? So everybody wants to know, what are the use cases? Is this stuff working? Is it in production? So it's going to be super awesome. You're going to listen to these talks. There's 143 exhibitors. Please go to the Expo hall and check out what they're doing. They have a lot of stuff, a lot of products. So I'm going to go check it out as well. Super excited about that. Also, this community has contributed to a lot of open source projects and to open source. So this year, we're 11 years in to the Spark project. And we have over a billion downloads now a year. And same thing with the Delta Lake project. Over a billion downloads a year. And MLflow, which is a machine learning, gen AI ops platform, has now over 200 million downloads. And then we wanted to run a fun stats. We wanted to check how many lines of code has Databricks employees contributed to open source. And you can see here, it's 12 million, actually. This one was hard to compute. It took us a long time. Initially, we thought we contributed 100 million lines. But then we ran the stats carefully. It turns out a lot of it is just generated code. Okay. So we have to figure that out. So just put it in perspective. I think the whole Android project is about 3 million lines of code. Okay. So this is obviously, we're all in on open. And if you're all in on open source and open, partners are super important. The partner ecosystem around those open source projects, the companies around it are super important. So I want to thank all of our partners and all of our sponsors. Please check them out in the expo hall. We have, you can see here, we have all the hyperscalers. We have all the global size partnering here. But also lots of ISVs that are present. So check them out. Okay. So we have awesome lineup of speakers. So we're going to see talks from DuckDB creator. We're going to hear from Professor Fei-Fei Li. We're going to hear from the one and only, Jen St. Huang. Professor Jayjin will be talking about small language models. We're going to hear from Texas Rangers. We're going to hear from General Motors, Block. We're going to hear from Ryan Blue, the original creator of Apache Iceberg. We're going to hear from Teri from Posit, or company formerly known as RStudio. And then we have lots of lots of announcements from Databricks speakers. Okay. So in my keynote here, I just want to give you guys the vision of what we think we can do with the Databricks platform. I'm not going to have that many announcements. And then afterwards, we're going to hear talks from different bricksters. We'll get on stage here. And most of the announcements will be there. Okay. So we've said this for a long time. Every company now on the planet wants to be a data and AI company. Okay. So in the last 18 months, every CIO, even every CEO I meet of a Fortune 500 company or a small company, think that data and AI is going to be super strategic for them over the next five years. They think that that's how they're going to win. Okay. That's going to be the main differentiating factor is how they leverage data and AI, whether they're in financial sector or they're in retail or they're in media or they're in healthcare or in the public sector. Doesn't matter. All of it, it's going to be data and AI. Okay. And you know, at Databricks, this is why our mission since day one has been to democratize data and AI. Even when we were researchers back at UC Berkeley, we wanted to take this kind of technology and help everyone in the world to use it. Back then, 10, 15 years ago, if you were an organization, you would have to hire 10,000 engineers and build a full data and AI stack in-house. That's what Uber was doing. That's what Twitter was doing. That's what Airbnb was doing. But these days, you can actually just leverage these platforms and, you know, drive business and you can get a lot of value for your organization. But in the last 18 months, there's been a lot of pressure. Can you deliver actual use cases? Can you actually get into production? Can we really make sure that data and AI has value? And I've been talking to lots of leaders, lots of practitioners, and I keep hearing the same thing again and again and again. Okay. There's three problems that come up when I talk to folks. Okay. So what are those three problems? Everybody wants to do AI. Okay. It's like top of the top. It's coming from the board. There's a food fight inside organizations who owns Gen AI. So that's number one. Number two, everybody's worried about security and privacy of their data and of their Gen AI. Okay. And their whole data state, actually. They are worried about security and privacy for the whole data state. And that data state today is super fragmented. Okay. So, you know, it has a lot of issues. So let's double click on each of these three and go through what these problems are. And then I'm going to tell you about the Databricks platform and what we think is the path forward for solving these three. Okay. So let's start with the first one. Everybody wants Gen AI. Everybody wants AI in the organization, the one at yesterday. And it's been an amazing year. Actually, you can see here, this is a benchmark. This benchmark is probably the most popular benchmark. It's called MMLU, massive multi-language understanding. It's got like 72 categories of different things testing you on biology, history, you name it. And you can see that the AI models, the large language models are getting better and better. See, it's almost saturating up close to 100 score. And you can't go higher than that. But it's also been an awesome year for open source. So we're seeing that open source models are catching up. Okay. You can see DVRX there by Databricks, which was the best model in the world for two whole weeks. And... Thank you, Mark Zuckerberg, for releasing that model two weeks later. And then Lama 3. By the way, Lama 3, we haven't even seen the largest model yet being released. Okay. So it's safe to say open source catching up very, very fast. Okay. So this is awesome. These benchmarks are awesome. The results are breathtaking. But actually, the problem is when I talk to organizations, this is a, you know, actual quote, I don't care about standard benchmarks. You know, I want the model to do well on my data and my use case at my organization. I don't care, you know, if it's doing well on MMLU or not. That doesn't help my company. Okay. It doesn't help me succeed with my use case. So this is the number one thing. Okay. People don't know how these models are doing and for their use cases. And that's all they care about. The other thing is we did a survey of our customers and 85% of the use cases have not yet made it into production. The gen AI use cases. Okay. So they're still sort of trying them out. They're trying to make sure that they're ready to be shipped. But they're not quite in production. To summarize, the AI problem is how do you do gen AI on your data in your organization and how do you get it into production? Okay. And how do you do that while making sure that it has really high quality? So it's really doing the tasks that you have really well. Two, how do you make sure that it's doing that at a good cost? Right. We don't want the cost to be prohibitive. And three, how do you make sure that we can ensure privacy on that AI? So this slide summarizes what we're seeing in the gen AI space, the problems that people are struggling with. Okay. And when it comes to privacy, that's a whole other concern. Okay. So people are super worried about security and privacy of their AI. We're seeing intense pressure. There's talks of AI regulation. There's talks of even, you know, maybe they're going to ban open source models. People are worried about data privacy. But also, not just AI, the data is under attack. There's cyber attacks coming into data platforms. You know, people are trying to break into their companies. So people really want to make sure that they are secure. Okay. And it's not just security of AI. It's all of the data state. It's all of the raw data. It's all the structured data on structured data, the AI models, notebooks, dashboards, anything that's in your organization. This is kind of slowing things down and people are super cautious and super nervous. Okay. They want to make sure that their data state is secure. Okay. And then third, the data state is super fragmented. This is the number one thing I hear. Like every call I get on, people talk about the fragmentation of the data state. They basically say, typically, if I talk to a CIO, almost every call I have sounds like this. We have so many different pieces of software. I don't even know what they do. We have one of each, you know, it's, you know, I don't even know, but we have to cut it down. I'm under budget pressure. In fact, I don't just, it doesn't even look like this. I have many of each. Okay. I have many data warehouses. I have, you know, many data science platforms. I have data siloed everywhere. And consequence of this is lots of complexity, huge costs, and then lock into these proprietary different systems. Each of those systems is a little silo that you lock yourself into. Okay. So these are the three problems that we really, as a company, are focused on trying to address. At least we're trying to somehow help move things forward when it comes to these three. So let me walk you through how we do that. So at Databricks, we call this the data intelligence platform. And our vision starts with what we call the lake house. So let me walk you through that. This idea we had this about five years ago, which is, and we actually announced it at this conference, which is stop giving your data to vendors. Okay. They'll just lock you in. Stop giving your data to any vendors. Okay. It doesn't matter if it's a proprietary data warehouse in the cloud or if it's, you know, snowflake or if it's even Databricks. Don't give it to us either. Don't give your data to us. Don't give it, don't trust vendors. Don't give your data. They'll lock it in. They'll raise the price after a few years. And you have another silo on that picture now that you have to deal with. And you just keep adding these silos over the years. Okay. So what should you do instead? You should instead own your own data. You should have your own data. You should store it. And cheap hard drives in the cloud called data lakes. And just store them there. Pay for it independently. Make sure it has separated compute and storage completely from the compute. So it's just a basic data lake, right? Like S3, ADLS, you know, GCS. And but when you need to store it in a format, that's standard. So think of it as USB. So that's why we announced Open Source Delta Lake project here, I think, a bunch of years ago, maybe half a decade ago. And the idea is, okay, so then we have this USB format. And once we have this USB format, anyone can just plug in their data platform. Any of those vendors that I said don't give your data to, they should just plug in their USB stick into that data that you have in the cloud. And then let the best engine win. Let's see who's best, right? Maybe this week it's us. Maybe next week it's someone else. This brings disruption and removes the slot in and also reduces the cost and also lets you get many more use cases. Because you can use different engines for different purposes if you want. So this was our vision. Unfortunately, what happened is we almost succeeded and people are bought in. Okay, everybody here in the crowd wants this. They're like, we want this. We want to own our data. We want in a standardized format. But unfortunately, there's this like fragmentation. So there's now two camps. Okay. On Databricks, we have Delta Lake. We're seeing actually 92% of all of our data go to Delta. That's about four exabytes of data every day. So 4,000 petabytes every day that's processed going to Delta. But there's lots of us other vendors that are using this Apache Iceberg format, which is another format. Okay. So last week we announced that we're acquiring Tabular, which is... Thank you. It's a week old news, you know. And we announced that we're acquiring Tabular. Tabular was founded by the original creators of the Apache Iceberg project. Okay. So Ryan Blue and Dan Weeks started the Apache Iceberg project and we're working at Netflix. And we acquired this company. So I want to tell you a little bit about why did we do that, what's our thinking around that. Okay. So the reason we did this is that we want this problem to go away so that you don't have to pick which of the two silos do I have to... Which of the USB formats do I have to store this in? If I store it in this USB format, you know, these cables won't be able to plug in. If I store it in that one, the other engines can't plug in. We don't want it to be that way. Okay. Whatever you store it in, all the cables should just work. Okay. We just want very simple standard for everything. So our strategy is a year ago here we announced Project Uniform. Project Uniform is part of Delta and we're actually announcing it being GA this week here. And Uniform, it translates to both of these formats, Delta and Iceberg. It's already doing that today and it's GA. But, you know, we really understand Delta really well. We don't understand all the intricacies of the Iceberg format. But the original creators of Apache Iceberg, they do. So now at Databricks, we have the employees from both of these projects, from Delta and Iceberg. So we really, really want to double down on making sure that Uniform has full 100% compatibility and interoperability for both of those. So if you put your data in Uniform today, it should be a no-brainer, it should just work. Then in the background, what we want to do is we want to really work with these communities. The Delta Lake community and the Apache Iceberg community, these are open source communities, right, with people around, you know, all around the world. They're governed by these foundations, like the Apache Software Foundation and the Linux Foundation. We want to work with them and actually change the formats and bring them closer and closer to each other so that the differences between them do not matter. Okay. So, to store your data in Uniform right now, then over the next, you know, period of time, as the formats get closer and closer, it won't even matter which one you have. Like, the distinction will go away and I hope that in a year or two, we won't even care here about which one. It won't, it will be like VHS and Betamax, you know, who cares. Okay. So that's our strategy with respect to the data format. So we hope that we have just the USB format, it shouldn't matter, all the formats should be supported, and it should just work. Okay. So that's problem number one. Remember the fragmentation, your data is locked in, now the data is just sitting in a lake in a USB format, any engine should be able to access it. Okay. Remember the second problem, governance, security. How do I make sure that this stuff is super secure? So when it comes to that, we announced here Unity Catalog a few years back, and Unity Catalog is probably the most important development at Databricks since we started the company. It's probably the main reason people actually come and use Databricks these days. It's because Unity Catalog lets you do governance, not just for your tables, not just, you know, for your unstructured and raw files, but for all of your data state, including machine learning models, AI models, you name it. And it's not just access control and security, it's also discovery, it's also lineage. It's also being able to do auditing and data quality monitoring or AI model quality monitoring. Okay. So it's super, super important. So that's what that looks like. So now we have Delta, and then we have for governance, Unity Catalog, and I'm super excited to announce that we're also open sourcing Unity Catalog this week here. So please go to Matej's talk tomorrow on Unity Catalog, and he's going to cover all the interesting things that go into it, and we'll see him open source the project. So that's the vision of the lake house. Okay. So basically, all the silos that you had before, they can just access one copy of the data that's in a standardized USB format under your ownership, okay, in a lake like GCS or S3 or ADLS, and it goes through one governance layer that's just standardized, that's Unity Catalog for all of your data state, okay, and that's also open, and also that Unity Catalog open sourcing, one thing that's really important about it is that it supports two APIs that already are standard for governance, Hive Metastore API and Iceberg Rest Catalog API. So that's already supported in Unity Catalog. In fact, I think Tabular and Database are the only ones that support that Rest API Catalog. So Unity Catalog will also just be building on APIs that are already prevalent and everybody's using. So we basically standardize the data layer and the security layer, and you own your data, and everything goes through these open interfaces, and I think that's going to be awesome for the community, for everybody in here, because we're just going to have way more use cases, we're going to be able to do much more innovation, and we'll just expand this market for everybody involved, it's just going to be great. Okay, so that's the lake house, but I said data intelligence, so what is that? So what we're really excited about as a company moving forward is when you take this lake house that supports all of your data and all of your governance, and you combine it with generative AI, in particular last year on stage here, we announced the acquisition of Mosaic AI, and when you combine Mosaic AI, which was a platform to train custom AI's on your data, we combine Mosaic AI with the lake house platform, you get what we call data intelligence. Okay, so what is data intelligence? Data intelligence means that our platform trains generative AI models on your data in isolation for each customer, and leverages that throughout the platform for everything it does. Okay, so what does that mean? That sounds like a multiple, so what do we want to do? What is data intelligence? Well data intelligence for us is really, we want to apply it to two things. One, we want to democratize data, and second, we want to democratize AI. Okay, what is democratized data? What is democratized AI? Sounds very similar. Democratizing data means that anyone in your organization should be able to access the data directly. Okay, today that's not true, your CEO is not going to go access the data and ask questions from the data. He or she will go to the data team and ask them, hey, can you get me this report, and they'll say, by when do you need it, and then they're going to work on it, because your CEO does not speak SQL or Python, or at least doesn't know where to find the data and submit their own queries. So we're really hoping that we can democratize this, that if you speak English or any other natural language, you should just be able to ask your question from the data, and many, many more people in the organization should be able to get insights from that data. So we're very excited about that. Democratizing AI is different. Democratizing AI means practitioners like yourselves here in the room should be able to really easily create AI models that understand your data in your organization. That's what democratizing AI is, and we want to do both of these two things. Okay, so let's start with the first one. How do we actually make you talk to your data? Let anyone talk to their data. Okay, so this is what data intelligence is. We want you to be able to ask, how's the business doing on its FY goals? And we want the platform to be able to understand what that means. FY stands for fiscal year. In the particular company you're at, fiscal year might start 1st of February, or maybe 1st of July, it should know that. And business in your particular company means certain KPIs that are most important in your organization, the definition of those, it should understand those, and then we want it to give you back authority answers that are certified and that are correct, and they don't have any hallucinations, and we can actually verify that these are correct. That's what data intelligence is for us. Okay, so that's what we want to do. That's our vision. That's what the whole company is working on. Thank you. That's just a fake screenshot. It's not an actual product, okay? But we have actual real demos live, but I'm not going to take the risk anymore. Okay. Too nervous. Okay. And then if you click on a random table, data set in your organization, chances are you get something like this that's completely hard to understand. What does this even mean? TBHINCL underscore S underscore key. This is why your CEO is not asking questions from your data. Who knows what this means? Well, we did intelligence actually already today. We can fill out, so in Unity Catalog, we already fill out in English text what all your data sets do using generative AI. So we fill it out like this. It describes exactly in English what all the data assets are doing, and when we have that, then we can actually do proper search. So when someone comes in and says, where do I find shipping information for home goods, it knows that, okay, you're looking for this particular data set over there because it has all those descriptions. Okay. So that's data intelligence. That's what we're working on to make happen. And then I do want to put a big plug for something that's already in Databricks today, which is the assistant that we've trained using dbrx. We fine tuned it on all of Databricks documentation, all the errors that we've seen since we started Databricks. And you can ask it today to do pretty advanced stuff. You can say, hey, write code for me that, you know, does the streaming thing, you know, write your code like this. Okay. It just works. I use this every day. Okay. So this has made me much more productive. I'm getting a little bit rusty with my coding. But just work with assistant and it gets things right. I can even write half pseudo, you know, kind of looking right code, but no compiler would ever accept it. But the system will just fix it like this. It will just fix things for me. This is used over 100,000 of users every day use it in Databricks platform. So I do think this already is democratizing access to data and more and more people are able to talk to the data, ask questions from it. Okay. So that's what data intelligence is when it comes to talking to your data. Okay. What about democratizing AI? When it comes to democratizing AI, that's where our whole AI, generative AI stack comes in. And we're going to hear in, you know, two talks, you're going to hear the details of this and all the announcements that we have around this. But I'll just give you the high-level overview. In Mosaic AI, we basically have all the serverless GPUs around the world and we just enable you to very easily, seamlessly in a UI, be able to build your own AI on your custom data and productionize it and evaluate it. Okay. So the first step is preparing your data so that it's AI ready. So the platform is great for that. Second step is how do you train or how do you fine tune, whether you want to use some fancy techniques like Dora or you want to continuously pre-train the whole model or, you know, you want to use a vector search or whatever you want to do, you can build your AI there. Then you can deploy it in production. Deploying it in production means you're running it on our GPUs in, you know, different countries. Just in a serverless manner, they're just ready to go. We're paying a lot for those GPUs today. My CFO reminds me every week. And that's where we can run these. You can run our vector search database there. And then we have evaluation. We're going to talk a lot about the evaluation here. How do you actually know your AI is doing well in production? And then, of course, finally, really important for most people in this room, how do you govern it? So how do we actually secure it? How do you make sure that we can track it, rate limit it, track the tokens, you know, make sure it's not doing something that we don't want it to do? So that's the Mosaic AI. Okay. So we'll hear from my co-founder Patrick Wendell who will talk about this in detail. Okay. So that's the full platform of Databricks. So that's what we get, the data intelligence platform. You have unit catalog, you have Delta, you have all these things accessing it. Okay. One thing, one last thing I want to announce here is that we are also super excited to announce that all of Databricks now is available in serverless. Okay. So first of July, you'll be able to get everything in Databricks. Okay. So whether it's our notebooks, or whether it's our spark clusters, or whether it's, you know, workflows, job processing, all the different aspects. Databricks so far are only a few parts of it with serverless. Now you get all of it in a serverless fashion. This is a project that has involved hundreds of hundreds of engineers for over two years. It's been a long project internally. Two, three years ago, my co-founder Matei and I told the company we got to build a lift and shift simple version of serverless. And actually our engineers pushed back and said, hey, you guys are wrong. We should redesign it from scratch for the serverless era. And we told them, nope, we decide, you know, in the company. And turn out we were wrong. And the tech leads were right. And they've been working really hard for two years. So we basically redesigned many of the products, you know, the notebooks, the jobs, everything, as if we had started a new company. What would that look like in the serverless era? Okay. So how do we make sure that it instantaneously comes up? There's no more clusters. Everything just works super, super fast. Under the hood, we make sure that we multiply the resources. In fact, today, you're paying us for idle time if you're not using serverless. Actually, you're paying the cloud vendors a lot of money. And then you're paying us, in addition to that, for idle time. With serverless, you know, you're just paying for what you're using. In fact, there is no cluster to set up for it to be idle or not be idle. Okay. So we'll take care of all that for you under the hood. And one thing that we're excited about this, since we own all the machines now, it's no longer this, you know, joint responsibility over machines that are running in your account and in our account. We're able to really redesign so that this year, we'll be rolling a disaster recovery in a different way that's really custom for serverless. Cost control, so you can really do the tracking, and you can do the tagging, and you can really use AI to predict where your costs are going on the serverless infrastructure. And we're also able to do security in a different way because, again, we own all the machines, so we're able to really lock it down in a different way that's not possible when it's not serverless. So we're very excited about this. You know, all these knobs that we had before are gone, right? Cluster tuning. You have people setting up clusters. What type of machines should they use? Spot instances. This and that. Should it auto scale? None of that is available anymore. It's just gone. There's no such page. You can't do that. The data layout. How are you going to set out exactly your data sets? How are you going to optimize your data sets? That's also gone. Okay. We're just optimizing it behind the scenes because it's serverless. We're just running the background optimization on your data sets to make it really fast and optimal using machine learning. So that's also really awesome. Capacity planning, usage tracking, and then my favorite thing is no more versions. Okay. So there'll be no Spark versions when you're using this serverless edition. You can't pick a version of Spark anymore, so you don't have to worry about DBR upgrades and those kind of things. So we are super excited about this. Please start using serverless. And in the future, new products that we roll out, like next year when I'm here on stage, they'll probably only be available in serverless. So if your organization is not on serverless, please get on it. We're rolling out serverless infrastructure all around the world to make sure that we have availability in any region that's near you. Okay. Whichever country you're listening in from right now or representing where you flew in from, we'll hopefully have a serverless infrastructure near you there. Okay. All right. So that is the Data Intelligence Platform. Okay. And today, we have over 12,000 customers that are actually leveraging this platform. But enough hearing from me. I want you to welcome on stage Brian from General Motors, who's going to tell us how they leverage the Data Intelligence Platform to build better cars. Hey, Databricks community. Good to be here with you. And Databricks, thanks for giving us a few minutes to share our story. The GM mission, zero crashes, zero missions, zero congestion. To some people, words on a slide to others like me deeply personal, you see, I know a six-year-old that was killed in a car crash. So this future cannot come fast enough for me. But is it strategic? Yes. Customers increasingly want a vehicle that thinks about their safety, security, and comfort. So the company that figures these things out will increasingly write the rule book of the future, and GM wants to be that firm. But there's a trick. To get there, this 115-year-old company that is a storied legacy and hardware must become a software company. No easy trick. So where do we start? GM has a ton of data. That's not the problem. We had a beautiful on-prem infrastructure. Why change? Well, two reasons. Number one was data efficiency. GM did a time in motion study, and we determined that every year we spent 200-person years in the hunting and gathering of data. Now, don't quote me on that. We've made a lot of progress since that study was run, but you get the order of magnitude of the problem. But more importantly, in the last few years, the world changed. And GM understood that if we didn't have AI and ML in our arsenal, we could find ourselves at a competitive disadvantage. So we needed to transform. And about 15 months ago, GM decided if we're going to change the future, we must change ourselves today. So how do we go about doing it? Well, we were going to be all about the cloud, which meant that we needed to shift our culture safety first, safety first, safety first. That was immutable rule, but underneath that, it was all build our mindset. We were going to get in there. We were going to build things, and we were going to learn and grow, and that was going to be how we were going to approach it. We would find patterns that worked in the cloud, and we would build an insight factory. That would be the best in the West Coast approach, the best of the cloud, and we'd print that out into a blueprint that we would share with others at GM. And to do that, we decided we would, sorry, I'm catching up, build it all on Databricks. Why? Because we had totally aligned interests. We needed to move from data and solution silos to single sources of truth with rapid collaboration. We needed to move away from fragmented governance into simple unified governance, and we felt if we did those two things extremely well, that we'd be able to go from pockets of limited AI and ML execution in GM, have some wonderful things, to really building AI and ML into the DNA of GM. If we could do that, maybe we would change GM forever. So this is what the insight factory looks like today. I'm super proud of the work of the team. It's an end-to-end system with a React UI that allows us to control all of the data. We define the quality and the medallion process so we can see every hop and no hop missed. We control the strategic IP that goes in there because we should own it. That's our competitive advantage. We can refine quickly and present insights in a beautiful GM-branded React interface. But I have to admit, building this isn't easy. A system like this, super complex, it's dynamic, everything's changing every day, and on top of that, you make mistakes. So we pushed ourselves to our limit and sometimes beyond, but we built it off of scar tissue and grit and it took a village. But the good news is we got here in a record time. So imagine going from almost nowhere on this to having this entire system in maybe nine months. Here's some good news for us to share. Unity Catalog basically worked out of the box for us, got us in the game, and the open ecosystem allowed us to have a low friction way to go after that hunt and gather stat I talked about before, using a Muda for policy compression and Atlin for end-to-end visibility from the cloud all the way back to our on-prem. Two lessons learned along the way, managing a node in the mesh takes a lot of technical talent. So when things like serverless and Lakehouse apps come along, it really helps us with our ability to scale. Beta Bricks community, please keep those coming. They're really being put to good use. But the other thing that we learned is GM has a lot of very smart and talented people already. They've built AI and ML, but it's in these little pockets and in these silos. So something like ML flow that gives them a path to production, you can feel GM's data intelligence rising already. On top of that, everything that's happening with Gen AI, we're going to layer in next year. And we feel like with that, we can go toe to toe with anybody in the world. So we opened the factory and we just needed the killer app. So about two weeks after the factory opened, we got a knock on the door from a guardian angel. I don't think that's her actual title at the firm, but one of the nicest people in the world. And her job is customer safety. And at General Motors, that's a big deal. Nothing is more important. And her job is to know the health of all the cars on the road, which ones might need a little bit of attention or service. She was experiencing this exact problem. Insights that she should get in an afternoon might be taking her days or weeks or even a month. She was having to go to all these silos and do complex joints. Now imagine her world. Tens of millions of cars on the road, different combinations of sensors, and all these new cars are coming in and they're more intelligent, more sensors. So this is near a max complexity problem and there's only one way out of it and it's going to be AI and ML. So of course we could help. We welcome turn to the factory. 15 underlying sources, we plug that into the cloud, rapidly expose the data in Unity Catalog and GM was sketching their heads. The meeting that we went into where we said we had brought this all to life in the cloud, they were like, I thought we were going to have a follow up PowerPoint meeting or a discussion to align on the next steps. We said, no, we're just building. We started to hammer out this first silver table and it was terrible. Nobody liked it. Then we had another meeting. We said, listen, we're going to pivot and rapidly get this better. And they said, this one has too many warts, but then gradually, because we all had the same mission in mind, we were seeing the same data, we started to come into alignment. And that's the way GM is going to get to this future. Smart people looking at the same data and getting into flow state rapidly is the path out of this. So to me, that's a great start. It's year one at GM. We got momentum. The company got oriented. We put thousands of people into Databricks. We're reducing the time to insight and we're finding ways to contribute value. In year two, we're going to layer on AI and perhaps take another step at GM towards our mission of zero crashes. So if you're a person that loves a good challenge, we have the mission and the data and now the tech, what we need are people that are willing to change the world. Please take a look. We'd love for you to join us at GM. Thank you. Enjoy the rest of your conference. All right. Isn't that awesome? I think every year that goes, we're going to get closer to zero emissions, zero crashes. I would love zero congestion. I mean, also the others, but congestion is my least favorite one. Okay, awesome. So I mentioned to you that we're hearing from customers all the time. They want to build AI on their custom data and they want to do that while having great cost and great privacy. Okay, that's what we call data intelligence, custom AI on your data. So I want to welcome on stage my co-founder Patrick Wendell that are going to tell you all about Mosaic AI. Welcome, Patrick. Well, there's a lot of people here. Hey, everyone. I hope everyone's doing well. Yeah, thanks. It's great to be here and talk about the latest in generative AI, things we're working on at Databricks, things our customers are doing, as well as the latest in industry research. So I'm going to start with a slide from Elise Talk. It's been a super exciting 12 months because as of, you know, 12 months ago, we had one really great frontier model that was a super high quality AI model. And today we have five or six amazing frontier models, several of which are open source. So to understand how we leverage these capabilities, it's important to know how frontier models actually work. So the way that these large scale generative AI models work is that they're trained on data from the internet. And in fact, the big breakthrough with GPT-3 was that the scale of training data went way higher than had previously been done. And it turned out that resulted in a much better quality model. So these models are trained on internet data. And then they're optimized and evaluated on how well they do on what are called general knowledge tasks. So the last slide I had a benchmark, that benchmark is MMLU. It's kind of the canonical benchmark for how these really frontier AI models are evaluated. And not a lot of people look inside MMLU, but it has 50 different categories in which they're evaluated on sort of general knowledge. So I like to think of it like if you're playing Jeopardy, it's kind of facts and information about many different categories. So I just pulled this up. Here's a few example categories that are in the MMLU benchmark. Some are expected, some are kind of surprising. So nutrition, world religions, astronomy, human aging is kind of a funny one you wouldn't expect. I actually played with chat GPT this morning just to test this. I started asking questions about wrinkle reduction. It happened to be like having psychopedic knowledge of how to reduce wrinkles as you age. So they're all chasing these benchmarks and trying to get good at these particular topics. But what we work on at Databricks is not so much general purpose AI models, but it's actually helping our customers build AI capabilities into their products and their services. And those capabilities need to have a very deep knowledge of the context of that product and the data that powers that company and product. So what we're focused on is not so much these general purpose knowledge tasks, but it's how well your customers are able to benefit from your AIs. So for example, if you have a customer support AI, you know, how effective is that in answering customer questions? Most people don't ask their customer support about wrinkle reduction. They ask about solving the problem that you have with your product. If you're generating code in your in your UI, how often are you generating good code that your users are accepting? And if you're creating marketing content with AI, is that marketing content on brand and does it match your company? And the way we like to think about this is that what we focus on at Databricks is not so much pushing the frontier of general intelligence, which is an exciting task in and of itself, but it's really the data intelligent application. So helping you build data intelligence into your products and services. And today I'm going to share some of our findings working to build data intelligent systems with thousands of different customers. So I'll give examples of types of systems that are being built as well as the fundamental technologies that we're building to help those come to production. And this is actually not just an active area of interest in industry. It's a very active research area is how you take these general purpose capabilities and you adapt them to specific scenarios and specific tasks. And the sort of leading research in this area points to a solution called compound AI systems. This is a paper out of Berkeley that's just one of many different research groups that are looking at this problem. And what compound AI systems do is they take the general capabilities from leading AI models, but they customize it substantially. They do things like tuning models, adding retrieval and search to your model, giving your model the ability to use tools and take actions in your enterprise. And it's through these compound systems that our customers are able to build really, really high quality embedded AIs in their applications. So it's been a little bit abstract. So I can give an example here. So FACSET is a Databricks customer. And for those who don't know, FACSET is a sort of leading a company around financial analysis and builds products and services for basically people that are in the financial markets. And this is a picture of the FACSET UI. And though people can use the point and click UI in FACSET, what most of their power users use to get all the information that they have on equities, bonds and so forth, is they use a query language called FACSET query language or FQL. So this is an FQL statement in this box. I don't expect you to understand what it does. I'll get there in a minute. But the opportunity with GenAI for FACSET is that instead of having people type this sort of very specific query language they need to go learn, it would be a lot nicer if someone could just say in English or in their preferred language what they're trying to do and the FACSET software could just do it for them. So here's an English version of what that query is showing. Give me the current year and trailing in earnings per share for all U.S. listed equities. So even I understand that and I'm not a FACSET expert. The opportunity here is that FACSET could be approachable to many more users and existing users could be much more efficient if they can just use language to say what they're trying to do. So how did FACSET build this? Well, they actually started by taking one of these general purpose frontier AI models and trying to just give it a few examples of their query language but simply just call into this existing model and have it translate from English to the desired query. And unfortunately this didn't work very well. The accuracy of this technique was only about 50-50 so it would like half the time generate the wrong answer and it was also extremely slow because of the amount of context they needed to give the model. It was like 15 seconds. So that was not really sufficient to build into the FACSET product. So how did they solve that? Well, they built a compound AI system and I won't go into a ton of detail but that system involved tuning some open source models. FACSET happens to have a huge amount of data of existing queries with labeled English examples so they could tune a model that understands their data extremely well. They also included search and retrieval so they could go and search and look up things like if I, you know, mention a company's name but I don't mention the ticker symbol, you know, that could go search in a database and be able to resolve that. And they also tuned and customized other parts of the process and through this they were able to get 85% accuracy and triple the performance in terms of speed. So this kind of met the bar for what they were able to put inside of their product. So FACSET is just one example. They're actually giving a talk, I think, at the conference. So if you want to learn more you can go to their talk. But at Databricks we're focused on building general capabilities through our Mosaic AI platform that lets any company do this type of customization to go from generally intelligent models to data intelligent products and services. This is actually the result of an acquisition we did that many of you may have heard about of Mosaic ML and we're happy to announce today that those, the results of acquisitions are fully integrated now into Databricks product and services and I'm going to walk through some of the individual capabilities we're offering in this area. So as Ali mentioned it's a life cycle from preparing your data, customizing models and deploying applications in production. And the first area of those three we're happy to announce today a zero code fine-tuning of open source models in Databricks. So what that lets you do is start with a really high quality existing model that was already trained and then with no code tune that model on your enterprise data to be really good at the particular task that you care about. Databricks will manage all of the optimization. Tuning is actually fairly complex. There's different types of parameters you can tweak. There's different ways you can do it. Databricks will fully manage that but you end up with a fine-tune model that you own and you can use that model in your AI product or service. There's actually several companies talking about their use of tuning on Databricks at this conference so I won't spill the beans on these talks but a few that you might check out. One is Fox Sports. By the way I think Fox Sports gets the award for the coolest animations in their talks. I'm a big football fan. They have football animations throughout the whole talk so even just for that I think it's worth going. But they're customizing AI using lots of data they have. They have a hundred years of transcripts from NFL and other leagues of people discussing ongoing sporting events. They can use that to customize a model and to be able to generate live commentary and things like that. At last, Ian, it's very similar. They have a ton of existing they're using AI and their product to generate code and generate dashboards and so forth and they have tons of labeled examples of doing that so they've customized and tuned models for that purpose. So tuning takes an existing off-the-shelf model and kind of tweaks it to be better at the thing that you care about for your product or service. But in certain cases, companies have so much data that it's in their interest to actually fully build a model from scratch. So this is called pre-training in the sort of ML language but what it really means is that you create a model entirely from your data that didn't touch the internet and didn't touch any other kind of data. So a great example of a customer doing that on Databricks through our Mosaic AI training platform is Shutterstock. And Shutterstock is actually announcing today at this conference a brand new state-of-the-art image model that they're able to expose and let their customers use. A little bit about what Shutterstock does, they're actually the world's leading or one of the largest databases of proprietary images and they can take advantage of that huge dataset and IP that they've accumulated over the years and let their customer generate totally custom images for marketing purposes or personalization. And unlike other image models that are trained on internet data, the Shutterstock model is completely trained on a trusted dataset that they have full rights to. So they're able to take this intellectual property they have and build an awesome model in order to share that with their customers and they'll be talking more about that model at this conference. So we're actually excited to share today that more than 200,000 custom AI models have been built on Databricks for use in enterprise AI systems. And boy, just the hardware required to do this is insane between GPUs and other types of AI accelerated chips. So our friend in the leather jacket that's going to be talking later, we owe him a lot of thanks for creating the ability for us to do this. So building the underlying models is one of the most important part of AI systems. But the next part is extending the model with capabilities beyond just basic data reasoning. By far the most popular way to extend models these days is something called retrieval augmented generation. That's really, the AI community likes to use fancy words, that really just means your model knows how to search. And what matters for enterprises is that you can have your model search over proprietary and custom datasets that you have. Databricks released earlier this year, the ability to host, to manage your data in a hosted search index, a vector database. And we're super excited to announce today this month that that offering has gone GA. We've also added much a state of the art embedding model in that offering as well. A little bit of an example of how that's used. Corning is a materials research company. And what they're doing with AI is they're building an AI for their internal research team, that's their sort of core IP generation, to be way more efficient in exploring different types of materials to research for industrial use cases. And they use our vector search engine to include tons of proprietary information on patents and prior materials research that only Corning has, that's not available in general purpose AI models, to make that app work really well for the researchers. So retrieval is a really popular technique, this search augmentation. But I do want to emphasize that it's just one type of tool of many. And what we're increasingly seeing at Databricks is our customers desire not just to do search in order to do question answering, but to actually have tools and abilities that can manipulate and take actions based on the person who's using the AI. So a few examples of types of tools our customers have built. You may want to extend your AI to open or close support tickets if it's a support AI. You might want your AI to be able to execute a small amount of code on behalf of a customer and execute it in a secure environment. These are called tools and very similar to base models. Tools need to be heavily customized and written in a specific way for each enterprise use case. A general purpose model is not going to know how to interact with your business system, your ticketing systems, etc. So today we're actually announcing the Mosaic AI tool catalog in Databricks. So what this lets you do is it lets engineers and scientists in your teams author tools. They can build those tools on top of secure compute abstractions and then they can publish those tools for use by other people building AI applications inside of your company. So this can separate the usage of tools from the authoring. The authoring may involve credentials or other sensitive information and it lets all of your engineers discover these tools and then use them. Now the real benefit of tools is not just the individual use of a tool, but it's actually combining the tools to create what are called agents. Another fancy term, an agent just really means an end-to-end AI application that can do something on behalf of your customers or users. So a support bot would be an example of an agent. And we're also happy to announce today a framework for authoring, deploying, and evaluating agents inside of Databricks called the Mosaic AI agent framework. This framework can work with your existing Langchain or other chaining frameworks, but lets you author your agent in Databricks, deploy it to an API endpoint, and test its quality. So my colleague Casey will actually demo soon a lot of these abilities around tools and agents that were super excited to ship. So the last piece of the puzzle is how you evaluate and understand quality. AI is a super exciting space right now because it's so easy to quickly demo and put some data in an AI and build something that seems to be pretty cool. But what's really important when you go from a demo to a deployed application is that you make sure the quality is really, really good. I'm actually kind of curious to ask, show of hands, how many people in the audience have done some kind of internal, maybe they're built or used an internal demo of AI on the data inside of their company? How many people have done some kind of demo like that? Curious. Okay, so it's like, you know, 60% of people. So we're seeing that everywhere. Every company is building internal demos, but then they need to go from the phase of having a demo to a deployed application. And what's really, really important is that you make sure the quality of the generated content is good. We talked at the beginning of the talk that, you know, the whole difference between general purpose models and deployed AI applications is that you're optimizing for something that's not general knowledge. You're optimizing for closing tickets, for, you know, generating high quality code, for helping your customers. But it's important that you systematically measure that so you can make sure you're doing a good job at it and you're improving it. So the third piece of this agent framework that we built is an evaluation tool that helps you employ state-of-the-art techniques to measure the quality of the thing you're building. So the way it works is you can first give a few examples of high quality interactions. Then as you iterate and tweak your application, as you tune models, as you experiment with search or integrate tools, you can check is the quality of this thing improving or is it getting worse? And what's more important is that once you're at scale, once you have users, many users using your application, the bottleneck becomes how you get lots of really, really high quality evaluation data. So you may have hundreds, thousands of interactions and you want to understand are these high quality or are they working well? So our quality, our agent evaluation also lets you invite experts that could be part of your company or they could be external contractors to evaluate and score actual AI interactions. And finally, it lets Databricks learn how to train AIs to do the quality scoring, which can allow you to scale this up and actually evaluate almost every interaction that you're having inside of your application. MLflow 2.14, which comes out on Monday, the team promised me they're shipping it on Monday, so I'm holding them to it by talking about the keynote. That will also include substantial quality tools around these generative AI applications, in particular the ability to log and investigate traces when you have low quality interactions. So that's often a big piece of the quality puzzle is if you had a user that gave thumbs down, the thing didn't work so well, what exactly went wrong? Was it an issue with the retrieval? Was it an issue with the way I tune my model? And so that'll be part of MLflow and if you're using it in Databricks, well, so deeply integrate it into the Databricks UI. Cool. So we talked about building customized AI systems, deploying them and understanding their quality. The last piece of the puzzle is governance. And it's a really exciting time in AI. Everyone wants to move super fast. They want to compete in their markets. They want to build AI-based applications. But particularly for enterprises, it's very, very important that the AI that you deploy is safe and it's trusted. And an issue we've seen in a lot of our customers is sort of their victims of their own success in some way. You have one AI project that's really successful and then you turn around and now you have 10, and now you have 20, and now you have 50. But how do you, as a sort of global company, make sure that they're all adhering to the safety properties, meeting the quality that you expect and so forth? So a good example of a customer that had this type of challenge on Databricks was Edmunds. So they had tons of AI projects going into production, but things started to become a bit of a sprawl. So you had different teams that were managing their own credentials to some of these third-party model providers. The cost started getting really out of control, and the capacity management was also an issue. You know, GPUs are a scarce resource. You want to make sure that your most important applications have access to the most important, the resources that you have available to you for serving. So we're also excited to announce today the Mosaic AI Gateway, which is a central point inside of Databricks, where you can enforce all the sort of auditability and governance requirements that you have around use of your models. So this isn't meant to slow teams down. It's actually, from our experience, helped individual engineering teams move much faster because they have access to a very specific set of approved base models. They have a set of guardrails that's been agreed on for the company that they can use, and they can actually innovate faster and not sort of each reinvent the wheel around these foundational capabilities. Awesome. So as I said, we've been working on technologies to help, you know, not only the customers I mentioned here, but our 10,000 customers build and deploy AI products and systems. And I'm super, you know, I always believe in show, not tell. So I'm super excited to welcome my colleague, Casey Yulinuth, who will actually be taking you through these in a live demo. She's doing it live. So, you know, caveat in case there's any issues. She's going to take you through a live demo of these capabilities in our product and how an example customer might use them. So please join me in welcoming Casey Yulinuth. Thank you. All right. Thank you, Patrick. So, all right. Where are we? All right. So I work for a cookie conglomerate that has a bunch of franchises, and I want to create an AI agent to help all my franchise owners improve their business by allowing them to analyze customer data, allow them to create marketing campaigns, and analyze and develop sales strategies. And so one of the things that they'll be able to build with this AI agent is an Instagram ad campaign where they can promote the best-selling cookie in their franchise, and the AI agent is going to create an image for the Instagram ad, as well as a caption that's going to capture the hearts and minds of all of our cookie lovers to really drive sales. So I gave general intelligence to my franchise owners with the off-the-shelf model, and it was giving good results, but they were too generic, and they weren't tailored to our business or the individual franchises. And this is where the Mosaic AI platform comes in. Mosaic AI is going to allow us to extend this general intelligence with our enterprise data so that we can have data intelligence. In this demo, we're going to build an agent that's going to use the Unity catalog tools that Patrick just mentioned. So in this architecture, we're going to leverage these UC functions that can now be leveraged as tools. And UC functions can be SQL functions that access your data warehouse. They can be Python functions. They can be model endpoints, and they can even be remote functions, which are going to allow you to call external services like Slack or email or even file a ticket if you need to. So to build all of this, we're going to use the Mosaic AI platform, and with that, we're going to go ahead and jump right in. So here I am inside. So there's three capabilities that we're going to use inside of Mosaic AI to actually build this data intelligence. So the first is we're going to use our tools catalog to actually build the data intelligence. The next, we're going to build and understand our quality with agent evaluation, and then we're going to be able to debug and improve our quality with ML flow tracing capability. So with that, let's dive in to Databricks. So here we are inside Unity catalog. You can see that I have some functions that I'm going to use as tools, and these are governed alongside my AI, my unstructured data, and my structured data. So to help demystify what a tool is, we're going to go ahead and click into our franchise sales. So you can see in here, it's just a simple SQL query that's accessing my sensitive transaction data, and this is where it's really important that your tools are governed alongside your data, because only the people who have access to this underlying transactions table are able to successfully call this tool, and that's why we need this centralized governance across data, AI, and tools. So some of the other tools that we've created that are leveraging our enterprise data are in here as well. So franchises by city and by country are just like helper functions to help me get the sales data, and then this franchise reviews tool is actually grabbing customer reviews from our social media site. So all of these tools are leveraging my enterprise data. So we're going to go ahead now and extend a base model with these tools. So I'm going to come over here into the AI playground, and we're going to jump in, and then from the AI playground, I'm going to select a tools enabled base model. And so you can tell it's tools enabled because it has this little icon on the right. So I'm going to go ahead and select Lama 3. From here, I'm now going to add hosted tools. So these are my Unity catalog tools that are hosted inside the secure and scalable Databricks environment. So in here, we're going to access the tools that we just showed you in the AI schema, so I can use the syntactic sugar to grab all of those tools. And then my marketing team has created a tool for me, so I'm just going to copy paste it real quick because it's kind of long. So we're going to put this in here. And this tool is going to generate an Instagram image using the Shutterstock image AI model that Patrick just announced, as well as a caption. So now it's time to actually test this. Before I forget, we're going to crank our temperature down to zero because this is a live demo. And then now we're going to quickly copy paste a prompt in here. So this prompt is going to say, hey, send marketing an Instagram post with an image and a tagline for the best-selling cookie in the San Francisco store so we can increase our sales and show that we listened to customer feedback. So we're going to go ahead and here and click... Uh-oh. We're doing it live. Uh-uh-uh-uh-uh-uh. Well, that should have unfortunately worked. Hold on. We'll try again. So we'll go back in here. We're going to add our tools, AI star, and then we're going to come in here and add our marketing. Oops. Oh, it's now in our autocomplete in here. And now let's try this again. Unfortunately, it looks like there's a connection with our... Unfortunately, what is this? Oh, oh, oh, I'm so silly. I typed in the wrong name of the function. You all should have caught that for me. I need to type in retailpraw.ai. Thank goodness for error messages. So we're going to come in here and now we're going to add this in. And now we're going to go ahead and do this. Thank you. Thank you. All right. So what is happening here is going to be a little bit magical. So we're going to come inside... Oh, hold on. Oh, my goodness. All right. We're going to have to bear with me. We're going to have to do it all again. All right. We're coming in here. Coming in here. Sorry. This is why you don't do it live. All right. Retailpraw.ai. All of the functions in there. We're going to add in our marketing tool that's going to generate our Instagram ad. All right. We're in here. We're going to send the prompt. And we're going to make sure it's temperature zero, which is what we kind of forgot there. Temperature zero, live demo. All right. Here we go. Now we're going to send this. And what's happening is going to be kind of magical. So as we come in here, you're going to see that Lama 3 is going to do chain of thought reasoning. So it's going to figure out which tools it needs to call in order to execute this. So you can come in here and say, oh, the first thing it had to do was grab the franchise ID for my San Francisco store. The second thing is it needed that to be able to access our franchise sales because it's trying to identify what that best-selling cookie is. Then it's going to grab all of that sales data in here. So it grabbed all my sales data. And from here, I identify the best-selling cookie is this almond biscotti cookie. From there, it said, hey, we need to show that we listened to customer feedback. So we need to look into our customer reviews tool. From there, it's going to ask, hey, what do customers like about this biscotti cookie? It's going to say, oh, they like the crunchy texture and unique flavor. And it's going to send all of this to my Slack tool that's going to generate this Instagram image and caption and send it to my marketing team on Slack so they can review it before we post it on our social media. And so now the moment we've all been waiting for is let's see what it actually returned. So I'm going to jump over into my Slack. And this is the image that was generated by the Shutterstock image AI model that shows our image biscotti. And then you can also see that it creates this customized caption where it says, our customers rave about our biscotti for its crunchy texture, unique flavor, and perfect coffee dipping quality. And so this is what it has generated. And this is how you can use data intelligence to extend your general intelligence to improve a base model. Now, what happens if I remove the intelligence? So I can come in here, kind of like we did earlier, and we're going to remove all of these enterprise data-enabled tools, and we're going to run it all again. So now we've taken away all of our enterprise data access from this, and now it's still going to generate an image, and it's still going to generate a caption according to that prompt, but it's going to be much more generic. So as we jump back into Slack, and it's going to show me my new image soon, down here, okay, successfully sent. Here we go. So this is the image that I now created. So all I had to go off of was that it's a cookie that's in San Francisco, and so it tried to create some kind of cool Instagram ad. Yes. And if you take a deeper look at the caption, it's very generic. So it just says, our best-selling cookie is backed by popular demand. Share your favorite cookie moments with us. And so not really tailored to our specific business, or the franchise, or using our enterprise data at all, and this is why data intelligence is so important. So we just showed how you can use the tools catalog to extend your general intelligence with your enterprise data to create data intelligence, but how do I know that this agent is high quality? So the way that I know it's high quality is I'm going to use agent evaluation and MLflow tracing tools. So agents are really hard, like how do you know if what we just did was good or bad? So we're actually, and there's so many different things you can do with an agent as well. So we're actually going to have to launch a pilot program with a subset of the franchises and give them the agent evaluation review app. So this review app is going to allow all of your franchise owners to interact with your agent, whether or not they have a Databricks account, and then it's going to allow them to give feedback on the response. So they can come down here and they can say yes, and they can explain why or why not that this answer was good, and then they can go ahead and click done and submit this feedback. This feedback that this submitted is then logged in a Delta table in Unity Catalog in your account that you can then build an evaluation data set off of, get confidence that you can go into production, or as I have done, I enabled lake house monitoring on top so that I can observe how my pilot program is running. So you can see over here the different franchises that I set up this on, and here is me tracking their negative scores with the agent over time, and you can see that something is going terribly wrong with the Los Angeles franchise. They're getting a lot of negative feedback on the agent. So if I scroll a bit further to actually investigate their ratings and what questions they're getting, they're rating poorly, we can see that their actual feedback that they're giving me on here is that it's returning irrelevant reviews, so it's returning reviews from San Francisco stores or non-LA stores to them, or it's even hallucinated that there's a Liberty Chip cookie, which we don't sell one of those at the cookie conglomerate. So we're going to need to dive in deeper to figure out what's going wrong with our quality here. So we're going to use MLflow tracing to get deeper and figure out what exactly is going wrong. So I'm going to jump inside of a notebook where I've actually queried my assessment logs in here, and in here we have captured an MLflow trace automatically for you. So MLflow is a popular tracking API for GenEI and machine learning experimentation and deployment, and so we've extended it to now work with compound AI systems, where you can now trace your input to the system and how it's transformed as it goes through every step of the system along the way to actually create that output in the end. So I can click on one of these traces, and it's going to open up this stack view. If I click the top of the stack, you can see the question that was sent to the system and the output, so you can see this is the one that's saying what are customers saying about Liberty Chip, and then this is the absolute hallucination, which we learned from the review app, where it's saying, hey, customers are raving about this cookie, and it's out of this world delicious, but we know there's no such thing as this cookie. So we need to figure out what went wrong. So we're going to go over into our stack, and we're going to dig deeper into it, and so we're going to go to our first tool that was called, which is this customer reviews tool. As we go in here, we're going to see what the input and output of that were, and if I go here, you can see, okay, it's saying that the Liberty Chip cookie is out of this world delicious, so this is definitely where the problem is coming from, so we're going to need to dive even deeper into the stack, and when we get down here, we're going to get into our retriever. So this is the thing that is actually retrieving our customer reviews, and so if I look, and now, because of tracing, I can actually see the exact reviews that are returned, and so I can see this review is saying, hey, the staff is warm and welcoming, the store was spotless, and the cookies were out of this world delicious, so what's happening is my retriever, because I can't find anything about this Liberty Chip cookie, is just returning random reviews, and so I'm going to need to do two things to fix this. The first, I'm going to need to actually increase my criteria threshold for relevance on my review app retriever, so it says, hey, don't return reviews if the relevance is below a certain threshold, and then I'm going to need to do a little more prompt engineering to make sure that if the context is given to my model isn't relevant to the question that was answered, don't just summarize that context. So I've gone ahead and I've already made those two fixes, and I've redeployed my agent, and so now we've redeployed the review app as well, sent this out to my franchisees and in my pilot, and so now they can say what our customer is saying about the Liberty Chip cookie, and so if I type this in here, you can see that now, instead of hallucinating, it's saying that Liberty Chip cookie is not mentioned in the reviews, it's possible that's not sold in these stores, which is exactly what we want to say when this happens. All right, so in this demo, we just showed how you can use the tools catalog to build data intelligence by extending a general model, or general intelligent model. We saw how you can use agent evaluation to actually understand your quality by getting your agent into the hands of your franchise owners, even if they don't have Databricks accounts, and allowing them to give that human in the loop feedback with thumbs up, thumbs down, and then we used MLflow tracing to allow you to debug and iterate on your quality to improve your agent. So we talked a lot about many different things in this talk, but there's three main key takeaways that we want you all to really gather about the Mosaic AI platform. The first is that we have to move from general intelligence to data intelligence, and the way that we do this is we augment general intelligence with your enterprise data, and this is going to give you much better insights into what's happening in your business, as well as it's going to improve the quality of your applications, and we saw this with the franchise cookie agent, where the images and the captions for our Instagram ad campaign were much better once we gave it access to that enterprise data. The second thing is that you can also improve quality by moving from these monolithic models to modularizing them down into AI compound systems, where now you can specialize each step in the system to improve your quality, like we saw with the fact set use case, and also in many cases will also improve your latency. And lastly, the Mosaic AI platform is the best platform to build high-quality compound AI systems. We have thousands of customers using the Mosaic AI platform today, and one of those customers is Block. And so I'm very excited to welcome Jackie Brossamer from Block to the stage, who's going to talk about how her team has leveraged the Mosaic AI platform to build and deploy generative AI solutions. And so with that, welcome Jackie Brossamer to the stage. Thank you, Casey. I'm Jackie Brossamer, and I'm the head of Platform Engineering for AI, Data and Analytics at Block. Today, I'm going to tell you a little bit more about the journey we've had taking AI into real business impact using the Databricks platform. Unfortunately, I don't think my clicker is working. All right. Block is a really unique company because we have a bunch of decentralized business units that are all united by this common purpose of economic empowerment. We have Square, which is our first business unit where millions of small businesses leverage our products to take payments and grow their business through add-ons like banking. We have Cash App, where hundreds of millions of different consumers use our products to send payments to their friends and family, as well as manage their finances through products like investing and borrow. We have Tidal, which is a music streaming service founded by JZ that helps creators to monetize their creations. And we have TBD, which is a subsidiary working in decentralized technologies like Blockchain and Web3 Identity. One of the really unique challenges of our data platform team is that we have to have one data platform that's able to support all of these different diverse use cases, which also come with different people, culture, and practices. And so we have to make sure that we have a really flexible platform that can not only scale across these different types of data, we have billions of payments going through Square and Cash, so we have to really operate on scale when it comes to structured data moving in real time. We also have tons of unstructured data from Tidal, and so we have to be able to handle both types. And it's really been a challenge to have a platform that's flexible enough to not only cover all of these use cases, but also be able to scale to new use cases that we didn't see coming like generative AI. Before we talk about the platform behind the scenes that allows us to productionize all these use cases, let's take a minute to look at one of these use cases in the wild. So here we have one of our generative AI use cases for Square, where we allow a small business to onboard and automatically get a suggested menu that can get them started right away without having to go through and manually fill in. One of the principles we've really tried to rely on as we think about how to use generative AI is this principle of giving time back. We want to give time back to our Square sellers so that they have a chance to focus on the parts of the business that really differentiate them, and we can automate away those non-differentiated business operations like onboarding or creating a menu. And so in this case, we've seen up to 15% time savings for small businesses to get set up and start making money, and with small businesses with really small margins, that can be a really, really huge advantage. Not only do we focus on generative AI for a lot of these external facing use cases, but we also have a really big emphasis on internal facing productivity use cases like code generation and workflow automation. And we all power these use cases through this flexible Databricks platform that we're going to talk through in a little bit more detail now. We wanted to build our AI platform in a really flexible, scalable way where the data was already securely stored and this federated data lake that connects data across the BUs, business units, while still allowing those business units to implement their own security policies and access controls. We were able to quickly and seamlessly stand up a large language model platform on top of our existing Databricks infrastructure rather than having to start from scratch because of the composability of this platform where different business units can use some parts of it, but not all parts of it. Key components of our platform that we've been able to leverage for the large language model use cases is the Databricks AI platform and model serving, which allows us to manage calls from all model endpoints, as well as ML flow for large language model operations and governance, which is a huge developer experience advantage since many of our machine learning engineers are already familiar with ML flow. Because of both the complexity of our different business use cases and the quickly evolving external landscape, we have centered our AI strategy around supporting production quality use cases for AI while assuming that the models are going to completely change over the next few years. Our strategy has three key pillars. First of all, we center federation, the idea that we want this consistent interface that we can swap models out behind as the models continue to evolve. We also really center this idea of agility where we know that the patterns that we're using to call models today are not necessarily the patterns that we want to use to call models tomorrow, and so we want our platform to be able to evolve and scale to support that. And then finally, we really center this idea of control. As a financial technology company, we have a lot of really sensitive enterprise data that we want to make sure is staying secure as we use it for new use cases like large language models. As innovation continues to happen in the generative AI space and new models get released, we want to keep this optionality to easily switch out these models without having to write a whole bunch of new code. For instance, our company's philosophy is really aligned with open source, but a lot of our original use cases have used proprietary models like GPT-4. With the Databricks Mosaic AI gateway, we can really easily compare the Lama open source models to open AI's closed source models and match the same easy developer experience without having the need to implement new APIs. This federated approach ensures that we stay agile and responsive to advancements in AI technology. While most of our original use cases use state-of-the-art models like GPT-4, we're increasingly seeing that we have really specific use cases where we want to fine-tune open source models using proprietary data. With Mosaic AI training and Databricks, we can easily fine-tune these open source models in the same place within the platform, and most importantly, that means that the data doesn't have to leave the platform, which would be an additional security risk. And then once this model is fine-tuned, we can easily serve it through that same AI gateway. Finally, for most of our use cases, we don't just use a model, but we use this rag pattern that was mentioned earlier where we're sending a lot of context along with that model to make sure that we get the best and most relevant results. With Mosaic AI, we can easily implement this rag pattern right within the platform with the full and controlled governance of components like Unity Catalog and not have to worry about the data leaving the platform. This centralized approach really is important for our security posture to make sure that we can have the right granular access as well as centralized concerns like compliance and cost optimization, which can become really sprawling if we have a bunch of different decentralized endpoints. Using our flexible platform, we've been able to see real change in metrics that directly impact the business. We've seen a 26% improvement in the time that it takes to deliver a generative AI application to production. We've seen a 32% increase in developer productivity with engineers who are using the platform, and all of that has added up to around $10 million in additional productivity gain versus our original forecasts. Thank you, and if you're interested in joining, please check out our career page. Awesome. That's super cool. I really love working with Block and Square and especially Jacque's team. They're super regulated, right? It's a financial services firm, but they're one of our most cutting-edge partners that we work with. They're using all the latest stuff, and they're pushing us to the limit on everything. Wow. What an awesome demo by Casey. I was so nervous when she got that error. I told people in the back, no more live demos ever again when she fixed it. I was like, okay, more live demos. Awesome. It's a great pleasure to introduce on stage our professor, Fei-Fei Li. She's going to be talking about what she calls spatial intelligence, but I think of it as world models, LLMs that not just understand language, but they understand the whole world. So, super excited about this. So, let's welcome Fei-Fei Li to stage. Thank you, Abby. Hi. Good morning, everyone. Really, really happy to be here. I'm not going to show you products or live demos, so I'm here to share with you a glimpse of the future. A glimpse of the future that goes beyond just understanding language, and I call it from seeing to doing. So, let me start by showing you something. Actually, please have my fonts a little larger. In fact, I'm going to show you nothing. This is not a glitch. This is our world 540 million years ago. Pure, endless darkness. It was, it was a dark due to the lack of light. It was dark due to the lack of sight. Indeed, sunlight filtered a thousand meters below the ocean surface, and light permeated from hydrothermal vents onto the seafloor. Although brimming with life, there was not a single eye to be found. Alien were in these oceans. No retinas, no corneas, no lenses. So, all this light, all this life, went unseen. There was a time when the very idea of seeing didn't exist yet, when there was something when it was something that had simply not been done before, until it was. For reasons were only beginning to understand, trilobites, the first organisms that could sense light, emerged. They were the first inhabitants of the reality we now all take for granted. The first to discover a world in which something exists beyond the self, a world of many other selves. This ability to see is thought to have helped usher in a period called Cambrian explosion, where a huge variety of animal species entered fossil records. What began as a passive experience, the simple act of letting light in soon become much richer and far more active. The nervous system began to evolve. Sight turned into insight. Seeing became understanding. Understanding led to actions, and all of these gave rise to intelligence. So, half a billion years later, we're no longer satisfied with just nature's gift of intelligence. Humans are now on a quest to explore how to create machines that can see just as intelligently as we can, if not better. Nine years ago, I gave a talk at TED, and I delivered what was an early progress report on computer vision, a subfield of artificial intelligence. Three powerful forces had emerged for the first time about a decade ago. A family of algorithms called the neural network. Fast, specialized hardware called graphic processing units, or GPUs, a urgula here from Jensen later. And big data, like the collection of 50 million photos that my lab spent years curating called ImageNet. When combined, these factors cause computers not only to see better than ever, but they also ushered in the age of modern AI. We have come a long way since then. Back then, a decade ago, just labeling objects was a breakthrough, like the first glimpse of light for those early trilobites. But the speed and accuracy of neural network algorithm rapidly improved year after year. The annual ImageNet challenge led by my lab gauged the performance of these algorithms. And every year, the submissions broke records. As you can see from this plot showing the annual progress and some of the milestone models, they are really incredible. But we're not satisfied since then. We have further developed models in our lab as well as other labs that can segment objects and recognize the results of them. But there's more to come. I remember when I first showed the world the first computer vision algorithm that can describe images and photos in human natural language, a way to do automatic picture captioning, caption writing. That was joint work with my brilliant former student, Andre Capathie. At that time, I pushed my luck and asked Andre to reverse this, give a sentence, and ask computers to generate photos. And Andre just said, haha, that's impossible. Well, as you can see from this recent tweet from him, that just a mere few years later, the impossible has become possible. This is thanks to the development of recent diffusion models used in generative AI. AI programs can now take any human input sentence and create a photograph or a video of something that's entirely new. Many of you have seen the beautiful result of Sora by OpenAI and many other companies recently. But even without the enormous number of GPUs, my students and my collaborators were able to create a generative model called WALT month before Sora was released. And here are just some of the results. Of course, you can see we have room for growth and we do make mistakes. I mean, look at that cat's eye, right? It dips underneath the water without even getting wet. I call it a catastrophe. I hope someone is making better AI jokes for me. But if the past is a prologue, we will learn from these mistakes and create a future that we imagine. And in that future, we want to take full advantage of all that AI can do. For years, I have said that taking a picture is not the same as to see and understanding it. Now I would like to add on to that. Simply seeing is not enough. Seeing is for doing and learning. When we act upon the world in 3D space and type, we learn and we learn to see and do better. Nature has created a virtuous cycle of seeing and doing powered by spatial intelligence. To illustrate what your spatial intelligence does constantly, let's look at this picture. Raise your hand if you feel like this photo want to make you do something. Keep your hand up if this has actually happened in real life. In the last split of a second, your brain looked at the geometry of the glass, the place in 3D space, its relationship with the table, the cat, and everything around it. And you predicted what will happen next. And then you will dive towards that glass to save your carpet. This urge to act is innate for beings with spatial intelligence, which links perception with action. So to advance AI beyond what is capable of today, we need more than AI that can see or talk. We need AI that can do, just like what nature did to us. And indeed, we're making exciting progress here. Our recent milestone in spatial intelligence are catalyzing that virtuous cycle of teaching computers to see, do, learn, and then see and do better. But this is not easy. It took millions of years for animals to evolve spatial intelligence. And in contrast, it took only a couple of hundred thousand years to evolve language. And that evolution depends on the eye using light to project 2D images onto the retina and the brain translating those images into 3D. Only recently, a team of computer vision researchers at Google just did that. They created an algorithm that can take just a set of photos and turn that data into 3D shape or 3D scene. And here are more examples of that work. In the meantime, my students and colleagues are inspired by this work at Stanford and went a step further and created an algorithm that only required one image to generate 3D shape, like you see here. And here are a few more examples of that recent work. And recall we previously used text input to create videos. A group of researchers at University of Michigan figured out how to translate a line of text into 3D room layout. And you're seeing an example here. In the meantime, my colleagues and their students at Stanford have developed an algorithm that can take an image and generate infinitely plausible spaces for a viewer to explore. These prototypes are the first budding signs of a future possibility, one where human race captures our entire world in digital forms and is able to model the richness and nuances of our world. What nature was able to do implicitly in our individual minds, spatial intelligence AI can now hope to do in our collective conscious. As the progress of spatial intelligence accelerates, a new era in this virtual cycle is playing out before our eyes. This back and forth is catalyzing robotic learning, a critical component to early embody intelligence system that needs to directly understand and interact with the 3D world. A decade ago, ImageNet from my lab enabled a database with millions of high quality images to help computers to learn to see. Now we're doing that with behaviors and actions that teach computers how to act in 3D world. Instead of manually creating training examples, we're now using simulation environments like the one provided by NVIDIA omniverse powered by 3D spatial models that offer endless varieties and interactions. You're now seeing a small set of those examples of the infinite possibilities of training our robots in simulation environments in a project that my lab has been leading called behavior. There's also exciting progress in robotic language intelligence, combining vision and spatial intelligence. Using large language model based inputs, my student and collaborators are among the first team to show robotic arm performing a wide range of tasks based on verbal instructions like this one asking the robot to open a drawer but watch out for the vase or this one to unplug cell phone. It's a kind of an unusual way to unplug but okay. And this one to put to make a sandwich. And well typically, I would like a little more on my sandwich but this is not a bad start. So in that primordial ocean 540 million years ago, the ability to see and perceive one's environment set off a can bring an explosion of interactions with other life forms. Today, that light is starting to reach digital minds just as it once did to our ancestors. Spatial intelligence technologies are allowing machines to interact with one another with humans and with the 3D world real or imagined. With this future taking shape, we can imagine how it will have a profound impact on so many lives. Let's just take healthcare as an example. In the past decade, my lab took some of the first steps towards applying AI technology to challenges impacting patient outcome and medical staff burnout. Together with my students and colleagues at Stanford School of Medicine and partnering hospitals were piloting smart sensors that can detect when a clinician enters a patient room without properly washing their hands, keep track of instruments during surgery or alert care team when a patient is at physical risk such as falling. We consider these technologies to be forms of ambient intelligence and these extra pairs of eyes can make a difference. But I would love to see more interactive help for patients, clinicians and caregivers who also desperately need an extra pair of hands. Imagine autonomous robots transporting medical supplies so that caregivers can have more quality time with our patients or augmented reality guiding surgeons towards safer, more efficient and less invasive operations. Imagine patients with severe paralysis controlling robots with their thoughts. That's right with brainwaves so that they can do everyday tasks that you and I take for granted. You're actually seeing a glimpse of that future now. In this pilot study from my lab, as you can see in the video, here a robot arm is cooking a Japanese sukiyaki meal controlled only by brain electrical signal, non-invasively collected through EEG caps. So no chips or no electrodes were inserted into the person's brain. This entire robotic action is done by remote brain control. Thank you. Half a billion years ago, the emergence of vision not only turned a world of darkness upside down, it also kicked off the most profound evolutionary process, the development of intelligence in the animal world. AI's breathtaking progress in the last decade is just as astounding. But the true digital Cambrian explosion won't realize its fullest potential until computers and robots have developed the kind of spatial intelligence that nature has endowed to all of us. It's now time to train our digital companions to learn how to reason and interact with this incredible 3D space we call home and to create many new worlds for all of us to explore. Realizing this future won't be easy and it will require all of us taking thoughtful steps to develop technology that can always put humans in the center. If done right, computers and robots powered by spatial intelligence will not only be useful tools, but they can also be trusted partners that can augment and enhance our productivity and humanity while respecting our individual dignity and lifting our collective prosperity. What excites me the most is a future in which as AI grows ever more perceptive, insightful, and spatially aware, it joins us in our quest to satisfy our curiosity, to always pursue a better way so we can make a better world. Thank you. Wow, that's so awesome. I love the, you know, telling Andre Carparti like after he did all this research, like hey, now do it backwards, do the reverse. Just take the text and produce the images. I wish I could do that. Here's the audience, put together a whole show, use holograms, figure it out, do the announcements, and be done. Awesome. So I am super, super excited to introduce our next guest who actually is a man who does not need any introduction. So I want to welcome the world's one and only rock star CEO and video's Jensen Wang to stage. Awesome. Thank you for coming. So, Matt, I just want to start, you know, just looking at Nvidia's amazing performance, you know, three trillion dollars, like did you imagine it would be this way, say five years ago, that the world would unfold this way? Sure, from the very beginning. It's so awesome to see. Any advice for a, you know, fellow CEO? How do we get there? Whatever you do, don't build GPUs. Okay. All right, let me tell the team, we need to back out. Awesome, man. So we spent a lot of time this morning talking about data intelligence, which by what we mean, enterprises, you know, they have all these proprietary data, training AI models that's customized on their data that they have. How important is that? Is that something you see? You know, is that something that we need to invest more in? What are you hearing? Well, every company's business data is their gold mine. And there's every company is sitting on gold mines. If you have a flywheel of services or products, customers enjoying those services and products, giving you feedback, you've been collecting data for a long period of time. It could be customer related, it could be market related, it could be supply chain related. All of us have flywheels of data that we've been collecting for a long time. We're sitting on mountains of it. But the fact of the matter is, none of us have really been able to extract insight, or even more importantly, distill intelligence out of it until now. And so we're pretty fired up. I know we are. And we're using it in our chip design, we're using it in our bugs database, we're using it in creating new products and services, and we're using it in our supply chain. And, you know, for the very first time, we now have a business, we now have an engineering process that starts with data processing and refinement and learning models and then deploying the models and connecting that flywheel, collecting more data. Isn't that right? Yeah. And so we're doing that in our company and is making it possible for us to frankly be one of the largest small companies in the world. And the reason for that, of course, is because we have so many AIs in the company helping us out, doing amazing things. And I think every company is like this, you know? And so I think this is just an extraordinary time and it starts with data. It starts with Databricks. That's awesome. Thank you so much. Curious, you know, there's this whole debate brewing. Closed models versus open source models, you know, are open source going to catch up? Are both going to exist? Is it going to eventually just be dominated by one giant closed source model? What are you seeing? What are you thinking about the whole open source ecosystem and how important has it been for sort of development of LLMs and how important is it going to be going forward? Well, we need frontier models. We need amazing frontier models. Of course, the work that OpenAI is doing, the work that Google is doing, really, really important and pushing the frontiers and helping us discover what's possible. But if you were to look at this year, probably the most important events this year were related to open source. Lama 2, now Lama 3, Mistrawl, the work that you guys did, Databricks, DBRX, do I have to say DBRX? DBRX, right? DBRX, I think really, really cool stuff. And the reason why it's really cool is because it activated every single enterprise company. It made it possible for every company to be an AI company. Isn't that right? You're seeing this yourself and we're seeing this all over the place. We recently turned Lama 3 into a fully containerized inference microservice. And it's available for downloads. You can go to HuggingFace, you can go to, of course, Databricks. And it's now being integrated into several hundred companies around the world. And so that tells you something about how open source has activated every company to be an AI company. We're using open source models all over our company and we create some proprietary ones. We fine tune open source ones, train them for our data and for our skills. And so I think without open source, it wouldn't have activated this entire global movement for every company to be an AI company. I think it's a huge deal. Yeah, that's super awesome. So both are going to be around and we need both, like open and close. And is this a NIMM framework? You're talking about NIMS of how you do the survey? Yeah, we call them NIMS. Yeah, we're super excited. I'm super excited to announce here that we're going to put DBRX inside NIMS and we're going to serve it on Databricks and actually any new models that we develop in the future. So we're super excited about NIMS. Yeah. It's actually quite an amazing thing. In order to create one of these endpoints, these APIs, these large language model APIs, the stack is really complicated. These are giant models even though they seem small these days. They're still computationally really large. And the computing stack is really complicated. There are hundreds of dependencies necessary to create one of these endpoints. And so we created this thing called the NVIDIA inference microservice where we package up all of the dependencies. We optimize all of it. We have a factory in the company with all these engineers working who are expert in doing this. And we package it up into a microservice and you could enjoy it at Databricks. You could download it and take it with you. You can fine tune it with microservices that we call NIMO and use it anywhere you like. It runs in every single cloud, runs on-prem. You can enjoy it everywhere. That's a really amazing thing. Yeah. And it's awesome you can even run it on-prem. You don't have to be on the cloud. That's super awesome. Okay. So when we talk to customers, we're hearing that they have to develop this sort of expertise in-house to customize models to gain advantage. What are your thoughts on that? Well, I think in the future, look, what's happening in the world today is that we figured out a way to tokenize almost any information, almost any data. And we can extract structure, understand, learn its representation, understand the meaning of that information of almost any kind. It could be, of course, sound, speech, words, language, images, videos. It could be chemicals and proteins. It could even be robotics, articulation and manipulation. It could be steering wheel, articulation, driving. We can tokenize almost anything. And because these cloud data centers are really producing tokens, we're manufacturing something that is quite unique for the very first time. You have this instrument called these AI supercomputers that we build. It's producing tokens, generating tokens in essentially a factory that's designed for that one job. And this ability for us to manufacture intelligence at scale is pretty new. And that's one of the reasons why I'm almost certain now as we're building these AI factories everywhere for all these different industries that we're in the beginning of a new industrial revolution. Instead of generating electricity, we're generating intelligence. Every company, of course, at its foundation is about domain specific intelligence. Very few companies on the planet knows more about data and data processing and AI and the infrastructure necessary to do all that than Databricks. We are quite specialized in the work that we do. And we're at this foundation all about that domain specific intelligence. Every company has it could be financial services, could be healthcare, whatnot. And so at the end of the day, every one of us will become intelligence manufacturers. And if you're going to be intelligence manufacturers, today you have HR. In the future, you're going to have, you know, HR for AI. And we call them AI factories. So every single company will have to do that. We're doing that. You're doing that. We see companies large and small doing that. And so in the future, 100% of us will do that. You start with, of course, your domain specific data. It's sitting in Databricks somewhere. You're going to process that data and refine and extract intelligence out of it. You're going to put it into a flywheel. You're going to have an AI factory. All of us will. Yeah, this is so awesome. I totally 100% believe in this. And one thing we're excited about is, you know, so we do a lot of data processing. And data processing is like massive amounts. I think we process about four exabytes every day, you know, 4,000 petabytes every day in Databricks. And, you know, it is the single largest computing demand on the planet today, processing data. Every single company does it. Yeah, exactly. And, you know, it's actually highly paralyzable. You know, we do the same operations again and again and again. So I'm really, really, really excited to partner together to really bring that kind of GPU acceleration to data processing. So we can do the same revolution that AI models have seen on the core data processing. So we're super excited to partner with you on using GPU acceleration for our photon engine to be able to really kind of enter this new era of also applying GPUs to core data processing, right? These massive workloads that today have to run on CPUs, get them also run on NVIDIA GPUs. We're very excited about that. Yeah, this is a big, by the way, this is a big announcement. The two most important trends in computing today is accelerated computing and generative AI. NVIDIA and Databricks are going to partner to combine our skills in these areas to bring it to all of you. And, yep. And this work in accelerating data processing, as you know, it's highly paralyzable. But it's really arcane. It's really complicated. And the reason for that is just there's so many data formats, there's so many different ways to group and join. And, you know, just wrangling data is a really complicated suite of libraries. Spark is a super complicated suite of libraries. And it's taking us five years working around the clock to finally have a suite of libraries that can now accelerate photon. And this is such a big deal. We've been working on this for a long time. Yeah, for many years. So now we're going to accelerate photon and make it possible for all of you to wrangle data, process your data a lot faster, a lot more cost effectively, and very importantly consume a lot less energy. Yeah, makes a lot of sense. Huge deal. Yeah. It makes a lot of sense, right? Because in the end of the day, even though it's very complicated and it has a lot of corner cases, it is highly paralyzable. And it is specialized still. You don't really need generic compute for that. We want to do it like, you know, same thing again and again and again on Xabyte to data. We're not doing Xabyte to data that's completely unique. So I'm very, very excited about this. And I think it really has the ability to revolutionize and really bring faster performance, lower the cost and just, you know, it's going to be amazing. Yeah, look what happened when we're able to process enormous amounts of data so quickly. It made it possible for researchers to one day wake up and say, guess what? Let's just go get all of the data on the internet and train a giant model because it doesn't take that long. Without acceleration, without accelerated computing, nobody would have ever conceived of doing that. It would have been way too expensive or taken too much time. But now, you know, it's kind of a mundane thing to do. So, you know, we're going to be able to process Xabytes and Xabytes of data so much more cost effectively and so much more efficiently from a time perspective. Imagine all of the ideas that you're going to have. It's going to be, hey, let's just take all of the data of our company and we're going to train our super AI. You're going to do it. Yeah, the day is going to come. Yeah, I mean, it was a sci-fi idea, right, to take the whole internet. Nobody thought you could do it. We needed the hardware to get there, the infrastructure to be there so we could specialize it. And now, you know, everybody's doing it. So, I want to switch gears. God, I love myself. Just kidding. We love you too. So, I want to switch gears. So, you know, this generative AI boom has been amazing. You know, but the early days, you know, most enterprises started with chatbots. Let's build it on chatbot, you know, customize it on our data and so on. But now we're seeing people branch out to more and more sophisticated use cases. What new applications in AI are you the most excited about going forward? The number one most impactful will probably be customer service for all of the enterprises that are here. Customer service, you know, represents probably several trillion dollars worth of expenses. And every company has it. Every company has it. Every single industry has it. Every company has it. And the important thing about the chatbot, the customer service, is partly about the fact that you could automate, but it's mostly about the data flywheel. You want to capture the conversation. You want to capture the engagement in your data flywheel. It's going to create more data, of course. We're probably, you know, right now we're seeing data expanding about growing about 10x every five years. I would not be surprised to see data growing 100x every five years because of customer service. And so we're going to connect everything into a flywheel. It's going to collect more data, capture more insight. We're going to extract better intelligence out of it, which will provide better service. Maybe it's even more predictive in the sense that proactive in the sense that before a problem even arises, you reach out to the customer and say, you know, this thing is about to expire or we notice that you're still using this version or whatever it is, and you reach out to the customer and proactively solve a problem. Just like preemptive maintenance, we're going to have proactive customer support. It's just going to create more data. We're going to write that flywheel. And so I think customer service is probably going to be the most profoundly supercharging capability for most companies because of that, because of the data it's going to collect. But we've tokenized everything. I'm excited about the fact that we're generating chemicals. We're generating proteins. We're carbon capture materials, carbon capture enzymes, incredible batteries that are being designed. And so we're generating physics, physical AI. And recently we made it possible to do regional weather prediction down to a couple of kilometers. Now, it would have taken a supercomputer about 10,000 times more capability to be able to predict weather down to a kilometer. And now we're using generative AI to do that. And so as a result, logistics will be enhanced, insurance will be enhanced. Of course, keeping people out of harm's way will be enhanced. And so physical things, biological things, of course, generative AI for 3D graphics, digital twins, creating virtual worlds for video games. I mean, generative AI is just everywhere, every single industry. If your industry is not involved in generative AI, it's just because you haven't been paying attention. It's everywhere. Yeah, I totally believe we're going to see, there's no area where we're not going to see applications of this. Makes a lot of sense. It's so exciting. These new frontiers are super exciting and there's huge needs for data, AI. What's your thoughts on how we can help enterprises make AI that's more sustainable? Well, sustainability has a lot of different perspectives. One of the sustainability has to do with energy. And remember, AI doesn't care where it went to school. We don't need to put AI training data centers near population where the energy grid is challenged already. We could put it somewhere where it's not challenging. And so you know that the world has, Earth has a lot more energy. It's just in the wrong places. And so I think for the very first time, we can go capture that excess energy, compress it into an AI model, and bring these AI models back to society where we could use it. That's one major thought. And another is, I remember that AI is not about training. AI is about inference. And it's about the generative capabilities of the AI. You're training the model so that you could use it. And when you think about the longitudinal benefit of AI, and I just gave you the example of predicting weather using AI instead of using supercomputers, we understand basically the laws of physics that's involved in weather prediction. We don't need to simulate it from first principles every single time. We have to generate it using AI. And by generating it using AI, not only do we reduce the amount of time that it takes, improve the resolution that we can generate for, but also the amount of energy by thousands of times, not tens of, you know, not percentages, thousands of X factors. Well, by doing that, we're doing the same thing by designing chips that you're using in cell phones. You train the model once, design better chips with those models. As a result, you save energy for everybody involved. Just when you think about the longitudinal benefit of AI, I'm fairly certain that it will demonstrate the amount of energy that's saved. And then one last thought about generative AI. You know that today, and why is such a big deal from a computer science perspective? Today's computing experience is retrieval based. Largely, you know, we touch the phone. And even though when we use our phone, we think it uses very little energy, every single time you touch it, it goes off and sends, activates REST APIs all over the world, retrieves information. The internet is on, you know, lit up, brings back a little bit of information for you from all these different data centers, assembles it based on a recommender system, presents it to you. Well, in the future, it's going to be more contextual, more generative, right there on the device, running a model, a small language model. The amount of internet traffic will be dramatically reduced. And it'll be much more generative with some retrieval to augment, right? And so the balance of computation will be dramatically shifted towards immediate generation. Well, this is very, you know, it's going to save a ton of energy and it's very sensible. And the reason for that is this. Imagine every single question that Ali asked me, I got a race back to my office, go get some files, bring it back and present it to him, let him decide which piece of that information he wants to extract for himself. Instead, I'm generating everything, you know, from about 25 watts right now as we speak, right? And so the amount of energy that we save is going to be extraordinary and the computing model is going to transform completely. And so this way of computing is going to save tons of energy. Of course, we're going to get our answers a lot more efficiently instead of us combing through stuff. But then we'll have even more questions, right? We'll have more questions, which is really, in fact, that's the big idea. A big idea about the future, us working with AI's is prompting, we're going to have so many more interesting questions, because we're going to get a lot of answers very quickly. Yeah, so this is a very big deal. Very exciting future. Okay, my final question to you. How do we help customers, you know, organizations here get started today? What's the best way? Well, you know, I told you before that I thought the pivot of data bricks expanding from data processing to data governance and store and then extending it into all the way longitudinally, all the way to extracting intelligence out of that data. I think that that was completely genius. And I forget her name, but I thought Cookie Lady did an incredible job. Casey. What's that? Casey. Okay. Don't steal her, please. I thought she did an amazing job. I was enjoying, we were backstage and everybody wanted to talk, but I just wanted to watch her give her demo. I thought the platform is incredible. And you've made it easy for people to manage their data, extract information, process that data, wrangle that data. You know, wrangling data is still a very big part of training the model. People talk about training the model, but long before you train the model, you got to go figure out what data, right? It's about data quality. It's about data formats, about data preparation. And so I think the way you start is come to data bricks and use the data bricks data intelligence platform am I right? Yeah, absolutely. Who wouldn't call their platform dip? Such a good idea. You know, dip data breaks dip. Sounds good. I like it. It's almost as good as nims. All right. All right. There you go. Well, you can do both together, right? Yeah, you don't have to go get yourself a nim on dip. Yeah, I agree. Why not? That's the way to do it. That's that's that's absolutely start whatever you do just start whatever you do start you have to engage you have to engage this incredibly fast moving train. Remember, generative AI is growing exponentially. You don't want to wait observe an exponential trend. Because in a couple of years, you'll be so far behind. It's incredible. Just get on the train. Enjoy the train as it's getting faster and faster exponentially. Learn along the way. And so, you know, this is this is one of those things you can't learn by watching, you don't want to learn by reading about it, just learn by doing. And which is the way we're doing it. And so just get engaged. All right, that's great advice. Jensen, it's been an amazing decade. Thank you for everything. We've been great partners looking forward to our next decade together. Databricks. All right. All right. Okay, so we're going to shift gears. Okay, so we're going to go back to the core of data platforms. What do data platforms do data processing? We're going to talk about data warehousing. Okay, so I'm super excited to welcome on stage my co-founder, Randall Chin. He's been leading the data warehousing revolution on Databricks and assembled the world-class team and they have amazing results every month. They keep improving the platform, making it better, faster, cheaper. Someone welcome to the stage, Randall Chin. Thank you, Ali. All right, morning. It's a little bit hard to top off after Jensen and Ali. So we announced Databricks SQL, the prior preview of it about four years ago. And ever since, we've been humbled by reception. Databricks SQL become the fastest growing product in the history of Databricks. And today, over 7,000 customers worldwide, large and small, including Shell, AT&T, Adobe are using Databricks SQL for their data warehousing workloads on Databricks. But one of the fundamental reasons why Databricks SQL is taken off so quickly really goes back to the idea of the lake house itself. Before lake houses, here's what a typical enterprise data architecture would look like. You might have one or multiple of data warehouses for your business intelligence workloads and what I call looking back in time. And you have one or maybe multiple of your data lakes used by your data scientists, data engineers, and AI engineers for building machine learning models looking to the future. And the two disparate stacks were really incompatible. They have different governance models, have different storage formats, proprietary versus open, and that led to a lot of data duplication, data silos, and honestly, a governance nightmare. So when we looked at this problem a little bit over four years ago, we came up with this cute little term called the lake house. And the idea of the lake house is fairly simple at the high level. Let's marry the best of both worlds from data warehouses and data lakes and combine it into a same package. But the technologies weren't completely in place back then. We have to build a lot of stuff. We have to create Delta Lake for the foundational storage layer. We have to create Unity catalog for the governance layer. But over time, the writing become on the wall, the lake houses will be the future. And even proprietary warehouses started talking about lake houses. Forester, a leading analyst firm, even come up with a whole new forested wave called the lake house. And we're very proud. Databricks, by far, the leader of the forested wave. Now, going back to data warehousing, one of the most important workloads on the lake house is the ability to support data warehousing workloads. Databricks SQL is our product to support that. Some of you, many of you are among the 7,000 customers using Databricks SQL every day. Some of you have never tried it. Some of you might have tried it three or four years ago and informed your impression since. You might still have this impression that Databricks is great for data engineers, for data scientists, for the super technical people with PhDs in computer science, but not necessarily for all your analysts or your business users. But what we've really done is we've changed so much of the platform from every single layer that now the platform looks completely different. And to give you one example would be when we first released Databricks SQL, it would take about 370 seconds to acquire a warehouse for compute. But today, the number is down to less than 5. So that's a more than 70 times improvement just in three years. And that's just one example. We looked at what are the most important fundamental areas and we picked three and we really worked on every single intro bit. And this includes core data warehousing capabilities, out-of-the-box performance, and ease of use. Let's get started with core data warehousing functionality. Of course, to support data warehousing workflow, we needed a lot of features and functionalities that were not available in the Lakehouse. We needed full NCCC support, we needed materialized views, we needed role-based access control. These were not features that were available out-of-the-box back then. So we built all of them. And building on top of those, we now have built a very large data and AI partner ecosystem. So all your favorite tools were out-of-the-box for data warehousing, and especially in the area of business intelligence. Tableau, Power BI, ThoughtSpot, Luka, Sigma, click, all of those just worked out-of-the-box on the Databricks SQL. And this really substantially lowers the migration cost from traditional data warehouses over to Databricks SQL. The second area that we spent a lot of time on is price performance, out-of-the-box price performance. And price performance is one of the most important, typically one of the most important evaluation criteria in data warehousing POCs. And for two reasons. One is data warehouses tend to be one of the most expensive, if not the most expensive business systems out there. You spend a lot of money on it. To be able to save on it is essential to your operational efficiency. And the second is you want to guarantee all of your analysts and business users have the best class and experience. When Monday at nine o'clock, all of them come to work, start opening their dashboards, thousands of queries, hitting the data warehouse, you want to guarantee particularly low latency. So one thing I hope was clear from past data and AI summits is Databricks has always been really, really good at ETL performance. Last year, this is not a new chart. Last year, we published a study that would compare Databricks SQL versus a leading cloud data warehouse on how they would perform as we vary and grow data volume, data in size. So we started with 100 gigabytes, we grew all the way to 30 terabytes, we're just benchmark how well ETL performance would work against these two different platforms. Initially, the two compare fairly similarly when the data set were small. But as you scale the data set, the economics of Databricks SQL really start to show because Databricks SQL scale roughly linearly in terms of cost, whereas the data warehouse scale exponentially. And it might sound surprising, but in practice, it's actually not that surprising to think about it. The whole foundation of data warehouses were built to handle initially transactional business data out of OLTP databases, which tend to be relatively small. So the whole system designed from ground up to be really optimized for smaller amount of data. And when you do POCs, you typically don't load enormous amount of data into the warehouse. You grew your data over time. So they really tried to optimize for the earlier stage. But in Databricks SQL, because we come from a more of a data lake heritage, we really make sure the whole system was scaled far beyond 30 terabytes, even to petabytes range. Now, at last year at this conference, I gave a very different style of keynote. I gave a whole technical talk about how we're using AI systems to improve data warehousing engine performance. At the time, we knew AI would be something, but we didn't have any idea how big and how important it would be. In the last 12 months, we've revamped almost every single layer of our engine to incorporate AI systems in them. And this goes all the way from a bottom physical data layout, the middle layer query engine, and the top workload management scheduling. And then we're really seeing with our own eyes how much the AI systems can improve. And honestly, I think most of us, myself included, underestimate the impact AI system would have. At this point, you care. Tell me more. Show me the actual numbers. But before I do that, let me just show you a couple of examples of things we've done to give you a little bit more concrete idea. And some of these examples might even sound abstract and difficult to understand, but that's okay. The whole point is you don't have to understand all of it, but you can just see the benefit with your own eyes. The first example is Michael would tell you more about in the Delta Lake talk tomorrow, about liquid clustering, which is a new fundamental breakthrough in data clustering. But on data bricks, we have built AI systems to learn based on your workloads and automatically actually suggest and cluster data for you. No need to suggest the type of columns you can cluster by, but it also actually decides should we run all the operations online as part of the ETL workloads so you get the fastest time to clustering, or does it run it offline with minimal impact on ETL workloads. The same AI system is also applied to statistics, which is one of the most fundamental ingredients in career optimization for warehousing. The AI system would actually decide what type of statistics do you need on different types of columns, how do you actually minimize the amount of overhead, do we apply higher fidelity statistics, or do we apply actually more sketches, depending on the type of workload you need. And one concrete example I gave you last year, I saw you there remember, is this thing called predictive IO. And at the time I joked about the term indexes, which is very difficult to pronounce. The whole idea is predictive IO applies a machine learning model to predict where the data would be and give you the benefit of indexing without the overhead, the rewrite amplification of indexing. Thanks to the Mosaic AI stacks improvement over the last 12 months, we now can employ a model as an order of magnitude larger in terms of parameters and feature vectors in predictive IOs. So we just wrote that predictive IO 2.0. This is not something you have to opt in, it just works automatically under the hood, but you can accelerate way more workloads beyond the simple selections I've shown you last year. So, okay, numbers. There will be a lot of numbers in the rest of the talk. You've all seen charts like this, a vendor showing some benchmarks and beating a competitor by a wide margin, right? Every single vendor talk you've been to show that. I'll show you some talks, decks charts like that too, but this will be the first time I bet you've seen a vendor's own chart in which the vendor didn't perform favorably. So I think it's sort of conventional wisdom. If you have a PhD in Databricks optimization, you could get the best price performance out of Databricks one way or another. But what I really wanted to make sure is if you don't have time or you don't have that expertise to optimize your system, can you get the best performance out of the box without doing anything? So we created a synthetic benchmark exactly for that scenario, which is continuously ingest a lot of data. And then we run the TPC queries against them. And when we first did that benchmark in 2022, about two years ago, the data warehouse we benchmarked earlier was actually doing better than Databricks SQL. Last year, when we incorporated more of the AI capabilities, the two systems got to roughly on par. But something fairly magical happened last year, in the course of the last 12 months. As the AI systems evolved and become larger and larger and more sophisticated, as you learn more and more workloads, it actually got 60% faster out of the box compared to 2022. And it now has industry leading performance. And this is completely out of the box, no tuning required. All right. Now, some of you would ask, hey, you started the talk by talking about performance and thousands of analysts hitting the system. But that benchmark seemed to be about running individual queries. So we also created a scenario, a benchmark for the scenario really matter, which is 9am Monday, everybody opens their dashboard. In this benchmark, we created a bunch of robots that just keep hitting the warehouse using a variety of BI queries. Robots don't take coffee breaks. So each robot is probably like 10 times the normal human being. And we again just compare a leading cloud data warehouse versus Databricks SQL. When there's lower degree of concurrency, Databricks SQL and the leading warehouse would have roughly the similar low latency. But as we continue to scale and adding to 512 users, the difference really started to show. The warehouse had trouble scaling to the large number of users. And 512 robots probably simulate tens of thousands of users at this point, where Databricks SQL continued to deliver the same low latency. Now, some of you actually, all of you probably also think, hey, I don't trust benchmarks to show, even though you say you simulate real world scenario, but I really only care about my own workloads, my own real world scenario. That's exactly what I tell our engineering team when they show me all these benchmarks. And I said, hey, I really only care about how our customers queries would actually perform. So we took a stab at this. We looked at all the queries, all the BI queries are repeating on Databricks SQL to establish a baseline 2022, about two years ago. And we measure their performance over time, which tracked them for the same query. How well they performing over time. And today, this year, in 2024, the same BI queries will run on average 33% faster. What does 33% or 73% faster? What does 73% mean? A query that used to take 10 seconds now takes 2.7 seconds. It's almost 4x faster. And the best thing is, this is without you having to pay a single dime more, without you having to do anything. It's just a system getting better and better under the hood. It's because how much you're obsessed and how much we know performance matters to you. The last part is of use of use. Again, I said earlier, so you have the impression that Databricks is great for data scientists, data engineers, but not necessarily great for analysts. So in the last few years, we've heard a lot of feedback. We've revamped the user experience completely. But there's really no way for me to show you how much different you've gotten. You really have to try yourself. The best I can do is show you a few screenshots that demonstrated before and after. One example would be, hey, if you want to look at the data lineage or history, or how data are created, in the past, you have to run a SQL query to get a tabular result. But this year, all you need to do is the system will show you visually how the entire end-to-end lineage of all the data sets, and not just for tables, also machining models. Error messages used to be daunting to the last technical user. Engineers might love it because they see stack traces. They point out exactly which line number have the issue. But this is very daunting to business users. Now we actually output a really simple error message to error codes they can Google. But even better, the AI models on the side would actually recommend an automatically fixed error for you. We also added a lot of quality of live improvements when it comes to SQL features themselves beyond what standard SQL would give you. This is like SQL UDFs, lateral column aliases, session variables. All of this is a feature once you start using. You ask yourself, how come not every warehouse has this feature? What have I been missing out on in the last 10 years? But of course, what we really think we can leaf log is through the use of AI. Databricks Assistant is extremely popular, has been extended to cover all cases of SQL. We introduced AI functions in DB SQL. So all of your analysts can now get the full power of off the shelf, both open source and proprietary large language models out of a box. But even better, your AI engineers can build a new model, publish it to the catalog, and make it immediately available in DB SQL for all your analysts to consume. Same thing is applied to most AI stacks vector search. DB SQL can now actually create the vector index automatically. Now, rather than me showing more about this, I would like to invite Pearl onto stage to show you a live demo of what all this looked like. Pearl. Thank you, Reynolds. So as Reynolds shared, we're using AI to better your Databricks SQL experience by making the lake house much simpler and more powerful. So right now, I'm actually in the SQL editor, and I have some revenue data for a few shops plotted as a time series graph. And this is great and all, but I would love to see what revenue could look like several months from now. Better yet, it'd be great if I could use AI to predict this. So I actually can with our AI functions, and I'm going to go ahead and use the assistant here to help me do exactly that. So I'm going to add a forecast through September 2024. All right. So it looks like the assistant took my original code here, and it's unioning it with the forecast results, and it's doing that by just doing a simple function call to my AI forecast function. This looks good. So let me accept it and hit run. Just like that, I have my revenue forecasted, no Python or data scientists needed. But this revenue trend, it's kind of bad. And I definitely want to share this with the management team with their BI tool of choice, which is Power BI. So I'm going to use the assistant again here, and I'm going to create a materialized view that I will share to our published to Power BI. All right. Assistant is hard at work. I'm going to accept that and hit run. So why a materialized view? Materialized views are really great because it accelerates queries, especially those that use AI for downstream users. Okay. My materialized view is done and is actually a governed object in your needed catalog. And I can search for it here. All right. Let's go ahead and publish this bad boy to Power BI. As you can see, we have some offerings for Tableau as well. And I'm going to leverage the Databricks SQL Compute Engine. I'm going to direct query to Power BI. And this connection is made by securely through my SSO login. I'm going to select all the columns here, build a column chart, let me resize it. And this can be shared with the management team now. So let's head back to the SQL editor and do more AI-powered analysis. So here I already have a query built out, and it's going to show me a bunch of Yelp reviews. And I'm actually going to go ahead and use AI query, which is actually an AI function here. And it's going to call a custom model that's going to build responses to those reviews. AI query is great because it also allows you to call foundational models like DBRX, but also external models as well. So here I have all my reviews along with the responses from my custom model, but I'm more focused on the reviews. I kind of want to see how people feel about our food and drink. So let me add a filter here. All right. Food. It looks like the filter gave me an exact match for the word food. That's perfect. Let's try drinks. There's no word match, but that's okay. Because thanks to our new vector search function, we can actually use, we can actually search using an embedding model for related reviews in regards to drinks. And this is awesome because it doesn't have to be an exact word match. As you can see, I have Brutaspresso, I have a rich cold brew, a Brutacappuccino. These are all drinks. So what the filter couldn't do, vector search can. So I'm going to build a parameter here. Data Bricks makes it super easy to do so. And I can easily search for reviews about any item. For example, here I'm doing pastries. All right. So we have all the reviews here, but I need to do a little bit more analysis. This is kind of a lot. So I'm going to use an AI function. As you can see, we have quite a few to choose from, from fixing grammar to translating text, but I'm going to use analyze sentiment to perform sentiment analysis on the review column. Okay. Let's run that. So this should give me another column that has sentiment, a sentiment value on it. And it looks like it does. I have my review along with my sentiment. I literally just built a mini rag and sequel. So now I'm just wanting to plot this so I can understand the distribution of sentiment. I'm going to hit save. And it looks like pastries has a pretty good reputation. But let me try service quality. The store will need to know this to improve if needed. And this viz makes it super easy. So it looks like service could definitely use a bit of improvement. Look at all those negative reviews. So to wrap, I shared with you how Databricks sequels, data warehousing capabilities allow you to query AI functions, call models, and make your data work for you. So with that, let's get to work. Back to you, Reynolds. Thank you, April. All right. So in the last four years, we really revamped the entire experience of data warehousing. Databricks sequels today look nothing like the Databricks sequel when we first released it. We made it, we implemented almost all of the core data warehousing capabilities so we can easily, lowest cost migration over to Databricks sequel. We provide best in class out of a box performance. And we also made it dramatically easier to use the Databricks no longer just a platform for data scientists and data engineers, but also all your analysts. And all of this are really possible with built by building on top of data intelligence stack. And when you combine, that's why we say the best data warehouse is a lakehouse. Because all of us would rather stay in an open area environment of the lakehouse rather than being trapped in a data warehouse. We feel a lot more productive like that. In this case, the lakehouse is also a hell lot cheaper than the warehouses. Thank you. Awesome. All right. So I'm really excited about the next talk. Okay? So a year ago, over a year ago, we asked a team that was not really sort of working on the data warehousing stuff to sort of splinter off. And we said, hey, imagine you're not working at Databricks and you have to use generative AI to completely disrupt and build something from scratch. And the next talk is going to introduce that product. Okay? So I'm very excited to welcome on stage Ken Wong. Awesome. I'll do this. Boom. So as Ali mentioned at the beginning of today, our mission at Databricks is to democratize data and AI for everyone. Now, for most people, using data today basically means using reports and dashboards. Reports like this one. Now, this is an actual real dashboard that one of our product teams have built. It's all blurred here, so you can't see what it actually says. But it started off as a simple, straightforward dashboard that told us exactly what we needed to know about the state of the business, just a simple, straightforward dashboard. But as we started looking at the data there, we had new questions and we built some new queries and built some new visuals. And we started adding it back into the dashboard because all that query building took a lot of time. And over time, we added them back. And it started looking like this. It was sort of a hot mess. And, well, this is sort of the fundamental challenge with dashboards and BI reporting today is you have to know the questions you want to answer and you have to build them into the asset in order to answer those questions. Now, in the last few years, there's been this excitement about using AI to solve this problem. And when LLMs came bursting into the scene, everyone in the market started just rushing to bolt on some off the shelf LLM to add AI-assisted capabilities. The idea is super compelling, right? So you saw how great the assistant was at writing SQL. So if you just bolt on, you can create an experience where the user can just ask questions in natural language and just get answers. And how everyone in the industry has a demo like this, right? You can just ask questions and get a beautiful dashboard. No more data scientists, no more data analysts. Everyone gets the data they need. But is that what's really happening in the real world? Well, we took a look and we actually tried some of the leading BI tools that are available today, including one that I personally think is really leading the field, especially in this area. And we used the realistic sales opportunity dataset and asked it a simple question, which is, how's my pipeline? How's my sales pipeline in this case? Now, these are some of the answers that we got. One vendor showed us that we had a whole bunch of nulls, which was truthful to the data, right? We did have a bunch of nulls, but this was not exactly useful. Another tool told us that, well, I have no pipeline at all. And the reason for that was that it generated some, you know, realistic looking sales stages and thought that thought, you know, we should have, but we didn't have those. And so it's told us we have no pipeline. And a third vendor told us, well, just try again. I don't actually understand your question, because you never defined what pipeline was in the semantic model underneath our tool, and so we don't know what pipeline means. Now, I'm showing you this not to make fun of these tools, because these are actually great BI tools. And I bet with a little bit of work in wrangling or semantic modeling, we can get all these tools to work and answer this question. But my point is actually much more fundamental, which is this idea of just bolting on a generic LLM simply isn't enough to realize the transformative power of AI and BI together. And that's because generic LLMs just don't understand the uniqueness of your data. They don't understand how messy your data is and all the tricks that your analysts and data scientists have put in their dashboards, queries, and notebooks in order to handle it. And they also don't understand the domain-specific semantics that you have. Jensen talked about it a little bit earlier. And every company has their own concept of things. Everyone knows what churn is, but the churn is different for every single company. Now, the traditional BI approach to solving this problem is to stuff everything and pre-model it inside a semantic layer. And semantic layers are great. Symmatic layers are super powerful, but at the same time, the reality is trying to model everything in your enterprise is also just not realistic. And the good news, though, is that we think AI actually can solve these problems, but it requires a ground-up approach, not bolting on something, starting from the grounds up, which is what we've done. And that's why I'm so excited to announce today that we are launching Databricks AI BI and AI First Approach to Business Intelligence. Now, we think this is really the first step in really, truly democratizing data and analytics for everyone. So what is it? What is AI BI? Well, it's BI, so it contains dashboards. And our goal here is just to simply provide the quickest, simplest way to build a dashboard in Databricks and share it with a lot of people. Now, AI BI dashboards might not have all the bells and whistles of our amazing BI partners, but it covers the basics. It has a no-code drag-and-drop experience. It has scheduling. It has exporting. It even has cross-filtering, as the animation is showing here. And it's all built to be super fast and deeply integrated into Unity Catalog. And it's built into Databricks SQL, so you don't have to manage some separate service. But the really cool part of AI BI is that with one click, you can flip to the other experience. We call it Genie. Now, Genie is a conversational experience similar to chatting with an analyst or type chatting with him on Slack. And it takes in regularly express business questions in English and answers them back in visits and queries. Now, that might sound almost exactly like the same pitch that I was just telling you about. So, what makes Genie special? Well, I think you guys are all experts in compound AI systems now, because we talked so much about them. But it's a compound AI system that continuously learns the unique data and semantics of your business. It uses an ensemble of specialized AI agents that uses different LLMs working in concert. And these agents are able to leverage the tool and context that's available as part of the data intelligence platform. So, this includes things like Unity Catalog metadata, like PKFK constraints, all the comments that you've put in there. It also includes the execution query history of all your different workloads. And this is how it's able to pick up contextually how all the unique logic that your business uses. It also uses related assets and look at things like notebooks and dashboards and queries to understand the context as well. And if a semantic model is available, it'll also use that. Now, all of this is connected in an agentic workflow. And we've taught it in such a way that it goes back to the user to seek clarification when it's not able to infer the answer from all the tools that's available to it. And the really special part is it remembers these clarifications. So, it continues to learn and get better and better so that it's able to actually answer really complicated questions far beyond what you could pre-model in a semantic model as it kind of learns over time. That's a lot of slides and it's very abstract. So, the best way to see what AIBI is is obviously to see it. So, I'd like to invite Miranda Luna, Product Lead for AIBI to the stage to show you. Thank you, Ken. Hi, everybody. My name is Miranda and I'm a product manager here at Databricks. And it is my distinct pleasure to show you AIBI in action today. So, why don't we do the BI side of the house first? Dashboards are part of our lives. This is some CRM data. They're not going anywhere. We are really excited to move beyond there. But I'm just going to create this guy real quick. Let's do a bar chart of opportunities by region, perhaps. So, let's see. Let's ask for a bar chart. Pipeline, region. Shorter, accept it. And then let's also do this little switch on this side here if we want to point and click. So, you've got options. But if I want to break glass and go to the actual SQL, let's get some comments here. So, this is the same guy. Pearl showed us. I'm going to accept it. So, now, this query is doing a couple things, right? It's helping clean up some messy data. It's defining pipeline at stage 234. And it's getting all that context from the platform, right? It knows this because it was in a notebook. We're a little behind today, so I'm going fast. I'm going to publish. And then if we come on over here, that's what you expect to see in a BI tool. I'm going to go ahead and make sure that we're moving ahead. So, we are on the canvas. And perfect. Okay. So, we're going to go ahead and scoot on over to the canvas. And this is where I'm going to go double check that we have everything we expect from the dashboarding layer. So, we're going to go ahead and publish. There we go. And now, we're going to go ahead and take a look at what we have here in publish mode. Now, of course, I can share this dashboard with anybody in my org. You know, I can share that with folks that don't have access to the workspace, which is exciting. I can certainly also make sure I have PDF subscriptions. I can take a look at the lineage. I'm pulling that up here. So, we've got that connected. You can kind of see what I'm highlighting. And then, of course, I can cross filter. So, I'm going ahead and doing my favorite point and click here, which is fantastic. But this is not where the demo ends, right? The exciting thing about AI BI is that we're not done just at the data viz layer. We're going to go ahead and launch Genie and ask new questions instead of asking an analyst. Let's do how's my pipeline? How's my pipeline? Ken's favorite question. I'll leave that part out. And, yep, looks like we've got it by region. Love to see that. Perfect. Let's slice and dice. Americas, maybe? Let's tap. Americas. And let's maybe look at it by segment. Perfect. So, we've got our segment here. And let's go ahead and do a pie chart. Cool. So, while Genie is getting that going for us, let's take a look at the code. So, the code here is actually going to be exactly what we saw on the dashboard side of the house. We're filtering out these forecast categories are null. We've got demo stage, validation stage, procurement stage. And we are, again, ingesting all that context from the fact that it's in our pipelines and such. Perfect. So, let's go ahead and ask a new question. Let's ask something that was not in the dashboard. Let's ask about sales reps. Maybe who's our top rep? Let's take a look at that. Who created the most pipeline? Perfect. Great. So, let's see who's doing the best here. Lauren. Okay. Snaps for Lauren. How does Lauren compare to our average reps? So, compare to our average rep. Perfect. Awesome. So, let's go ahead and take a look. We confirmed we're getting kind of some different numbers there. But, Ken promised us we couldn't just get through messy data. He promised us we couldn't just get through specifics to the business. He told us we could plow through missing semantics. So, let's ask Genie something it doesn't know. Let's go ahead and ask about churn, perhaps. Okay. Great. So, Genie did not hallucinate. We love to see that. It's asking us to tell it more. Instead of me having to go find someone with permission to update the semantic layer, I'm just going to tell it what churn is. I know what it is. So, let's go ahead and do that. Churn is when we lose an op and don't win another after that. Perfect. Let's see, Genie, can you learn this? Perfect. Okay. So, we've got some churned opportunities right here. And, you know, the nice thing I'm going to do now is I'm going to hit this Save as instruction. So, when I save it as an instruction, that's going to let everybody else in the company leverage that same definition. You know, I have a colleague in Europe, they want to come in, they want to ask the same thing, they are going to be able to. And if they need to modify that definition, they can. So, let's be that colleague in Europe now. Let's go ahead and who churned last quarter in Europe. Perfect. So, it looks like we're seeing a list of those churned customers. Again, Genie also connected the dots between Amia and Europe. So, we're going to go ahead and say that's a W. But let's actually see if Genie can extend this knowledge. Let's ask it to calculate a churn rate. So, can Genie calculate a churn rate? Let's say, again, in Europe, we're there, we'll stay there, Mediterranean, it's summer. And let's really challenge it. What was churn rate last quarter? Awesome. Okay. So, just like that, Genie was able to learn what churn is from me and apply it. Let's just toss this in a little viz real quick. Make sure we have a nice kind of chart of where everything looks like churn rate by region. Awesome. Looks like America is doing pretty well on a low churn rate. But this is great, right? We were able to see how we went from a dashboard to asking follow-up questions that were not in the context of that dashboard. And we were actually able to teach Genie some missing semantic information, have it then apply that in multiple contexts for multiple users, and for Genie to extend that understanding to new data. So, with that, I'm going to hand it back over to Mr. Ken, and we are so excited to see what you do with AIBI. Awesome. What do you guys think? All right. Now, as I said up top, anyone can make slides or throw together a demo. The hard part is really making it work on real-world data, doing real-world analysis, which is why we've made a point to start working with customers on Genie and AIBI from the very beginning. Customers like Accolade, Sega, Kathera, T-Mobile. But my favorite feedback actually came from a conversation between one of our essays and Brian Fox. He's the CTO of Sonotype. He had lots of different feedback for us, but this is how he wrapped up the conversation. So, I'll let you read it. I won't repeat it here. But this type of feedback is really why we're super excited about the future that's ahead of for AIBI. But the truth is this is going to be a long journey, and we're really just getting started. And we really truly believe in this idea that in order to solve this problem for real, we really need to work with real-world data on real-world problems with all of you, which is why I am very, very happy to announce that AIBI is available to all Databricks SQL customers today. So, you can start immediately building your dashboards. Those are fully generally available, and you can also just toggle on Genie in your workspace right now. Our goal really is to make it possible for as many people to be able to use AIBI. That's our mission. And along the same lines, we're equally excited about the possibilities of opening up our AIBI APIs. Well, that's a mouthful. Opening that up for all of our partners so that they can benefit from all the work we've done as well. We believe in partnering and creating an open ecosystem so that everyone can truly democratize data and AI no matter what tools and experience they use. Thank you. Awesome. Super excited about Genie and AIBI. Please try it out. That's really the pinnacle of data intelligence for us. To be able to really talk to your data, we're going to make it better and better and better. Give us feedback. And we're opening up the APIs as we said so that our BI partners can also leverage it so you can get the same kind of intelligence in your favorite BI tool. Okay. So this is great. I'm just going to wrap it up. Just give me one minute here. So what have we done today? You see this is the data intelligence platform. You can see it on the slide here. So we've covered, there's AKI, the generative AI stuff. We've covered Databricks SQL. Rainel did that. AIBI, we just heard that. And tomorrow we have awesome talks. Let me just summarize for you a little bit what we saw just today. So we saw on the tabular side, the acquisition, how we're going to bring interoperability on these formats. On generative AI, how we can build your own AI and build it into production on your own custom data. Data warehousing, we saw amazing performance. Okay. And especially on BI. So give it a try. It's getting better every year. And then, of course, AIBI where you can just talk in English. And tomorrow we're going to focus on workflows, how to do data engineering, and how we're going to actually run streaming pipelines, as well as Unity catalog. We're going to hear from Matei and the open sourcing. And then, of course, Delta Lake. And we're going to have Ryan Blue from Apache iceberg on stage here. Okay. So that's it for today. Please go grab your lunch and be back here at 12.30. This is a super awesome talk on Gen AI that a handling tank is going to give. The CTO from Mosaic. So runs all of the Mosaic AI stuff for us today. And we're going to also hear about DSPy, which is an open source framework that's super cool. Okay. Thanks, everybody. Data is big. But its potential is even bigger. Data intelligence holds the promise of curing diseases, saving lives, reversing climate change, and changing the way we live. We believe the future depends on data and unlocking its limitless potential with AI. Databricks is the data and AI company. We help companies take control of their data, put it to work with AI, and solve the world's toughest problems. Because the challenges we face as businesses, as people, and as a planet aren't easy. They can't be solved in silos. They are too important to be left to the few. We need all the data, all the AI, all the brain power. We need all hands on deck in one place on the new data intelligence platform. The only platform that brings AI to your data so you can bring AI to the world. And that changes everything. It expands our sense of what's possible. It makes things simple. It turns reactive into proactive so you can innovate faster. Because that's how innovation should happen, collaborative and fast. So let's defy assumptions, break the mold, map every genome, cure every cancer, binge watch the cosmos, make every verse, the universe count every vote, take more moonshots, and land them. And now on nothing stands between you and your data. You and the answers. Because the power of data intelligence is the power of knowing. Now you know.