# Summary

### Summary of the Technical Talk on Generative AI at Databricks:

- **Introduction**:
  - The speaker introduces the latest developments in generative AI at Databricks, discussing customer applications and industry research.
  
- **Frontier Models**:
  - Over the past 12 months, the landscape has shifted from having one high-quality frontier model to several, including open-source options. 
  - These large-scale AI models are trained on vast internet data, and GPT-3's breakthrough was due to the increased scale of training data.
  - The MMLU benchmark is used for evaluating these models on general knowledge across 50 diverse categories, similar to a Jeopardy-like breadth of knowledge.

- **Focus at Databricks**:
  - Databricks' emphasis is on applying AI to specific products and services rather than general-purpose AI. 
  - They aim to leverage AI for practical applications like customer support, code generation, and marketing content, enhancing effectiveness and contextual relevance.

- **Compound AI Systems**:
  - Compound AI systems customize general AI capabilities for specific scenarios and tasks, improving performance through tuned models and integrated tools.
  - A case study of the financial analysis company FactSet demonstrates these concepts, highlighting the transition from general models to highly efficient, specific solutions through tuning, search, and retrieval.

- **Mosaic AI Platform**:
  - Databricks' Mosaic AI platform facilitates customization of general AI models, integrating enterprise data to create specialized AI applications.
  - Zero-code fine-tuning of open-source models is a key feature, allowing enterprises to optimize models for their specific needs without extensive coding.

- **Case Studies**:
  - **Fox Sports**: Uses tuning techniques to create AI for generating live sports commentary.
  - **Atlassian**: Customizes AI for code and dashboard generation.
  - **Shutterstock**: Announces a new image model, trained entirely on proprietary data to generate custom images for marketing.

- **Retrieval Augmented Generation**:
  - Databricks’ search index capabilities allow AI models to access and leverage proprietary data.
  - Example: Corning uses AI integrated with proprietary research for efficient material exploration.

- **AI Tools and Agents**:
  - Introduction of the Mosaic Tool Catalog allows enterprises to build, publish, and utilize customized tools.
  - Mosaic AI Agent Framework enables the creation of comprehensive AI applications that can perform specific tasks and interact securely with business systems.

- **Quality and Governance**:
  - Importance of evaluation and quality measurement for deployed AI applications.
  - New tools in MLflow 2.14 help log, investigate, and improve interactions in AI systems.
  - The Mosaic AI Gateway provides centralized governance and control for deploying and managing AI models, ensuring safety and adherence to enterprise standards.

- **Live Demo**:
  - A live demonstration showcased how the Mosaic AI platform enables building AI agents to assist franchise owners in creating marketing campaigns, revealing tangible improvements in performance and relevance when using enterprise-specific data.

- **Conclusion**:
  - Emphasis on the necessity of moving from general to data-intelligent AI.
  - Modularizing AI into compound systems for better specialization and efficiency.
  - Databricks' Mosaic AI platform as the premier choice for building high-quality, customized AI systems.

Closing remarks included the introduction of Jackie Brossamer from Block, who shared insights on utilizing Databricks’ Mosaic AI platform for deploying generative AI solutions.

# Transcription

 Well, there's a lot of people here. Hey, everyone. Hope everyone's doing well. Yeah, thanks. It's great to be here and talk about the latest in generative AI, things we're working on at Databricks, things our customers are doing, as well as the latest in industry research. So I'm going to start with a slide from Elise Talk. It's been a super exciting 12 months ago, we had one really great frontier model that was a super high quality AI model, and today we have five or six amazing frontier models, several of which are open source. So to understand how we leverage these capabilities, it's important to know how frontier models actually work. So the way that these large scale generative AI models work is that they're trained on data from the internet. And in fact, the big breakthrough with GPT-3 was that the scale of training data went way higher than had previously been done, and it turned out that resulted in a much better quality model. So these models are trained on internet data, and then they're optimized and evaluated on how well they do on what are called general knowledge tasks. So the last slide I had a benchmark, that benchmark is MMLU. It's kind of the canonical benchmark for how these really frontier AI models are evaluated. And not a lot of people look inside MMLU, but it has 50 different categories in which they're evaluated on sort of general knowledge. So I like to think of it like if you're playing Jeopardy, it's kind of facts and information about many different categories. So I just pulled this up. Here's a few example categories that are in the MMLU benchmark. Some are expected, some are kind of surprising. So nutrition, world religions, astronomy, human aging is kind of a funny one you wouldn't expect. I actually played with chat GPT this morning just to test this. I started asking questions about wrinkle reduction. It happened to be having psychopedic knowledge of how to reduce wrinkles as you age. So they're all chasing these benchmarks and trying to get good at these particular topics. But what we work on at Databricks is not so much general purpose AI models, but it's actually helping our customers build AI capabilities into their products and their services. And those capabilities need to have a very deep knowledge of the context of that product and the data that powers that company and product. So what we're focused on is not so much these general purpose knowledge tasks, but it's how well your customers are able to benefit from your AIs. So for example, if you have a customer support AI, how effective is that in answering customer questions? Most people don't ask their customer support about wrinkle reduction. They ask about solving the problem that you have with your product. If you're generating code in your UI, how often are you generating good code that your users are accepting? And if you're creating marketing content with AI, is that marketing content on brand? And does it match your company? And the way we like to think about this is that what we focus on at Databricks is not so much pushing the frontier of general intelligence, which is an exciting task in and of itself, but it's really the data intelligent application. So helping you build data intelligence into your products and services. And today I'm going to share some of our findings working to build data intelligence systems with thousands of different customers. So I'll give examples of types of systems that are being built as well as the fundamental technologies that we're building to help those come to production. And this is actually not just an active area of interest in industry. It's a very active research area is how you take these general purpose capabilities and you adapt them to specific scenarios and specific tasks. And the sort of leading research in this area points to a solution called compound AI systems. This is a paper out of Berkeley that's just one of many different research groups that are looking at this problem. And what compound AI systems do is they take the general capabilities from leading AI models, but they customize it substantially. They do things like tuning models, adding retrieval and search to your to your model, giving your model the ability to use tools and take actions in your enterprise. And it's through these compound systems that our customers are able to build really, really high quality embedded AIs in their applications. So it's been a little bit abstract. So I can give an example here. So FACSET is a Databricks customer. And for those who don't know, FACSET is a sort of leading a company around financial analysis and builds products and services for basically people that are in the financial markets. And this is a picture of the FACSET UI. And though people can use the point and click UI in FACSET, what most of their power users use to get, you know, all the information that they have on equities, bonds and so forth, is they use a query language called FACSET query language or FQL. So this is an FQL statement in this box. I don't expect you to understand what it does. I'll get there in a minute. But the opportunity with Gen AI for FACSET is that instead of having people type these this sort of very specific query language they need to go learn, it would be a lot nicer if someone could just say in English or in their preferred language what they're trying to do and the FACSET software could just do it for them. So here's an English version of what that query is showing. Give me the current year and trailing in earnings per share for all US listed equities. So even I understand that and I'm not a FACSET expert. The opportunity here is that, you know, FACSET could be approachable to many more users and existing users could be much more efficient if they can just use language to say what they're trying to do. So how did FACSET build this? Well, they actually started by taking one of these general purpose frontier AI models and trying to just give it a few examples of their query language but simply just call into this existing model and have it translate from English to the desired query. And unfortunately this didn't work very well. The accuracy of this technique was only about, you know, 50-50 so it would like half the time generate the wrong answer. And it was also extremely slow because of the amount of context they needed to give the model was like 15 seconds. So that was not really sufficient to build into the FACSET product. So how did they solve that? Well, they built a compound AI system and I won't go into a ton of detail but that system involved tuning some open source models. FACSET happens to have a huge amount of data of existing queries with labeled English examples so they could tune a model that understands their data extremely well. They also included search and retrieval so they could go and search and look up things like if I, you know, mention a company's name but I don't mention the ticker symbol, you know, that could go search in a database and be able to resolve that. And they also tuned and customized other parts of the process and through this they were able to get 85% accuracy and triple the performance in terms of speed. So this kind of met the bar for what they were able to put inside of their product. So FACSET is just one example. They're actually giving a talk, I think, at the conference. So if you want to learn more you can go to their talk. But at Databricks we're focused on building general capabilities through our Mosaic AI platform that lets any company do this type of customization to go from generally intelligent models to data intelligent products and services. This is actually the result of an acquisition we did that many of you may have heard about of Mosaic ML and we're happy to announce today that those, the results of the acquisition are fully integrated now into Databricks products and services and I'm going to walk through some of the individual capabilities we're offering in this area. So as Ali mentioned, it's a life cycle from preparing your data, customizing models and deploying applications in production. And the first area of those three, we're happy to announce today a zero code fine-tuning of open source models in Databricks. So what that lets you do is start with a really high quality existing model that was already trained and then with no code, tune that model on your enterprise data to be really good at the particular task that you care about. Databricks will manage all of the optimization. You know, tuning is actually fairly complex. There's different types of parameters you can tweak. There's different ways you can do it. Databricks will fully manage that but you end up with a fine-tune model that you own and you can use that model in your AI product or service. There's actually several companies talking about their use of tuning on Databricks at this conference so I won't spill the beans on these talks but a few that you might check out. One is Fox Sports. By the way, I think Fox Sports gets the award for the coolest animations in their talks. I'm a big football fan. They have football animations throughout the whole talk so even just for that, I think it's worth going. But they're customizing AI using lots of data they have. They have 100 years of transcripts from NFL and other leagues of people discussing ongoing sporting events. They can use that to customize a model and to be able to generate live commentary and things like that. At Lassian, it's very similar. They have a ton of existing, they're using AI and their product to generate code and generate dashboards and so forth and they have tons of labeled examples of doing that so they've customized and tuned models for that purpose. So tuning takes an existing off-the-shelf model and kind of tweaks it to be better at the thing that you care about for your product or service. But in certain cases, companies have so much data that it's in their interest to actually fully build a model from scratch. So this is called pre-training in the ML language but what it really means is that you create a model entirely from your data that didn't touch the internet and didn't touch any other kind of data. So a great example of a customer doing that on Databricks through our Mosaic AI training platform is Shutterstock. And Shutterstock is actually announcing today at this conference a brand new state-of-the-art image model that they're able to expose and let their customers use. A little bit about what Shutterstock does, they're actually the world's leading or one of the largest databases of proprietary images and they can take advantage of that huge dataset and IP that they've accumulated over the years and let their customer generate totally custom images for marketing purposes or personalization. And unlike other image models that are trained on internet data, the Shutterstock model is completely trained on a trusted dataset that they have full rights to. So they're able to take this intellectual property they have and build an awesome model in order to share that with their customers and they'll be talking more about that model at this conference. So we're actually excited to share today that more than 200,000 custom AI models have been built on Databricks for use in enterprise AI systems. And boy, just the hardware required to do this is insane between GPUs and other types of AI accelerated chips. So our friend in the leather jacket that's going to be talking later, we owe him a lot of thanks for creating the ability for us to do this. So building the underlying models is one of the most important part of AI systems. But the next part is extending the model with capabilities beyond just basic data reasoning. By far the most popular way to extend models these days is something called retrieval augmented generation. That's really, the AI community likes to use fancy words, that really just means your model knows how to search. And what matters for enterprises is that you can have your model search over proprietary and custom datasets that you have. Databricks released earlier this year, the ability to host, to manage your data in a hosted search index, a vector database. And we're super excited to announce today this month that that offering has gone GA. We've also added much a state of the art embedding model in that offering as well. A little bit of an example of how that's used. Corning is a materials research company. And what they're doing with AI is they're building an AI for their internal research team. That's their core IP generation to be way more efficient in exploring different types of materials to research for industrial use cases. And they use our vector search engine to include tons of proprietary information on patents and prior materials research that only Corning has that's not available in general purpose AI models to make that app work really well for their researchers. So retrieval is a really popular technique, this search augmentation. But I do want to emphasize that it's just one type of tool of many. And what we're increasingly seeing at Databricks is our customer's desire not just to do search in order to do question answering, but to actually have tools and abilities that can manipulate and take actions based on the person who's using the AI. So a few examples of types of tools our customers have built. You may want to extend your AI to open or close support tickets if it's a support AI. You might want your AI to be able to execute a small amount of code on behalf of a customer and execute it in a secure environment. These are called tools and very similar to base models. Tools need to be heavily customized and written in a specific way for each enterprise use case. A general purpose model is not going to know how to interact with your business system, your ticketing systems, et cetera. So today we're actually announcing the mosaic tool catalog in Databricks. So what this lets you do is it lets engineers and scientists in your teams author tools. They can build those tools on top of secure compute abstractions and then they can publish those tools for use by other people building AI applications inside of your company. So this can separate the usage of tools from the authoring. The authoring may involve credentials or other sensitive information and it lets all of your engineers discover these tools and then use them. Now the real benefit of tools is not just the individual use of a tool but it's actually combining the tools to create what are called agents. Another fancy term, an agent just really means an end-to-end AI application that can do something on behalf of your customers or users. So a support bot would be an example of an agent. And we're also happy to announce today a framework for authoring, deploying, and evaluating agents inside of Databricks called the mosaic AI agent framework. This framework can work with your existing Wang chain or other chaining frameworks but lets you author your agent in Databricks, deploy it to an API endpoint, and test its quality. So my colleague Casey will actually demo soon a lot of these abilities around tools and agents that were super excited to ship. So the last piece of the puzzle is how you evaluate and understand quality. AI is a super exciting space right now because it's so easy to quickly demo and put some data in an AI and build something that seems to be pretty cool. But what's really important when you go from a demo to a deployed application is that you make sure the quality is really, really good. I'm actually kind of curious to ask, show of hands, how many people in the audience have done some kind of internal, they've either built or used an internal demo of AI on the data inside of their company. How many people have done some kind of demo like that? Curious. Okay, so it's like 60% of people. So we're seeing that everywhere. Every company is building internal demos but then they need to go from the phase of having a demo to a deployed application. And what's really, really important is that you make sure the quality of the generated content is good. We talked at the beginning of the talk that the whole difference between general purpose models and deployed AI applications is that you're optimizing for something that's not general knowledge. You're optimizing for closing tickets, for generating high quality code, for helping your customers. But it's important that you systematically measure that so you can make sure you're doing a good job at it and you're improving it. So the third piece of this agent framework that we built is an evaluation tool that helps you employ state-of-the-art techniques to measure the quality of the thing you're building. So the way it works is you can first give a few examples of high quality interactions. Then as you iterate and tweak your application, as you tune models, as you experiment with search or integrate tools, you can check is the quality of this thing improving or is it getting worse. And what's more important is that once you're at scale, once you have users, many users using your application, the bottleneck becomes how you get lots of really, really high quality evaluation data. So you may have hundreds, thousands of interactions and you want to understand are these high quality or are they working well? So our agent evaluation also lets you invite experts that could be part of your company or they could be external contractors to evaluate and score actual AI interactions. And finally, it lets Databricks learn how to train AIs to do the quality scoring, which can allow you to scale this up and actually evaluate almost every interaction that you're having inside of your application. MLflow 2.14, which comes out on Monday. The team promised me they're shipping it on Monday, so I'm holding them to it by talking about the keynote. They that will also include substantial quality tools around these generative AI applications, in particular the ability to log and investigate traces when you have low quality interactions. So that's that's often a big piece of the quality puzzle is if you had a user that gave thumbs down, the thing didn't work so well, what exactly went wrong? Was it an issue with the retrieval? Was it an issue with the way I tune my model? And so that'll be part of MLflow. And if you're using it in Databricks, we'll also deeply integrate it into the Databricks UI. Cool. So we talked about building customized AI systems, deploying them and understanding their quality. The last piece of the puzzle is governance. And it's a really exciting time in AI. Everyone wants to move super fast, they want to compete in their markets, they want to build AI based applications. But particularly for enterprises, it's very, very important that the AI that you deploy is safe and it's trusted. And an issue we've seen in a lot of our customers is sort of their victims of their own success in some way. You have one AI project that's really successful, and then you turn around and now you have 10, and now you have 20, and now you have 50. But how do you, as a sort of a global company, make sure that they're all adhering to the safety properties, meeting the quality that you expect and so forth? So a good example of a customer that had this type of challenge on Databricks was Edmonds, so they had tons of AI projects going into production, but things started to become a bit of a sprawl. So you had different teams that were managing their own credentials to some of these third-party model providers, the cost started getting really out of control, and the capacity management was also an issue. You know, GPUs are a scarce resource. You want to make sure that your most important applications have access to the most important, the resources that you have available to you for serving. So we're also excited to announce today the Mosaic AI Gateway, which is a central point inside of Databricks where you can enforce all the sort of auditability and governance requirements that you have around use of your models. So this isn't meant to slow teams down. It's actually, from our experience, helped individual engineering teams move much faster because they have access to a very specific set of approved base models. They have a set of guardrails that's been agreed on for the company that they can use, and they can actually innovate faster and not sort of each reinvent the wheel around these foundational capabilities. Awesome. So as I said, we've been working on technologies to help, not only the customers I mentioned here, but our 10,000 customers build and deploy AI products and systems. And I'm super, I always believe in show, not tell. So I'm super excited to welcome my colleague, Casey Ulanuth, who will actually be taking you through these in a live demo. She's doing it live, so caveat in case there's any issues. She's going to take you through a live demo of these capabilities in our product and how, for example, a customer might use them. So please join me in welcoming Casey Ulanuth. Thank you. All right. Thank you, Patrick. So, all right. Where are we? All right. So I work for a cookie conglomerate that has a bunch of franchises, and I want to create an AI agent to help all my franchise owners improve their business by allowing them to analyze customer data, allowing them to create marketing campaigns, and analyze and develop sales strategies. And so one of the things that they'll be able to build with this AI agent is an Instagram ad campaign, where they can promote the best-selling cookie in their franchise, and the AI agent is going to create an image for the Instagram ad, as well as a caption that's going to capture the hearts and minds of all of our cookie lovers to really drive sales. So I gave general intelligence to my franchise owners with the off-the-shelf model, and it was giving good results, but they were too generic, and they weren't tailored to our business or the individual franchises. And this is where the Mosaic AI platform comes in. Mosaic AI is going to allow us to extend this general intelligence with our enterprise data so that we can have data intelligence. In this demo, we're going to build, sorry, in this demo, we're going to build an agent that's going to use the Unity catalog tools that Patrick just mentioned. So in this architecture, we're going to leverage these UC functions that can now be leveraged as tools. And UC functions can be SQL functions that access your data warehouse. They can be Python functions. They can be model endpoints, and they can even be remote functions, which are going to allow you to call external services like Slack or email or even file a ticket if you need to. So to build all of this, we're going to use the Mosaic AI platform, and with that, we're going to go ahead and jump right in. So here I am inside. So there's three capabilities that we're going to use inside of Mosaic AI to actually build this data intelligence. So the first is we're going to use our tools catalog to actually build the data intelligence. The next, we're going to build and understand our quality with agent evaluation, and then we're going to be able to debug and improve our quality with ML flow tracing capability. So with that, let's dive in to Databricks. So here we are inside Unity catalog. You can see that I have some functions that I'm going to use as tools, and these are governed alongside my AI, my unstructured data, and my structured data. So to help demystify what a tool is, we're going to go ahead and click into our franchise sales. So you can see in here, it's just a simple SQL query that's accessing my sensitive transaction data, and this is where it's really important that your tools are governed alongside your data, because only the people who have access to this underlying transactions table are able to successfully call this tool, and that's why we need this centralized governance across data, AI, and tools. So some of the other tools that we've created that are leveraging our enterprise data are in here as well. So franchises by city and by country are just like helper functions to help me get the sales data, and then this franchise reviews tool is actually grabbing customer reviews from our social media site. So all of these tools are leveraging my enterprise data. So we're going to go ahead now and extend a base model with these tools. So I'm going to come over here into the AI playground, and we're going to jump in, and then from the AI playground, I'm going to select a tools enabled base model. And so I'm going to, you can tell it's tools enabled because it has this little icon on the right. So I'm going to go ahead and select Lama 3. From here, I'm now going to add hosted tools. So these are my Unity catalog tools that are hosted inside the secure and scalable Databricks environment. So in here, we're going to access the tools that we just showed you in the AI schema. So I can use the syntactic sugar to grab all of those tools. And then my marketing team has created a tool for me, so I'm just going to copy paste it real quick because it's kind of long. So we're going to put this in here, and this tool is going to generate an Instagram image using the Shutterstock image AI model that Patrick just announced, as well as a caption. So now it's time to actually test this. Before I forget, we're going to crank our temperature down to zero because this is a live demo. And then now we're going to quickly copy paste a prompt in here. So this prompt is going to say, hey, send marketing an Instagram post with an image and a tagline for the best-selling cookie in the San Francisco store so we can increase our sales and show that we listened to customer feedback. So we're going to go ahead and here and click, uh-oh. We're doing it live. Well, that should have unfortunately worked. Hold on. We'll try again. So we'll go back in here. We're going to add our tools, AI star, and then we're going to come in here and add our marketing. It's now in our autocomplete in here. And now let's try this again. Hmm. Unfortunately, it looks like there's a connection with our, uh, unfortunately, what is this? Oh, oh, oh, I'm so silly. I typed in the wrong name of the function. You all should have caught that for me. I need to type in retailpraw.ai. Thank goodness for error messages. So we're going to come in here and now we're going to add this in. And now we're going to go ahead and do this. Thank you. Thank you. All right. So what is happening here is going to be a little bit magical. So we're going to come inside. Oh, hold on. Oh, oh my goodness. All right. We're going to have to bear with me. We're going to have to do it all again. All right. We're coming in here. Uh, coming in here. Sorry. Uh, this is why you don't do it live. All right. Retailpraw.ai. All of the functions in there. We're going to add in our marketing tool that's going to generate our Instagram ad. All right. We're in here. We're going to send the prompt and we're going to make sure it's temperature zero, which is what we kind of forgot there. Temperature zero, live demo. All right. Here we go. Now we're going to send this. And what's happening is going to be kind of magical. So as we come in here, you're going to see that llama3 is going to do chain of thought reasoning. So it's going to figure out which tools it needs to call in order to execute this. So you can come in here and say, oh, the first thing it had to do was grab the franchise ID for my San Francisco store. The second thing is it needed that to be able to access our franchise sales because it's trying to identify what that best selling cookie is. Then it's going to grab all of that sales data in here. So it grabbed all my sales data. And from here, I identify the best selling cookie is this almond biscotti cookie. From there, it said, hey, we need to show that we listened to customer feedback. So we need to look into our customer reviews tool. From there, it's going to ask, hey, what do customers like about this biscotti cookie? It's going to say, oh, they like the crunchy texture and unique flavor. And it's going to send all of this to my Slack tool that's going to generate this Instagram image and caption and send it to my marketing team on Slack so they can review it before we post it on our social media. And so now the moment we've all been waiting for is let's see what it actually returned. So I'm going to jump over into my Slack. And this is the image that was generated by the Shutterstock image AI model that shows our image biscotti. And then you can also see that it creates this customized caption where it says, our customers rave about our biscotti for its crunchy texture, unique flavor, and perfect coffee dipping quality. And so this is what it has generated. And this is how you can use data intelligence to extend your general intelligence to improve a base model. Now, what happens if I remove the intelligence? So I can come in here, kind of like we did earlier, and we're going to remove all of these enterprise data-enabled tools, and we're going to run it all again. So now we've taken away all of our enterprise data access from this, and now it's still going to generate an image, and it's still going to generate a caption according to that prompt, but it's going to be much more generic. So as we jump back into Slack, and it's going to show me my new image soon, down here, okay, successfully sent. Here we go. So this is the image that I now created. So all I had to go off of was that it's a cookie that's in San Francisco, and so it tried to create some kind of cool Instagram ad. Yes. And if you take a deeper look at the caption, it's very generic. So it just says, our best-selling cookie is backed by popular demand. Share your favorite cookie moments with us. And so not really tailored to our specific business, or the franchise, or using our enterprise data at all, and this is why data intelligence is so important. So we just showed how you can use the tools catalog to extend your general intelligence with your enterprise data to create data intelligence, but how do I know that this agent is high quality? So the way that I know it's high quality is I'm going to use agent evaluation and MLflow tracing tools. So agents are really highly, how do you know if what we just did was good or bad? So we're actually, and there's so many different things you can do with an agent as well. So we're actually going to have to launch a pilot program with a subset of the franchises and give them the agent evaluation review app. So this review app is going to allow all of your franchise owners to interact with your agent, whether or not they have a Databricks account, and then it's going to allow them to give feedback on the response. So they can come down here and they can say yes, and they can explain why or why not that this answer was good, and then they can go ahead and click done and submit this feedback. This feedback that is submitted is then logged in a Delta table in Unity Catalog in your account that you can then build an evaluation data set off of, get confidence that you can go into production, or as I have done, I enabled lake house monitoring on top so that I can observe how my pilot program is running. So you can see over here the different franchises that I set up this on, and here is me tracking their negative scores with the agent over time, and you can see that something is going terribly wrong with the Los Angeles franchise. They're getting a lot of negative feedback on the agent. So if I scroll a bit further to actually investigate their ratings and what questions they're getting, they're rating poorly, we can see that their actual feedback that they're giving me on here is that it's returning irrelevant reviews, so it's returning reviews from San Francisco stores or non-LA stores to them, or it's even hallucinated that there's a Liberty chip cookie, which we don't sell one of those at the cookie conglomerate. So we're going to need to dive in deeper to figure out what's going wrong with our quality here. So we're going to use MLflow tracing to get deeper and figure out what exactly is going wrong. So I'm going to jump inside of a notebook where I've actually queried my assessment logs in here, and in here we have captured an MLflow trace automatically for you. So MLflow is a popular tracking API for GenEI and machine learning experimentation and deployment, and so we've extended it to now work with compound AI systems, where you can now trace your input to the system and how it's transformed as it goes through every step of the system along the way to actually create that output in the end. So I can click on one of these traces, and it's going to open up this stack view. If I click the top of the stack, you can see the question that was sent to the system and the output. So you can see this is the one saying what are customers saying about Liberty Chip, and then this is the absolute hallucination which we learned from the review app, where it's saying, hey, customers are raving about this cookie, and it's out of this world delicious, but we know there's no such thing as this cookie. So we need to figure out what went wrong. So we're going to go over into our stack, and we're going to dig deeper into it, and so we're going to go to our first tool that was called, which is this customer reviews tool. As we go in here, we're going to see what the input and output of that were, and if I go here, you can see, okay, it's saying that the Liberty Chip cookie is out of this world delicious, so this is definitely where the problem is coming from, so we're going to need to dive even deeper into the stack, and when we get down here, we're going to get into our retriever. So this is the thing that is actually retrieving our customer reviews, and so if I look, and now, because of tracing, I can actually see the exact reviews that are returned, and so I can see this review is saying, hey, the staff is warm and welcoming, the store is spotless, and the cookies were out of this world delicious. So what's happening is my retriever, because I can't find anything about this Liberty Chip cookie, is just returning random reviews, and so I'm going to need to do two things to fix this. The first, I'm going to need to actually increase my criteria threshold for relevance on my review app retriever, so it says, hey, don't return reviews if the relevance is below a certain threshold, and then I'm going to need to do a little more prompt engineering to make sure that if the context is given to my model, isn't relevant to the question that was answered, don't just summarize that context. So I've gone ahead and I've already made those two fixes, and I've redeployed my agent, and so now we've redeployed the review app as well, sent this out to my franchisees and in my pilot, and so now they can say what are customers saying about the Liberty Chip cookie, and so if I type this in here, you can see that now, instead of hallucinating, it's saying that Liberty Chip cookie is not mentioned in the reviews, it's possible that's not sold in these stores, which is exactly what we wanted to say when this happens. All right, so in this demo, we just showed how you can use the tools catalog to build data intelligence by extending a general model, we saw how you can use agent evaluation to actually understand your quality by getting your agent into the hands of your franchise owners, even if they don't have Databricks accounts, and allowing them to give that human in the loop feedback with thumbs up, thumbs down, and then we used MLflow tracing to allow you to debug and iterate on your quality to improve your agent. So we talked a lot about many different things in this talk, but there's three main key takeaways that we want you all to really gather about the Mosaic AI platform. The first is that we have to move from general intelligence to data intelligence, and the way that we do this is we augment general intelligence with your enterprise data, and this is going to give you much better insights into what's happening in your business, as well as it's going to improve the quality of your applications, and we saw this with the franchise cookie agent, where the images and the captions for our Instagram ad campaign were much better once we gave it access to that enterprise data. The second thing is that you can also improve quality by moving from these monolithic models to modularizing them down into AI compound systems, where now you can specialize each step in the system to improve your quality, like we saw with the fact set use case, and also in many cases will also improve your latency. And lastly, the Mosaic AI platform is the best platform to build high-quality compound AI systems. We have thousands of customers using the Mosaic AI platform today, and one of those customers is Block, and so I'm very excited to welcome Jackie Brossamer from Block to the stage, who's going to talk about how her team has leveraged the Mosaic AI platform to build and deploy generative AI solutions. And so with that, welcome Jackie Brossamer to the stage.