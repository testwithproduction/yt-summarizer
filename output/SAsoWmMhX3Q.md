# Summary

The technical talk features an engaging conversation with Jensen Huang, CEO of NVIDIA, focusing on several key developments and strategic insights in the tech industry:

1. **Company Success and Vision:** Huang reflects on NVIDIA's success, with a market cap reaching $3 trillion. He jokes about not building GPUs as a strategy and emphasizes the critical role of data and AI in their operations.

2. **Data Intelligence in Enterprises:** Huang highlights the vast potential of proprietary data that companies possess, equating it to gold mines. Emphasis is placed on refining and learning from this data through AI models to improve products, services, and supply chains.

3. **Open Source vs. Closed Models:** Huang discusses the importance of both closed and open source models, citing significant events like the release of LLama and DBRX. Open source models have activated enterprises to leverage AI, leading to widespread adoption and customization.

4. **AI Factories and Tokenization:** Huang introduces the concept of AI factories and tokenization of data, likening it to an industrial revolution where AI supercomputers convert data into intelligence. He envisions all companies becoming intelligence manufacturers, developing domain-specific AI expertise.

5. **Accelerated Computing and Generative AI Partnership:** Huang announces a partnership between NVIDIA and Databricks to accelerate data processing using GPU technology, enhancing efficiency and reducing costs. This collaboration aims to revolutionize data processing workloads currently handled by CPUs.

6. **Generative AI's Impact:** Huang discusses the transformative potential of generative AI across industries, including customer service automation, biological research, logistics, and real-time weather prediction. He foresees generative AI significantly changing computational paradigms and energy efficiency.

7. **Sustainability in AI:** Huang emphasizes sustainable AI practices, including the strategic location of AI training data centers and the energy savings from advanced AI models. He promotes the longitudinal benefits of AI in various applications, such as weather prediction and chip design.

8. **Future of AI Implementation:** Huang predicts an exponential growth in AI applications, urging enterprises to start integrating AI immediately. He lauds Databricks' platform for data intelligence, advocating for its use in managing and wrangling data to extract valuable insights for AI development.

9. **Concluding Advice:** Huang concludes with advice for organizations to engage with AI actively, noting the rapid advancement and the importance of learning by doing rather than just observing or reading about AI trends.

The talk underscores the transformative potential of AI and data intelligence, advocating for immediate and active engagement in AI initiatives.

# Transcription

 So I am super, super excited to introduce our next guest, who actually is a man who does not need any introduction. So I want to welcome the world's one and only Rockstar CEO and video's Jensen Huang to stage. Awesome, thank you for coming. So Matt, I just want to start, you know, just looking at Nvidia's amazing performance, you know, three trillion dollars, like did you imagine it would be this way, say five years ago, that the world would unfold this way? Sure, from the very beginning. It's so awesome to see any advice for a, you know, fellow CEO. How do we get there? Whatever you do, don't build GPUs. Okay, all right, let me tell the team, we need to back out. Awesome, man. So we spent a lot of time this morning talking about data intelligence, which, by what we mean, enterprises, you know, they have all this proprietary data, training AI models that's customized on their data that they have. How important is that? Is that something you see, you know, is that something that we need to invest more in? What are you hearing? Well, every company's business data is their gold mine. And there's every company is sitting on gold mines. If you have a flywheel of services or products, customers enjoying those services and products, giving you feedback, you've been collecting data for a long period of time. It could be customer related, it could be market related, it could be supply chain related. All of us have flywheels of data that we've been collecting for a long time. We're sitting on mountains of it. But the fact of the matter is, none of us have really been able to extract insight or even more importantly, distill intelligence out of it until now. And so we're pretty fired up. I know we are. And we're using it in our chip design. We're using it in our bugs database. We're using it in creating new products and services and we're using it in our supply chain. And you know, for the very first time, we now have a business, we now have an engineering process that starts with data processing and refinement and learning models and then deploying the models and connecting that flywheel, collecting more data. Isn't that right? Yeah. And so we're doing that in our company and it's making it possible for us to frankly be one of the largest small companies in the world. And the reason for that of course is because we have so many AIs in the company helping us out, doing amazing things. And I think every company is like this. And so I think this is just an extraordinary time and it starts with data. It starts with Databricks. That's awesome. Thank you so much. Curious, there's this whole debate brewing. Closed models versus open source models. Are open source going to catch up? Are both going to exist? Is it going to eventually just be dominated by one giant closed source model? What are you seeing? What are you thinking about the whole open source ecosystem and how important has it been for sort of development of LLMs and how important is it going to be going forward? Well we need frontier models. We need amazing frontier models. Of course the work that OpenAI is doing, the work that Google is doing, really, really important and pushing the frontiers and helping us discover what's possible. But if you were to look at this year, probably the most important events this year were related to open source. LLama 2, now LLama 3, Mistrawl, the work that you guys did, Databricks, DBRX, do I have to say DBRX? It's DBRX, right? DBRX, DBRX, DBRX. I think really, really cool stuff. And the reason why it's really cool is because it activated every single enterprise company. It made it possible for every company to be an AI company. Isn't that right? You're seeing this yourself and we're seeing this all over the place. We recently turned LLama 3 into a fully containerized inference microservice and it's available for downloads. You can go to HuggingFace, you can go to, of course, Databricks and it's now being integrated into several hundred companies around the world. And so that tells you something about how open source has activated every company to be an AI company. We're using open source models all over our company and we create some proprietary ones. We fine tune open source ones, train them for our data and for our skills. And so I think without open source it wouldn't have activated this entire global movement for every company to be an AI company. I think it's a huge deal. Yeah, that's super awesome. So both are going to be around and we need both, like open and close. And is this a NIM framework? You're talking about NIMs of how you do the survey. We call them NIMs. Yeah, yeah. We're super excited. You know, I'm super excited to announce here that we're going to put DBRX inside NIMs and we're going to serve it on Databricks and actually any new models that we develop in the future. So we're super excited about NIMs. Yeah. It's actually quite an amazing thing. In order to create one of these endpoints, these APIs, these large language model APIs, the stack is really complicated. These are giant models. Even though they seem small these days, they're still computationally really large. And the computing stack is really complicated. There are hundreds of dependencies necessary to create one of these endpoints. And so we created this thing called the NVIDIA inference microservice where we package up all of the dependencies. We optimize all of it. We have a factory in the company with all these engineers working who are expert on doing this. And we package it up into a microservice and you could enjoy it at Databricks. You could download it and take it with you. You can fine tune it with microservices that we call NIMO and use it anywhere you like. It runs in every single cloud, runs on-prem. You can enjoy it everywhere. That's super awesome. It's an amazing thing. Yeah. And it's awesome you can even run it on-prem. You don't have to be on the cloud. That's super awesome. So when we talk to customers, we're hearing that they have to develop this sort of expertise in-house to customize models to gain advantage. What are your thoughts on that? Well, I think in the future, what's happening in the world today is that we figured out a way to tokenize almost any information, almost any data. And we can extract structure, understand, learn its representation, understand the meaning of that information of almost any kind. It could be, of course, sound, speech, words, language, images, videos. It could be chemicals and proteins. It could even be robotics articulation and manipulation. It could be steering wheel articulation, driving. We can tokenize almost anything. And because these cloud data centers are really producing tokens, we're manufacturing something that is quite unique for the very first time. We have this instrument called these AI supercomputers that we build. It's producing tokens, generating tokens, and essentially a factory that's designed for that one job. And this ability for us to manufacture intelligence at scale is pretty new. And that's one of the reasons why I'm almost certain now as we're building these AI factories everywhere for all these different industries that we're in the beginning of a new industrial revolution instead of generating electricity, we're generating intelligence. Every company, of course, at its foundation is about domain-specific intelligence. Very few companies on the planet knows more about data and data processing and AI and the infrastructure necessary to do all that than Databricks. We are quite specialized in the work that we do and we're at this foundation all about that domain-specific intelligence. Every company has it. It could be in financial services, it could be healthcare, whatnot. And so at the end of the day, every one of us will become intelligence manufacturers. And if you're going to be intelligence manufacturers, today you have HR. In the future, you're going to have HR for AI. And we call them AI factories. So every single company will have to do that. We are doing that. You're doing that. We see companies large and small doing that. And so in the future, 100% of us will do that. You start with, of course, your domain-specific data. It's sitting in Databricks somewhere. You're going to process that data and refine and extract intelligence out of it. You're going to put it into a flywheel. You're going to have an AI factory. All of us will. Yeah, this is so, so awesome. I totally 100% believe in this. And one thing we're excited about is, so we do a lot of data processing. And data processing is like massive amounts. I think we process about four exabytes every day, 4,000 petabytes every day in Databricks. And it is the single largest computing demand on the planet today, processing data. Every single company does it. Yeah, exactly. And it's actually highly parallelizable. We do the same operations again and again and again. I'm really, really excited to partner together to really bring that kind of GPU acceleration to data processing. So we can do the same revolution that AI models have seen on the core data processing. So we're super excited to partner with you on using GPU acceleration for our photon engine to be able to really enter this new era of also applying GPUs to core data processing. These massive workloads that today have to run on CPUs get them also run on NVIDIA GPUs. We're very excited about that. Yeah, this is a big, by the way, this is a big announcement. Yeah. The two most important trends in computing today is accelerated computing and generative AI. NVIDIA and Databricks are going to partner to combine our skills in these areas to bring it to all of you. And yep. And this work in accelerating data processing, as you know, it's highly parallelizable. But it's really arcane. It's really complicated. And the reason for that is just there's so many data formats. There's so many different ways to group and join. Just wrangling data is a really complicated suite of libraries. Spark is a super complicated suite of libraries. And it's taking us five years working around the clock to finally have a suite of libraries that can now accelerate photon. And this is such a big deal. We've been working on this for a long time. Yeah, for many years. So now we're going to accelerate photon and make it possible for all of you to wrangle data, process your data a lot faster, a lot more cost effectively, and very importantly, consume a lot less energy. Yeah, it makes a lot of sense. It's a huge deal. Yeah. It makes a lot of sense, right? Because in the end of the day, even though it's very complicated and it has a lot of corner cases, it is highly parallelizable. And it is specialized still. You don't really need generic compute for that. We want to do it like, you know, same thing again and again and again on exabyte to data. We're not doing exabyte to data that's completely unique. So I'm very, very excited about this. And I think it really has the ability to revolutionize and really bring faster performance, lower the cost, and just, you know, it's going to be amazing. Yeah, look what happened when we're able to process enormous amounts of data so quickly. It made it possible for researchers to one day wake up and say, guess what? Let's just go get all of the data on the internet and train a giant model because it doesn't take that long. Without acceleration, without accelerated computing, nobody would have ever conceived of doing that. It would have been way too expensive or taken too much time. But now, you know, it's kind of a mundane thing to do. So we're going to be able to process exabytes and exabytes of data so much more cost-effectively and so much more efficiently from a time perspective. Imagine all of the ideas that you're going to have. You know, it's going to be, hey, let's just take all of the data of our company and we're going to train our super AI. You're going to do it. Yeah. The day is going to come. Yeah. I mean, it was a sci-fi idea, right, to take the whole internet. Nobody thought you could do it. We needed the hardware to get there, the infrastructure to be there so we could specialize it. And now, you know, everybody's doing it. So I want to switch gears. God, I love myself. Just kidding. We love you too. So I want to switch gears. So you know, this generative AI boom has been amazing. You know, but the early days, you know, most enterprises started with chatbots. Let's build it on chatbot, you know, customize it on our data and so on. But now we're seeing people branch out to more and more sophisticated use cases. What new applications in AI are you the most excited about going forward? The number one most impactful will probably be customer service for all of the enterprises that are here. Customer service, you know, represents probably several trillion dollars worth of expenses. And every company has it. Every company has it. Every single industry has it. Every company has it. And the important thing about the chatbot, the customer service, is partly about the fact that you could automate, but it's mostly about the data flywheel. You want to capture the conversation. You want to capture the engagement in your data flywheel. It's going to create more data, of course. We're probably, you know, right now we're seeing data expanding about growing, about 10x every five years. I would not be surprised to see data growing 100x every five years because of customer service. And so we're going to connect everything into a flywheel. It's going to collect more data, capture more insight. We're going to extract better intelligence out of it, which will provide better service. Maybe it's even more predictive in the sense that, proactive in the sense that before a problem even arises, you reach out to the customer and say, you know, this thing is about to expire or we notice that you're still using this version or whatever it is. And you reach out to the customer and proactively solve a problem, just like preemptive maintenance. We're going to have proactive customer support. Just going to create more data. We're going to write that flywheel. And so I think customer service is probably going to be the most profoundly supercharging capability for most companies because of that, because of the data it's going to collect. But we've tokenized everything. You know, I'm excited about the fact that we're generating chemicals. We're generating proteins. We're carbon capture materials. Carbon capture enzymes. Incredible batteries that are being designed. And so we're generating physics, physical AI. And recently we made it possible to do regional weather prediction down to a couple of kilometers. Now it would have taken a supercomputer about 10,000 times more capability to be able to predict weather down to a kilometer. And now we're using generative AI to do that. And so as a result, logistics will be enhanced. Insurance will be enhanced. Of course, keeping people out of harm's way will be enhanced. And so physical things, biological things, of course, generative AI for 3D graphics, digital twins, creating virtual worlds for video games. I mean, generative AI is just everywhere, every single industry. If your industry is not involved in generative AI, it's just because you haven't been paying attention. It's everywhere. Yeah. Yeah. I totally believe. We're going to see. There's no area, but we're not going to see applications of this. It makes a lot of sense. It's so exciting. You know, these new frontiers are super exciting. And there's huge needs for data, AI. What's your thoughts on how we can help enterprises make AI that's more sustainable? Well, sustainability has a lot of different perspective. One of the sustainability has to do with energy. And remember, AI doesn't care where it went to school. We don't need to put AI training data centers near population where the energy grid is challenged already. We could put it somewhere where it's not challenging. And so you know that the world has, Earth has a lot more energy. It's just in the wrong places. And so I think for the very first time, we can go capture that excess energy, compress it into an AI model, and bring these, you know, AI models back to society where we could use it. That's one major thought. And another is, I remember that AI is not about training. AI is about inference. And it's about the generative capabilities of the AI. You're training the model so that you could use it. And when you think about the longitudinal benefit of AI, and I just gave you the example of predicting weather using AI instead of using supercomputers, we understand basically the laws of physics that's involved in weather prediction. We don't need to simulate it from first principles every single time. We have to generate it using AI. And by generating it using AI, not only do we reduce the amount of time that it takes, improve the resolution that we can generate for, but also the amount of energy by thousands of times, not tens of, you know, not percentages, thousands of X factors. Well, by doing that, we're doing the same thing by designing chips that you're using in cell phones. You train the model once, design better chips with those models. As a result, you save energy for everybody involved. When you think about the longitudinal benefit of AI, I'm fairly certain that it will demonstrate the amount of energy that's saved. And then one last thought about generative AI. You know that today's, and why is such a big deal from a computer science perspective, today's computing experience is retrieval-based. Largely, you know, we touch the phone, and even though when we use our phone, we think it uses very little energy, every single time you touch it, it goes off and activates REST APIs all over the world, retrieves information. The internet is on, you know, lit up, brings back a little bit of information for you from all these different data centers, assembles it based on a recommender system, presents it to you. Well, in the future, it's going to be more contextual, more generative, right there on the device, running a model, a small language model. The amount of internet traffic will be dramatically reduced, and it'll be much more generative with some retrieval to augment, right? And so the balance of computation will be dramatically shifted towards immediate generation. Well, this is very, you know, it's going to save a ton of energy, and it's very sensible, and the reason for that is this. Imagine every single question that Ali asked me, I got to race back to my office, go get some files, bring it back, and present it to him, let him decide which piece of that information he wants to extract for himself. Instead, I'm generating everything, you know, from about 25 watts right now as we speak, right? The amount of energy that we save is going to be extraordinary, and the computing model is going to transform completely. And so this way of computing is going to save tons of energy. Of course, we're going to get our answers a lot more efficiently instead of us combing through stuff. But then we'll have even more questions, right? We'll have more questions, which is really, in fact, that's the big idea. The big idea about the future, us working with AI, is prompting. We're going to have so many more interesting questions, because we're going to get a lot of answers very quickly. So this is a very big deal. Very exciting future. Okay, my final question to you. How do we help customers, you know, organizations here get started today? What's the best way? Well, you know, I told you before that I thought the pivot of Databricks expanding from data processing to data governance and store, and then extending it into all the way, longitudinally, all the way to extracting intelligence out of that data. I think that was completely genius. And I forget her name, but I thought Cookie Lady did an incredible job. Casey. What's that? Casey. Okay. Don't steal her, please. I thought she did an amazing job. I was enjoying it. We were backstage, and everybody wanted to talk, but I just wanted to watch her give her demo. I thought the platform is incredible, and you've made it easy for people to manage their data, extract information, process that data, wrangle that data. You know, wrangling data is still a very big part of training the model. People talk about training the model, but long before you train the model, you've got to go figure out what data, right? It's about data quality. It's about data formats, about data preparation. And so I think the way you start is come to Databricks and use the Databricks data intelligence platform. Am I right? Yeah. Absolutely. Who wouldn't call their platform DIP? That was such a good idea. You know, Databricks DIP sounds good. I like it. It's almost as good as NIMS. All right, all right. There you go. Well, you can do both together, right? Yeah. You don't have to pick. Go get yourself a NIM on DIP. Yeah, I agree. Why not? That's the way to do it. You can dip that cookie. I would absolutely start. Whatever you do, just start. Whatever you do, start. You have to engage this incredibly fast moving train. Remember, generative AI is growing exponentially. You don't want to wait, observe an exponential trend because in a couple of years, you'll be so far behind, it's incredible. Just get on the train. And enjoy the train as it's getting faster and faster exponentially. Learn along the way. And so, you know, this is one of those things you can't learn by watching. You don't want to learn by reading about it. Just learn by doing. And which is the way we're doing it. And so, just get engaged. All right. That's great advice. Jensen, it's been an amazing decade. Thank you for everything. We've been great partners looking forward to our next decade together. Databricks. All right. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks.