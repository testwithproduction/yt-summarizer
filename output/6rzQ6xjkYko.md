# Summary

The talk discusses the new Databricks product called Lakeflow and focuses on its features and benefits. Below are the key points highlighted from the transcription:

1. **Introduction to Databricks Lakeflow**: 
    - Emphasized the importance of good data for machine learning, BI, and analytics.
    - Identified the complexities and costs associated with traditional data toolchains.

2. **Components of Lakeflow**:
    - **Lakeflow Connect**:
        - Simplifies data ingestion from various enterprise applications and databases (e.g., SQL Server, Salesforce).
        - Utilizes technology from Archeon to handle change data capture (CDC) efficiently and reliably.
        - Demonstrated how Insulet used Lakeflow Connect to significantly reduce time for insights from Salesforce.

    - **Lakeflow Pipelines**:
        - Facilitates data transformation using plain SQL for both batch and streaming processes.
        - Eliminates the need for manual infrastructure setup and complex scripting.
        - Utilizes materialized views for automatic handling of schema evolution, failures, retries, and incremental processing.

    - **Workflows and Orchestration**:
        - Integrates Databricks Workflows to replace tools like Airflow.
        - Provides a unified canvas to build and orchestrate data pipelines and dashboards.
        - Features real-time mode for fast and consistent streaming results.

3. **Live Demonstrations**:
    - Showcased how to ingest data from Salesforce and SQL Server into Lakeflow.
    - Built a sample transformation pipeline by writing SQL and conducted a live demo.
    - Demonstrated the capability to integrate batch and streaming data processes.
    - Created and ran a dashboard using unified canvas and suggested templates from the Assistant.

4. **Innovations and Benefits**:
    - **Unified Monitoring**: Integrated monitoring for data health, freshness, cost, and performance.
    - **Serverless Compute**: Simplifies management of clusters and instances, boosting performance and cost-effectiveness.
    - **Data Intelligence Integration**: Includes Databricks IQ and the Assistant to facilitate authoring, monitoring, and diagnosing pipelines.
    - **Enhanced Streaming Ingest**: Faster and more cost-effective streaming ingest, providing high performance.

5. **Availability and Compatibility**:
    - Lakeflow Connect in preview with planned expansions for Pipelines and jobs.
    - Ensured backward compatibility with the existing Delta Live Tables and workflows.

In summary, Databricks Lakeflow aims to streamline data handling by integrating seamless ingestion, transformation, and orchestration in a unified, efficient, and cost-effective manner, leveraging the robust Databricks ecosystem.

# Transcription

 All right. Good morning. So I'll get started. As it turns out, there are five Bilal's at Databricks. I asked all five to give me a little cheer, but that was more than five. So thank you. OK, so thank you, Ali, for the introduction. So we've heard about machine learning. We've heard about BI. We've heard about analytics and all these amazing things. And I'm here to tell you that every single one of them, everything starts with good data. All right. How do you get to good data? Well, there are three steps you have to follow. And every single one of us, including me, we are traditionally cobbling together lots of different tools in an ever-increasing tool chain that gets more and more expensive, more and more complex. Let's go through that real quick. So Spark, and especially Databricks, is already very good at big data. As Ronald was telling you, this is the world's biggest big data processing framework. But as it turns out, that a lot of your really valuable data is sometimes in smaller data. So for example, you may have MySQL, Oracle, Postgres, all these different databases. They're incredibly valuable. So you might be setting up Debezium and Kafka and a monitoring stack and a cost management stack just to get the changes from these systems into Databricks. Or you might, I'm actually pretty confident, that every single one of us is using a CRM of some kind. Maybe you're using Salesforce, NetSuite. Maybe you use HRMs like Workday and NetSuite, right? Tons of valuable data in there just waiting to get into Databricks. So you can start using it. And then once your data is in a data platform like Databricks, the next step, the very next step is to transform it. As it turns out, newly ingested data is almost never ready for use by the business. You have to filter it. You have to aggregate it. You have to append it and clean it. Lots of technology choices. DBT, a great open source project. You might have heard about Delta Life Tables in PySpark. Ronald was telling you how popular it is. Which one of these do you use? Again, how do you monitor it? How do you maintain it? And once your data is transformed, that's really not even half the battle. You get the value out of data by actually running your production pipelines in production. I don't like waking up at 2 in the morning with an alert. So now you have to orchestrate. So you might be using Airflow. Great. Now your tool chain just expanding just a little bit more. You're responsible for managing Airflow and its authentication stack and so forth. And then, of course, you might have to monitor all these things in CloudWatch. This is unnecessarily complex. And this is inefficient. And it's actually very expensive. Which is why I am extremely proud to unveil what we're calling Databricks Lakeflow. This is a new product that's built. Thank you. [? APPLAUSE ?] This is a new product that's actually built on a fundamental foundation of Databricks workflows and Delta Life Tables with a little bit of magic sauce added on. I'm actually going to start with the magic sauce. And it gives you one simple, intelligent product for injection, transformation, and orchestration. All right, let's get into it. The very first component of these three components is something we call Lakeflow Connect. Lakeflow Connect is native to the Lakehouse. These are native connectors. And when I say they're native and they're high performance, and they're simple for all these different enterprise applications and databases. If in the audience today, you're using SQL Server, Postgres, a legacy data warehouse, or you're using these enterprise applications, we're on a mission to make it really simple to get all of this data into Databricks. And this is actually powered by Archeon Technology, a company we acquired last year. So I'll give you a quick demo in a moment. But I actually want to talk about one of our customers called Insulet. And Insulet manufactures a very innovative insulin management system called the Omnipod. And they had a lot of customer support data locked up in Salesforce. And they're one of our happy customers of Lakeflow Connect. With that, they're able to get to the used to spend days on getting insights. Now they have it down to minutes. It's super exciting. All right. So actions speak louder than words. So let's take a look. All right. So you're in Lakeflow here. And what I'm going to do is I'm going to click on Injust. And you see it's pointing click, which is pretty awesome. And it's designed for everybody. I'm going to use click on Salesforce. And my friend Eric Orgren has set up a connection. By the way, everything in Lakeflow is governed by Unity Catalog and is secured by Unity. So you can manage it very easily in government. And there are three steps. And now, OK, great. So now I see these objects from Salesforce. And I'm going to choose orders. I actually work for Casey. I don't know if you remember her cookie company. I'm building the data pipeline. She's my CEO. So I'm going to bring in some order information for our ever-growing cookie business into this catalog and schema. And hang on a second. There we go. And within seconds, data should show up in our lake house. Excellent. All right. That's it. That's all it took. There are no more steps. All right. Let's get back to slides. So I want to do something here and kind of give you a peek behind the curtain. We're all engineers here. And there's actually something pretty magical that's happening inside of Lakeflow Connect. You might think, gosh, how hard could it be to connect to these APIs and these databases? Can't you run a SQL query? It turns out that what you actually want to do is only obtain the new data, the change data capture from these source systems, from these databases and enterprise applications. And as it turns out, this is a really, really hard problem. You don't want to tip over the source database. You don't want to exhaust API limits. The data has to arrive in order. It has to be applied in order. Things go wrong. It's the real world. And you're coupling systems together. And you have to be able to recover. All of this is undifferentiated heavy lifting. And I'm really glad that we're doing it, because with Archion Tech, CDC is no longer a three-letter word. It's point and click. It's automated operations. And it's consistent and reliable. Super exciting. All right. Let's go to the second part of this product, the second component. What happens once you bring in data? You're now able to load data from these databases and enterprise applications. The very next thing you have to do is to transform it, which is to prepare it. Remember, you have to filter, aggregate, join, and clean it. Typically, this involves writing really complicated programs that have to deal with a lot of error conditions. And I'll show you that in a moment. The magic trick behind Lakeflow Pipelines, because it's built on the foundation, is the evolution of Delta Life Tables, is that it lets you write plain SQL to express both batch and streaming. And the magic trick is we turn that into an incremental, efficient, and cost-effective pipeline. OK. So let's go back. And remember, I am making, I just did some ingestion of data. And what I'm going to show you is, I've also pulled in data from SQL Server. I won't show you that flow. So I have data from Salesforce. I have data from SQL Server. And I need to now go ahead and create a little bit of an aggregation out of that. OK. So let me show you how simple that is within Lakeflow. One of my favorite features here, by the way, is that it's one single unified canvas. So this little dag at the bottom, you always see it. You can hide it if you want. But I'm going to click here on Salesforce. And I'm going to write a transformation. OK. That's simple. Now, this is an intelligent application. It's built on the data intelligence platform. So I might just go ahead and ask the assistant what it thinks I should join. OK. It comes up with a pretty reasonable join. It says you can join these tables. And I'm just going to let it figure out how to join them. Figure out the key for me. And that's pretty awesome. OK. That looks about right. It found the customer ID key. I'm going to go ahead and accept that. And let me just run this transformation real quick. I don't have to deploy it. I can run it in development. And it'll actually give me the ability to debug it real quick. OK. Perfect. So I can see that I have orders, dates, products, customers. All of this came together really nicely. I have a nice little sample of data. Perfect. All right. So we can go back to slides now. OK. Woo! Woo! Woo! Woo! Thank you. That was one of the belals. OK. Great. So again, I'm going to give you a little bit of a peek behind the curtain here. Why is this pretty amazing? Notice that in this, there was no cluster. There was no compute. I didn't have to set up infrastructure. I didn't have to write a long script. I just wrote SQL. And this is the power of declarative transformations. This here is actually my valuable transformation. And instead, without Lakeflow Pipelines, you have to do table management. You have to do incrementalization many, many times. And even have to deal with schema evolution. I've spoken with some of our customers. They've written entire frameworks to do schema evolution and schema management. Again, that's undifferentiated, heavy lifting. Why should you spend time on that? And this beast just grows and goes. And Lakeflow Pipelines are powered by something called materialized views. And they're magical because they automatically handle the bookkeeping for you. They handle schema evolution. They handle failures and retries and backfills. And they magically choose the right way to incrementalize your data. So it's pretty awesome. OK. Now, you might be thinking, hey, that's pretty great. But you just wrote a couple of lines of SQL. My Pipelines are more complicated. So in my world, my CEO, my cookie CEO, is really demanding our e-commerce website. It's just really taking off. And now we need to be able to do real-time actions on our website. So from this pipeline, which looks like batch, I'm going to add some streaming to it. I'm going to go ahead and write some joined and enriched records into Kafka. So let me show you how easy that is. And let's take a look at that. Great. So remember, this is my pipeline here. I did this materialized view, a transformation. So again, from this unified canvas, I just add a Pipeline step. And I'm going to go live here. I'm going to write some code. OK. So I'm going to create something called a sync. Think of a sync as a destination. I'm just going to call it Kafka because I'm going to write to Kafka. And all I have to do is, this is kind of cool because all I'm doing is writing SQL here. And I'm going to point this at Kafka.databricks.com. And that should be enough to create a sync. And all the credentials are coming through Unity catalog. So this is, again, governed. And I'm going to create something called a flow. And the flow, think of that as an edge that writes changes into Kafka. I'm going to do target Kafka. And I'm going to select from the sales table that I just created. And I'm going to use the table changes, table value function. OK, something's not right here. And I need to do dev. Great. OK. So this looks good. And remember, this is what looks like a batch pipeline. And I'm going to turn this into streaming. There we go. And just like that, our data is in Kafka. Let's go to slides. [? APPLAUSE ?] And this is something super exciting. There is no code change here. I didn't have to make a change. I didn't have to choose another stack. Everything just works together. One of the coolest things we're doing is something called real-time mode for streaming. You can think of this as real-time mode for streaming makes streaming go really, really fast. And the magic trick here, it's not fast once or twice. It's consistently fast. So if you have operational streaming use case where you have to deliver data and insights, just turn it on. And this pipeline will go really, really fast. And we have talks about it. Ryan Neonhouse is doing a talk on it. So please go check it out. Perfect. So now I have ingested data from SQL Server and Salesforce. I've very quickly built a pipeline that is able to deliver batch and streaming results. It's always fresh. I didn't have to do manual orchestration. But now my CEO is very demanding. The cookie business continues to grow. And Casey wants insights. And she wants a dashboard that she can use to figure out how her business is doing. And this is where orchestration comes in. And orchestration is really, how do I do all the other things that are not involved with loading data and transforming data, such as building a dashboard and running it, or refreshing a dashboard? One of my favorite capabilities in Databricks is something called Databricks Workflows. And we've evolved it into the next generation. And Workflows is a drop in complete orchestrator. No need to use Airflow. Airflow is great. But it's completely included in Databricks. And this is just a list of innovations. It has lots of capabilities that you might be used to in traditional orchestrators. OK. So what I'm going to do here now is I'm going to walk over. And I am going to start building a dashboard. I'm going to run it after my pipeline is done. OK, let's take a look. OK. So remember, I have data going into Kafka. I have all this. I'm going to just add another step. I love this unified canvas. It's like a really nice context on where I am. And this is super cool. The Assistant suggests a dashboard that's pretty cool. That's useful. Revenue and product insights. I like that. That's what I would have wanted. And let me hide that a little bit. And there it is. That's our dashboard. So hey, good news. Our cookie business continues to grow. We're not all the way done with the business. And this is super cool. We actually have a really interesting insight here that Shikoku cookies tend to sell in the month of December. So super cool. So that's it. You don't have to do anything else. Let's get back to slides. So I'm going to wrap up really quickly. I'm super excited about one innovation that I think will make our lives as data teams and data engineers much, much better. Look, it's great to create DAGs, things that run after another. It's great to have schedules. When should something run? But as your organization grows, what you really want are triggers. And triggers, think of them as work happens when new data arrives or data is updated. And this is actually what allows us to do another magic trick, which is run your pipeline exactly when it's needed, when upstream tables are changed, when up upstream files are ready. This is super cool. It's completely available in the product. It's actually a foundational block of Lakeflow jobs. Perfect. So now everything is running. I've ingested data. I have transformed it. I've built a dashboard. My pipeline is running in production. Like I said, I hate waking up in the middle of the night. And typically, I have to glue together a lot of different tools to see cost and performance. Lakeflow includes unified monitoring. It includes unified monitoring for data health, for data freshness, cost, runs. And you can debug to your heart's content. But it has that single pane of glass, so you don't have to if you don't want to. Lakeflow is built on the Databricks Intelligence platform. It's native to it. This gives us a bunch of superpowers. You get full lineage through Unity Catalog. That includes ingested data. So all the data upstream from Salesforce or Workday or MySQL, we already captured the lineage. It includes federated data. It includes dashboards, even ML models, not a single line of code needed. It's built on top of serverless compute. Frease you up from managing clusters, managing instances, which how many executors, what type of instance should I use? It's serverless. It's secure and completely connected to Unity. So it flees you up from that hassle. But what's also really cool about it is that we did this benchmark. This is real data. For streaming ingest, it's 3 and 1 half times faster. And it's 30% more cost effective. So that's have your cake and eat it too. It's super exciting. [? APPLAUSE ?] Data intelligence is not just a buzzword. As you have seen in the last couple of days, it's actually foundational to Databricks. It's also foundational to Lakeflow. Lakeflow includes a complete integration with Databricks IQ and the Assistant. So every time you're writing code, every time you're building a DAG, every time you ingesting data, we're here to help you author, monitor, and diagnose. And one last thing, this is actually an evolution of our existing product. So you can confidently keep using desktop live tables and workflows. We'll make sure that everything is backwards compatible. All your jobs and pipelines will continue to work. And you can start enjoying Lakeflow. So Lakeflow is here. We're actually doing a talk. Elise and Peter are doing a talk on Lakeflow Connect, I think, very soon. Lakeflow Connect is in preview. Please join us. Give us feedback on what connectors you want. We're very excited about it. Pipelines and jobs are coming soon. All right, I think that's it. Thank you.